C:\Python312\python.exe C:\Dev\PairLIE\main_v2.py 
C:\Python312\Lib\site-packages\torch\utils\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.
  warnings.warn(
===> Loading datasets
===> Building model 
===> Epoch[1](10/324): Loss: 693.4663 || Learning rate: lr=0.0001.
===> Epoch[1](20/324): Loss: 822.4595 || Learning rate: lr=0.0001.
===> Epoch[1](30/324): Loss: 633.2654 || Learning rate: lr=0.0001.
===> Epoch[1](40/324): Loss: 632.6107 || Learning rate: lr=0.0001.
===> Epoch[1](50/324): Loss: 512.8384 || Learning rate: lr=0.0001.
===> Epoch[1](60/324): Loss: 290.0411 || Learning rate: lr=0.0001.
===> Epoch[1](70/324): Loss: 193.6133 || Learning rate: lr=0.0001.
===> Epoch[1](80/324): Loss: 232.5864 || Learning rate: lr=0.0001.
===> Epoch[1](90/324): Loss: 66.9816 || Learning rate: lr=0.0001.
===> Epoch[1](100/324): Loss: 174.5356 || Learning rate: lr=0.0001.
===> Epoch[1](110/324): Loss: 53.1035 || Learning rate: lr=0.0001.
===> Epoch[1](120/324): Loss: 65.4925 || Learning rate: lr=0.0001.
===> Epoch[1](130/324): Loss: 98.9213 || Learning rate: lr=0.0001.
===> Epoch[1](140/324): Loss: 200.3883 || Learning rate: lr=0.0001.
===> Epoch[1](150/324): Loss: 64.2286 || Learning rate: lr=0.0001.
===> Epoch[1](160/324): Loss: 172.6505 || Learning rate: lr=0.0001.
===> Epoch[1](170/324): Loss: 51.2717 || Learning rate: lr=0.0001.
===> Epoch[1](180/324): Loss: 29.3900 || Learning rate: lr=0.0001.
===> Epoch[1](190/324): Loss: 47.5373 || Learning rate: lr=0.0001.
===> Epoch[1](200/324): Loss: 21.2340 || Learning rate: lr=0.0001.
===> Epoch[1](210/324): Loss: 12.9114 || Learning rate: lr=0.0001.
===> Epoch[1](220/324): Loss: 15.7816 || Learning rate: lr=0.0001.
===> Epoch[1](230/324): Loss: 30.0042 || Learning rate: lr=0.0001.
===> Epoch[1](240/324): Loss: 10.3473 || Learning rate: lr=0.0001.
===> Epoch[1](250/324): Loss: 34.7450 || Learning rate: lr=0.0001.
===> Epoch[1](260/324): Loss: 13.3711 || Learning rate: lr=0.0001.
===> Epoch[1](270/324): Loss: 8.0892 || Learning rate: lr=0.0001.
===> Epoch[1](280/324): Loss: 19.9588 || Learning rate: lr=0.0001.
===> Epoch[1](290/324): Loss: 14.5045 || Learning rate: lr=0.0001.
===> Epoch[1](300/324): Loss: 19.1900 || Learning rate: lr=0.0001.
===> Epoch[1](310/324): Loss: 8.7305 || Learning rate: lr=0.0001.
===> Epoch[1](320/324): Loss: 10.7524 || Learning rate: lr=0.0001.
===> Epoch[2](10/324): Loss: 17.1734 || Learning rate: lr=0.0001.
===> Epoch[2](20/324): Loss: 7.6679 || Learning rate: lr=0.0001.
===> Epoch[2](30/324): Loss: 10.5551 || Learning rate: lr=0.0001.
===> Epoch[2](40/324): Loss: 21.6455 || Learning rate: lr=0.0001.
===> Epoch[2](50/324): Loss: 7.6271 || Learning rate: lr=0.0001.
===> Epoch[2](60/324): Loss: 6.0161 || Learning rate: lr=0.0001.
===> Epoch[2](70/324): Loss: 20.9803 || Learning rate: lr=0.0001.
===> Epoch[2](80/324): Loss: 19.2790 || Learning rate: lr=0.0001.
===> Epoch[2](90/324): Loss: 12.7102 || Learning rate: lr=0.0001.
===> Epoch[2](100/324): Loss: 13.1170 || Learning rate: lr=0.0001.
===> Epoch[2](110/324): Loss: 11.3599 || Learning rate: lr=0.0001.
===> Epoch[2](120/324): Loss: 8.6916 || Learning rate: lr=0.0001.
===> Epoch[2](130/324): Loss: 10.9826 || Learning rate: lr=0.0001.
===> Epoch[2](140/324): Loss: 11.8022 || Learning rate: lr=0.0001.
===> Epoch[2](150/324): Loss: 4.6992 || Learning rate: lr=0.0001.
===> Epoch[2](160/324): Loss: 12.4182 || Learning rate: lr=0.0001.
===> Epoch[2](170/324): Loss: 7.8599 || Learning rate: lr=0.0001.
===> Epoch[2](180/324): Loss: 15.4860 || Learning rate: lr=0.0001.
===> Epoch[2](190/324): Loss: 8.1850 || Learning rate: lr=0.0001.
===> Epoch[2](200/324): Loss: 5.4508 || Learning rate: lr=0.0001.
===> Epoch[2](210/324): Loss: 8.7492 || Learning rate: lr=0.0001.
===> Epoch[2](220/324): Loss: 12.6324 || Learning rate: lr=0.0001.
===> Epoch[2](230/324): Loss: 12.0536 || Learning rate: lr=0.0001.
===> Epoch[2](240/324): Loss: 5.1943 || Learning rate: lr=0.0001.
===> Epoch[2](250/324): Loss: 3.3044 || Learning rate: lr=0.0001.
===> Epoch[2](260/324): Loss: 9.5405 || Learning rate: lr=0.0001.
===> Epoch[2](270/324): Loss: 9.2354 || Learning rate: lr=0.0001.
===> Epoch[2](280/324): Loss: 5.8392 || Learning rate: lr=0.0001.
===> Epoch[2](290/324): Loss: 11.8028 || Learning rate: lr=0.0001.
===> Epoch[2](300/324): Loss: 22.0486 || Learning rate: lr=0.0001.
===> Epoch[2](310/324): Loss: 4.1504 || Learning rate: lr=0.0001.
===> Epoch[2](320/324): Loss: 17.0409 || Learning rate: lr=0.0001.
===> Epoch[3](10/324): Loss: 24.1674 || Learning rate: lr=0.0001.
===> Epoch[3](20/324): Loss: 5.3736 || Learning rate: lr=0.0001.
===> Epoch[3](30/324): Loss: 5.7374 || Learning rate: lr=0.0001.
===> Epoch[3](40/324): Loss: 11.0101 || Learning rate: lr=0.0001.
===> Epoch[3](50/324): Loss: 19.2399 || Learning rate: lr=0.0001.
===> Epoch[3](60/324): Loss: 10.1144 || Learning rate: lr=0.0001.
===> Epoch[3](70/324): Loss: 3.4061 || Learning rate: lr=0.0001.
===> Epoch[3](80/324): Loss: 14.8941 || Learning rate: lr=0.0001.
===> Epoch[3](90/324): Loss: 10.1574 || Learning rate: lr=0.0001.
===> Epoch[3](100/324): Loss: 12.4226 || Learning rate: lr=0.0001.
===> Epoch[3](110/324): Loss: 14.6617 || Learning rate: lr=0.0001.
===> Epoch[3](120/324): Loss: 4.2749 || Learning rate: lr=0.0001.
===> Epoch[3](130/324): Loss: 10.4128 || Learning rate: lr=0.0001.
===> Epoch[3](140/324): Loss: 11.8488 || Learning rate: lr=0.0001.
===> Epoch[3](150/324): Loss: 7.9664 || Learning rate: lr=0.0001.
===> Epoch[3](160/324): Loss: 7.2736 || Learning rate: lr=0.0001.
===> Epoch[3](170/324): Loss: 22.9996 || Learning rate: lr=0.0001.
===> Epoch[3](180/324): Loss: 16.1804 || Learning rate: lr=0.0001.
===> Epoch[3](190/324): Loss: 3.1309 || Learning rate: lr=0.0001.
===> Epoch[3](200/324): Loss: 8.9941 || Learning rate: lr=0.0001.
===> Epoch[3](210/324): Loss: 11.7042 || Learning rate: lr=0.0001.
===> Epoch[3](220/324): Loss: 8.8990 || Learning rate: lr=0.0001.
===> Epoch[3](230/324): Loss: 7.4690 || Learning rate: lr=0.0001.
===> Epoch[3](240/324): Loss: 11.5840 || Learning rate: lr=0.0001.
===> Epoch[3](250/324): Loss: 10.3355 || Learning rate: lr=0.0001.
===> Epoch[3](260/324): Loss: 14.7946 || Learning rate: lr=0.0001.
===> Epoch[3](270/324): Loss: 8.4248 || Learning rate: lr=0.0001.
===> Epoch[3](280/324): Loss: 4.1319 || Learning rate: lr=0.0001.
===> Epoch[3](290/324): Loss: 11.1575 || Learning rate: lr=0.0001.
===> Epoch[3](300/324): Loss: 3.4579 || Learning rate: lr=0.0001.
===> Epoch[3](310/324): Loss: 6.8812 || Learning rate: lr=0.0001.
===> Epoch[3](320/324): Loss: 7.3788 || Learning rate: lr=0.0001.
===> Epoch[4](10/324): Loss: 8.8952 || Learning rate: lr=0.0001.
===> Epoch[4](20/324): Loss: 4.0259 || Learning rate: lr=0.0001.
===> Epoch[4](30/324): Loss: 12.5255 || Learning rate: lr=0.0001.
===> Epoch[4](40/324): Loss: 4.5938 || Learning rate: lr=0.0001.
===> Epoch[4](50/324): Loss: 10.7231 || Learning rate: lr=0.0001.
===> Epoch[4](60/324): Loss: 8.7355 || Learning rate: lr=0.0001.
===> Epoch[4](70/324): Loss: 16.1399 || Learning rate: lr=0.0001.
===> Epoch[4](80/324): Loss: 8.0340 || Learning rate: lr=0.0001.
===> Epoch[4](90/324): Loss: 7.0706 || Learning rate: lr=0.0001.
===> Epoch[4](100/324): Loss: 8.4446 || Learning rate: lr=0.0001.
===> Epoch[4](110/324): Loss: 6.1637 || Learning rate: lr=0.0001.
===> Epoch[4](120/324): Loss: 11.5467 || Learning rate: lr=0.0001.
===> Epoch[4](130/324): Loss: 6.6657 || Learning rate: lr=0.0001.
===> Epoch[4](140/324): Loss: 11.1068 || Learning rate: lr=0.0001.
===> Epoch[4](150/324): Loss: 4.6451 || Learning rate: lr=0.0001.
===> Epoch[4](160/324): Loss: 9.2775 || Learning rate: lr=0.0001.
===> Epoch[4](170/324): Loss: 5.5886 || Learning rate: lr=0.0001.
===> Epoch[4](180/324): Loss: 4.2035 || Learning rate: lr=0.0001.
===> Epoch[4](190/324): Loss: 5.6904 || Learning rate: lr=0.0001.
===> Epoch[4](200/324): Loss: 2.8662 || Learning rate: lr=0.0001.
===> Epoch[4](210/324): Loss: 4.9663 || Learning rate: lr=0.0001.
===> Epoch[4](220/324): Loss: 11.2758 || Learning rate: lr=0.0001.
===> Epoch[4](230/324): Loss: 6.5622 || Learning rate: lr=0.0001.
===> Epoch[4](240/324): Loss: 6.4154 || Learning rate: lr=0.0001.
===> Epoch[4](250/324): Loss: 9.7223 || Learning rate: lr=0.0001.
===> Epoch[4](260/324): Loss: 5.6458 || Learning rate: lr=0.0001.
===> Epoch[4](270/324): Loss: 7.1526 || Learning rate: lr=0.0001.
===> Epoch[4](280/324): Loss: 7.5603 || Learning rate: lr=0.0001.
===> Epoch[4](290/324): Loss: 7.6532 || Learning rate: lr=0.0001.
===> Epoch[4](300/324): Loss: 4.7514 || Learning rate: lr=0.0001.
===> Epoch[4](310/324): Loss: 7.1941 || Learning rate: lr=0.0001.
===> Epoch[4](320/324): Loss: 6.9746 || Learning rate: lr=0.0001.
===> Epoch[5](10/324): Loss: 11.9329 || Learning rate: lr=0.0001.
===> Epoch[5](20/324): Loss: 12.2261 || Learning rate: lr=0.0001.
===> Epoch[5](30/324): Loss: 2.3212 || Learning rate: lr=0.0001.
===> Epoch[5](40/324): Loss: 5.7566 || Learning rate: lr=0.0001.
===> Epoch[5](50/324): Loss: 9.6639 || Learning rate: lr=0.0001.
===> Epoch[5](60/324): Loss: 7.7694 || Learning rate: lr=0.0001.
===> Epoch[5](70/324): Loss: 3.5717 || Learning rate: lr=0.0001.
===> Epoch[5](80/324): Loss: 5.1167 || Learning rate: lr=0.0001.
===> Epoch[5](90/324): Loss: 3.8056 || Learning rate: lr=0.0001.
===> Epoch[5](100/324): Loss: 6.5790 || Learning rate: lr=0.0001.
===> Epoch[5](110/324): Loss: 6.7460 || Learning rate: lr=0.0001.
===> Epoch[5](120/324): Loss: 3.7652 || Learning rate: lr=0.0001.
===> Epoch[5](130/324): Loss: 7.3694 || Learning rate: lr=0.0001.
===> Epoch[5](140/324): Loss: 10.1806 || Learning rate: lr=0.0001.
===> Epoch[5](150/324): Loss: 8.0520 || Learning rate: lr=0.0001.
===> Epoch[5](160/324): Loss: 16.1450 || Learning rate: lr=0.0001.
===> Epoch[5](170/324): Loss: 7.1768 || Learning rate: lr=0.0001.
===> Epoch[5](180/324): Loss: 6.1704 || Learning rate: lr=0.0001.
===> Epoch[5](190/324): Loss: 6.4729 || Learning rate: lr=0.0001.
===> Epoch[5](200/324): Loss: 7.1773 || Learning rate: lr=0.0001.
===> Epoch[5](210/324): Loss: 5.9357 || Learning rate: lr=0.0001.
===> Epoch[5](220/324): Loss: 11.8203 || Learning rate: lr=0.0001.
===> Epoch[5](230/324): Loss: 8.0084 || Learning rate: lr=0.0001.
===> Epoch[5](240/324): Loss: 12.9397 || Learning rate: lr=0.0001.
===> Epoch[5](250/324): Loss: 4.1723 || Learning rate: lr=0.0001.
===> Epoch[5](260/324): Loss: 5.2328 || Learning rate: lr=0.0001.
===> Epoch[5](270/324): Loss: 5.4492 || Learning rate: lr=0.0001.
===> Epoch[5](280/324): Loss: 5.8529 || Learning rate: lr=0.0001.
===> Epoch[5](290/324): Loss: 4.9631 || Learning rate: lr=0.0001.
===> Epoch[5](300/324): Loss: 4.1478 || Learning rate: lr=0.0001.
===> Epoch[5](310/324): Loss: 3.1283 || Learning rate: lr=0.0001.
===> Epoch[5](320/324): Loss: 3.7110 || Learning rate: lr=0.0001.
===> Epoch[6](10/324): Loss: 6.7766 || Learning rate: lr=0.0001.
===> Epoch[6](20/324): Loss: 2.8768 || Learning rate: lr=0.0001.
===> Epoch[6](30/324): Loss: 7.5780 || Learning rate: lr=0.0001.
===> Epoch[6](40/324): Loss: 3.6554 || Learning rate: lr=0.0001.
===> Epoch[6](50/324): Loss: 2.7179 || Learning rate: lr=0.0001.
===> Epoch[6](60/324): Loss: 4.2937 || Learning rate: lr=0.0001.
===> Epoch[6](70/324): Loss: 3.6336 || Learning rate: lr=0.0001.
===> Epoch[6](80/324): Loss: 6.9050 || Learning rate: lr=0.0001.
===> Epoch[6](90/324): Loss: 4.9206 || Learning rate: lr=0.0001.
===> Epoch[6](100/324): Loss: 9.6347 || Learning rate: lr=0.0001.
===> Epoch[6](110/324): Loss: 9.9496 || Learning rate: lr=0.0001.
===> Epoch[6](120/324): Loss: 4.7836 || Learning rate: lr=0.0001.
===> Epoch[6](130/324): Loss: 2.2576 || Learning rate: lr=0.0001.
===> Epoch[6](140/324): Loss: 3.5928 || Learning rate: lr=0.0001.
===> Epoch[6](150/324): Loss: 6.7105 || Learning rate: lr=0.0001.
===> Epoch[6](160/324): Loss: 6.8618 || Learning rate: lr=0.0001.
===> Epoch[6](170/324): Loss: 3.9910 || Learning rate: lr=0.0001.
===> Epoch[6](180/324): Loss: 12.5123 || Learning rate: lr=0.0001.
===> Epoch[6](190/324): Loss: 15.5068 || Learning rate: lr=0.0001.
===> Epoch[6](200/324): Loss: 9.9148 || Learning rate: lr=0.0001.
===> Epoch[6](210/324): Loss: 21.9537 || Learning rate: lr=0.0001.
===> Epoch[6](220/324): Loss: 15.3595 || Learning rate: lr=0.0001.
===> Epoch[6](230/324): Loss: 6.6361 || Learning rate: lr=0.0001.
===> Epoch[6](240/324): Loss: 12.3312 || Learning rate: lr=0.0001.
===> Epoch[6](250/324): Loss: 7.9499 || Learning rate: lr=0.0001.
===> Epoch[6](260/324): Loss: 3.0134 || Learning rate: lr=0.0001.
===> Epoch[6](270/324): Loss: 4.5851 || Learning rate: lr=0.0001.
===> Epoch[6](280/324): Loss: 2.2527 || Learning rate: lr=0.0001.
===> Epoch[6](290/324): Loss: 6.8473 || Learning rate: lr=0.0001.
===> Epoch[6](300/324): Loss: 4.5089 || Learning rate: lr=0.0001.
===> Epoch[6](310/324): Loss: 3.1847 || Learning rate: lr=0.0001.
===> Epoch[6](320/324): Loss: 7.5421 || Learning rate: lr=0.0001.
===> Epoch[7](10/324): Loss: 11.2559 || Learning rate: lr=0.0001.
===> Epoch[7](20/324): Loss: 6.0346 || Learning rate: lr=0.0001.
===> Epoch[7](30/324): Loss: 3.7955 || Learning rate: lr=0.0001.
===> Epoch[7](40/324): Loss: 2.6809 || Learning rate: lr=0.0001.
===> Epoch[7](50/324): Loss: 3.5751 || Learning rate: lr=0.0001.
===> Epoch[7](60/324): Loss: 10.3719 || Learning rate: lr=0.0001.
===> Epoch[7](70/324): Loss: 3.8939 || Learning rate: lr=0.0001.
===> Epoch[7](80/324): Loss: 2.9325 || Learning rate: lr=0.0001.
===> Epoch[7](90/324): Loss: 6.9710 || Learning rate: lr=0.0001.
===> Epoch[7](100/324): Loss: 6.8884 || Learning rate: lr=0.0001.
===> Epoch[7](110/324): Loss: 3.0560 || Learning rate: lr=0.0001.
===> Epoch[7](120/324): Loss: 7.8320 || Learning rate: lr=0.0001.
===> Epoch[7](130/324): Loss: 3.8521 || Learning rate: lr=0.0001.
===> Epoch[7](140/324): Loss: 3.2163 || Learning rate: lr=0.0001.
===> Epoch[7](150/324): Loss: 5.5049 || Learning rate: lr=0.0001.
===> Epoch[7](160/324): Loss: 4.6484 || Learning rate: lr=0.0001.
===> Epoch[7](170/324): Loss: 3.3699 || Learning rate: lr=0.0001.
===> Epoch[7](180/324): Loss: 3.8699 || Learning rate: lr=0.0001.
===> Epoch[7](190/324): Loss: 2.1999 || Learning rate: lr=0.0001.
===> Epoch[7](200/324): Loss: 4.1991 || Learning rate: lr=0.0001.
===> Epoch[7](210/324): Loss: 5.3729 || Learning rate: lr=0.0001.
===> Epoch[7](220/324): Loss: 2.5886 || Learning rate: lr=0.0001.
===> Epoch[7](230/324): Loss: 2.3513 || Learning rate: lr=0.0001.
===> Epoch[7](240/324): Loss: 3.9811 || Learning rate: lr=0.0001.
===> Epoch[7](250/324): Loss: 3.3294 || Learning rate: lr=0.0001.
===> Epoch[7](260/324): Loss: 6.6288 || Learning rate: lr=0.0001.
===> Epoch[7](270/324): Loss: 4.2714 || Learning rate: lr=0.0001.
===> Epoch[7](280/324): Loss: 6.5665 || Learning rate: lr=0.0001.
===> Epoch[7](290/324): Loss: 2.7479 || Learning rate: lr=0.0001.
===> Epoch[7](300/324): Loss: 4.6609 || Learning rate: lr=0.0001.
===> Epoch[7](310/324): Loss: 3.1994 || Learning rate: lr=0.0001.
===> Epoch[7](320/324): Loss: 4.2498 || Learning rate: lr=0.0001.
===> Epoch[8](10/324): Loss: 6.2305 || Learning rate: lr=0.0001.
===> Epoch[8](20/324): Loss: 2.0420 || Learning rate: lr=0.0001.
===> Epoch[8](30/324): Loss: 4.9560 || Learning rate: lr=0.0001.
===> Epoch[8](40/324): Loss: 8.1776 || Learning rate: lr=0.0001.
===> Epoch[8](50/324): Loss: 4.6509 || Learning rate: lr=0.0001.
===> Epoch[8](60/324): Loss: 5.2322 || Learning rate: lr=0.0001.
===> Epoch[8](70/324): Loss: 4.5982 || Learning rate: lr=0.0001.
===> Epoch[8](80/324): Loss: 2.4741 || Learning rate: lr=0.0001.
===> Epoch[8](90/324): Loss: 2.4467 || Learning rate: lr=0.0001.
===> Epoch[8](100/324): Loss: 3.5398 || Learning rate: lr=0.0001.
===> Epoch[8](110/324): Loss: 1.0587 || Learning rate: lr=0.0001.
===> Epoch[8](120/324): Loss: 2.9828 || Learning rate: lr=0.0001.
===> Epoch[8](130/324): Loss: 2.9087 || Learning rate: lr=0.0001.
===> Epoch[8](140/324): Loss: 4.8706 || Learning rate: lr=0.0001.
===> Epoch[8](150/324): Loss: 4.3200 || Learning rate: lr=0.0001.
===> Epoch[8](160/324): Loss: 5.2950 || Learning rate: lr=0.0001.
===> Epoch[8](170/324): Loss: 10.4215 || Learning rate: lr=0.0001.
===> Epoch[8](180/324): Loss: 2.9780 || Learning rate: lr=0.0001.
===> Epoch[8](190/324): Loss: 7.5952 || Learning rate: lr=0.0001.
===> Epoch[8](200/324): Loss: 9.1753 || Learning rate: lr=0.0001.
===> Epoch[8](210/324): Loss: 5.1409 || Learning rate: lr=0.0001.
===> Epoch[8](220/324): Loss: 2.8813 || Learning rate: lr=0.0001.
===> Epoch[8](230/324): Loss: 5.1457 || Learning rate: lr=0.0001.
===> Epoch[8](240/324): Loss: 2.3884 || Learning rate: lr=0.0001.
===> Epoch[8](250/324): Loss: 2.7990 || Learning rate: lr=0.0001.
===> Epoch[8](260/324): Loss: 5.0028 || Learning rate: lr=0.0001.
===> Epoch[8](270/324): Loss: 2.6217 || Learning rate: lr=0.0001.
===> Epoch[8](280/324): Loss: 2.1117 || Learning rate: lr=0.0001.
===> Epoch[8](290/324): Loss: 5.0208 || Learning rate: lr=0.0001.
===> Epoch[8](300/324): Loss: 6.0493 || Learning rate: lr=0.0001.
===> Epoch[8](310/324): Loss: 1.8861 || Learning rate: lr=0.0001.
===> Epoch[8](320/324): Loss: 1.5106 || Learning rate: lr=0.0001.
===> Epoch[9](10/324): Loss: 2.0696 || Learning rate: lr=0.0001.
===> Epoch[9](20/324): Loss: 2.8051 || Learning rate: lr=0.0001.
===> Epoch[9](30/324): Loss: 2.0687 || Learning rate: lr=0.0001.
===> Epoch[9](40/324): Loss: 7.3546 || Learning rate: lr=0.0001.
===> Epoch[9](50/324): Loss: 9.7914 || Learning rate: lr=0.0001.
===> Epoch[9](60/324): Loss: 4.6806 || Learning rate: lr=0.0001.
===> Epoch[9](70/324): Loss: 4.2901 || Learning rate: lr=0.0001.
===> Epoch[9](80/324): Loss: 2.5207 || Learning rate: lr=0.0001.
===> Epoch[9](90/324): Loss: 3.3881 || Learning rate: lr=0.0001.
===> Epoch[9](100/324): Loss: 2.2578 || Learning rate: lr=0.0001.
===> Epoch[9](110/324): Loss: 4.5440 || Learning rate: lr=0.0001.
===> Epoch[9](120/324): Loss: 3.6579 || Learning rate: lr=0.0001.
===> Epoch[9](130/324): Loss: 5.4432 || Learning rate: lr=0.0001.
===> Epoch[9](140/324): Loss: 3.1836 || Learning rate: lr=0.0001.
===> Epoch[9](150/324): Loss: 1.8684 || Learning rate: lr=0.0001.
===> Epoch[9](160/324): Loss: 3.7663 || Learning rate: lr=0.0001.
===> Epoch[9](170/324): Loss: 2.9055 || Learning rate: lr=0.0001.
===> Epoch[9](180/324): Loss: 4.6802 || Learning rate: lr=0.0001.
===> Epoch[9](190/324): Loss: 5.8952 || Learning rate: lr=0.0001.
===> Epoch[9](200/324): Loss: 3.6071 || Learning rate: lr=0.0001.
===> Epoch[9](210/324): Loss: 2.1914 || Learning rate: lr=0.0001.
===> Epoch[9](220/324): Loss: 3.2873 || Learning rate: lr=0.0001.
===> Epoch[9](230/324): Loss: 2.4072 || Learning rate: lr=0.0001.
===> Epoch[9](240/324): Loss: 3.4553 || Learning rate: lr=0.0001.
===> Epoch[9](250/324): Loss: 3.0453 || Learning rate: lr=0.0001.
===> Epoch[9](260/324): Loss: 5.1753 || Learning rate: lr=0.0001.
===> Epoch[9](270/324): Loss: 3.2139 || Learning rate: lr=0.0001.
===> Epoch[9](280/324): Loss: 1.4702 || Learning rate: lr=0.0001.
===> Epoch[9](290/324): Loss: 5.0301 || Learning rate: lr=0.0001.
===> Epoch[9](300/324): Loss: 1.2977 || Learning rate: lr=0.0001.
===> Epoch[9](310/324): Loss: 5.7691 || Learning rate: lr=0.0001.
===> Epoch[9](320/324): Loss: 3.8637 || Learning rate: lr=0.0001.
===> Epoch[10](10/324): Loss: 2.0367 || Learning rate: lr=0.0001.
===> Epoch[10](20/324): Loss: 4.8837 || Learning rate: lr=0.0001.
===> Epoch[10](30/324): Loss: 3.6945 || Learning rate: lr=0.0001.
===> Epoch[10](40/324): Loss: 2.0313 || Learning rate: lr=0.0001.
===> Epoch[10](50/324): Loss: 3.5846 || Learning rate: lr=0.0001.
===> Epoch[10](60/324): Loss: 5.2325 || Learning rate: lr=0.0001.
===> Epoch[10](70/324): Loss: 5.3578 || Learning rate: lr=0.0001.
===> Epoch[10](80/324): Loss: 1.5579 || Learning rate: lr=0.0001.
===> Epoch[10](90/324): Loss: 2.9268 || Learning rate: lr=0.0001.
===> Epoch[10](100/324): Loss: 4.1639 || Learning rate: lr=0.0001.
===> Epoch[10](110/324): Loss: 5.2880 || Learning rate: lr=0.0001.
===> Epoch[10](120/324): Loss: 3.3613 || Learning rate: lr=0.0001.
===> Epoch[10](130/324): Loss: 2.6868 || Learning rate: lr=0.0001.
===> Epoch[10](140/324): Loss: 2.2003 || Learning rate: lr=0.0001.
===> Epoch[10](150/324): Loss: 3.6508 || Learning rate: lr=0.0001.
===> Epoch[10](160/324): Loss: 6.2169 || Learning rate: lr=0.0001.
===> Epoch[10](170/324): Loss: 2.5547 || Learning rate: lr=0.0001.
===> Epoch[10](180/324): Loss: 1.9918 || Learning rate: lr=0.0001.
===> Epoch[10](190/324): Loss: 1.7175 || Learning rate: lr=0.0001.
===> Epoch[10](200/324): Loss: 2.0334 || Learning rate: lr=0.0001.
===> Epoch[10](210/324): Loss: 3.3035 || Learning rate: lr=0.0001.
===> Epoch[10](220/324): Loss: 3.2397 || Learning rate: lr=0.0001.
===> Epoch[10](230/324): Loss: 3.9267 || Learning rate: lr=0.0001.
===> Epoch[10](240/324): Loss: 1.8060 || Learning rate: lr=0.0001.
===> Epoch[10](250/324): Loss: 4.0025 || Learning rate: lr=0.0001.
===> Epoch[10](260/324): Loss: 4.5429 || Learning rate: lr=0.0001.
===> Epoch[10](270/324): Loss: 2.4344 || Learning rate: lr=0.0001.
===> Epoch[10](280/324): Loss: 3.8894 || Learning rate: lr=0.0001.
===> Epoch[10](290/324): Loss: 3.2564 || Learning rate: lr=0.0001.
===> Epoch[10](300/324): Loss: 2.9073 || Learning rate: lr=0.0001.
===> Epoch[10](310/324): Loss: 2.9317 || Learning rate: lr=0.0001.
===> Epoch[10](320/324): Loss: 3.1098 || Learning rate: lr=0.0001.
===> Epoch[11](10/324): Loss: 3.2389 || Learning rate: lr=0.0001.
===> Epoch[11](20/324): Loss: 8.5098 || Learning rate: lr=0.0001.
===> Epoch[11](30/324): Loss: 8.3919 || Learning rate: lr=0.0001.
===> Epoch[11](40/324): Loss: 5.5579 || Learning rate: lr=0.0001.
===> Epoch[11](50/324): Loss: 7.4845 || Learning rate: lr=0.0001.
===> Epoch[11](60/324): Loss: 10.1478 || Learning rate: lr=0.0001.
===> Epoch[11](70/324): Loss: 4.6780 || Learning rate: lr=0.0001.
===> Epoch[11](80/324): Loss: 3.3747 || Learning rate: lr=0.0001.
===> Epoch[11](90/324): Loss: 2.2011 || Learning rate: lr=0.0001.
===> Epoch[11](100/324): Loss: 1.8845 || Learning rate: lr=0.0001.
===> Epoch[11](110/324): Loss: 2.3209 || Learning rate: lr=0.0001.
===> Epoch[11](120/324): Loss: 2.4554 || Learning rate: lr=0.0001.
===> Epoch[11](130/324): Loss: 1.5423 || Learning rate: lr=0.0001.
===> Epoch[11](140/324): Loss: 8.6560 || Learning rate: lr=0.0001.
===> Epoch[11](150/324): Loss: 2.8771 || Learning rate: lr=0.0001.
===> Epoch[11](160/324): Loss: 2.4698 || Learning rate: lr=0.0001.
===> Epoch[11](170/324): Loss: 1.7549 || Learning rate: lr=0.0001.
===> Epoch[11](180/324): Loss: 3.8428 || Learning rate: lr=0.0001.
===> Epoch[11](190/324): Loss: 2.1997 || Learning rate: lr=0.0001.
===> Epoch[11](200/324): Loss: 2.8655 || Learning rate: lr=0.0001.
===> Epoch[11](210/324): Loss: 3.2317 || Learning rate: lr=0.0001.
===> Epoch[11](220/324): Loss: 3.6427 || Learning rate: lr=0.0001.
===> Epoch[11](230/324): Loss: 1.8126 || Learning rate: lr=0.0001.
===> Epoch[11](240/324): Loss: 2.1245 || Learning rate: lr=0.0001.
===> Epoch[11](250/324): Loss: 2.6809 || Learning rate: lr=0.0001.
===> Epoch[11](260/324): Loss: 2.7310 || Learning rate: lr=0.0001.
===> Epoch[11](270/324): Loss: 3.0586 || Learning rate: lr=0.0001.
===> Epoch[11](280/324): Loss: 3.3266 || Learning rate: lr=0.0001.
===> Epoch[11](290/324): Loss: 5.0812 || Learning rate: lr=0.0001.
===> Epoch[11](300/324): Loss: 2.3304 || Learning rate: lr=0.0001.
===> Epoch[11](310/324): Loss: 2.1144 || Learning rate: lr=0.0001.
===> Epoch[11](320/324): Loss: 3.8005 || Learning rate: lr=0.0001.
===> Epoch[12](10/324): Loss: 3.1734 || Learning rate: lr=0.0001.
===> Epoch[12](20/324): Loss: 4.7987 || Learning rate: lr=0.0001.
===> Epoch[12](30/324): Loss: 3.1683 || Learning rate: lr=0.0001.
===> Epoch[12](40/324): Loss: 3.8521 || Learning rate: lr=0.0001.
===> Epoch[12](50/324): Loss: 3.9352 || Learning rate: lr=0.0001.
===> Epoch[12](60/324): Loss: 7.7731 || Learning rate: lr=0.0001.
===> Epoch[12](70/324): Loss: 2.9198 || Learning rate: lr=0.0001.
===> Epoch[12](80/324): Loss: 3.8813 || Learning rate: lr=0.0001.
===> Epoch[12](90/324): Loss: 3.1617 || Learning rate: lr=0.0001.
===> Epoch[12](100/324): Loss: 4.1214 || Learning rate: lr=0.0001.
===> Epoch[12](110/324): Loss: 2.3455 || Learning rate: lr=0.0001.
===> Epoch[12](120/324): Loss: 8.2985 || Learning rate: lr=0.0001.
===> Epoch[12](130/324): Loss: 3.2142 || Learning rate: lr=0.0001.
===> Epoch[12](140/324): Loss: 1.8219 || Learning rate: lr=0.0001.
===> Epoch[12](150/324): Loss: 2.6097 || Learning rate: lr=0.0001.
===> Epoch[12](160/324): Loss: 1.6136 || Learning rate: lr=0.0001.
===> Epoch[12](170/324): Loss: 3.9287 || Learning rate: lr=0.0001.
===> Epoch[12](180/324): Loss: 2.2444 || Learning rate: lr=0.0001.
===> Epoch[12](190/324): Loss: 3.5288 || Learning rate: lr=0.0001.
===> Epoch[12](200/324): Loss: 4.1729 || Learning rate: lr=0.0001.
===> Epoch[12](210/324): Loss: 1.7219 || Learning rate: lr=0.0001.
===> Epoch[12](220/324): Loss: 4.0271 || Learning rate: lr=0.0001.
===> Epoch[12](230/324): Loss: 2.6504 || Learning rate: lr=0.0001.
===> Epoch[12](240/324): Loss: 2.7287 || Learning rate: lr=0.0001.
===> Epoch[12](250/324): Loss: 2.1017 || Learning rate: lr=0.0001.
===> Epoch[12](260/324): Loss: 4.7383 || Learning rate: lr=0.0001.
===> Epoch[12](270/324): Loss: 3.1650 || Learning rate: lr=0.0001.
===> Epoch[12](280/324): Loss: 2.1237 || Learning rate: lr=0.0001.
===> Epoch[12](290/324): Loss: 3.0101 || Learning rate: lr=0.0001.
===> Epoch[12](300/324): Loss: 1.4167 || Learning rate: lr=0.0001.
===> Epoch[12](310/324): Loss: 2.3010 || Learning rate: lr=0.0001.
===> Epoch[12](320/324): Loss: 1.2196 || Learning rate: lr=0.0001.
===> Epoch[13](10/324): Loss: 0.9532 || Learning rate: lr=0.0001.
===> Epoch[13](20/324): Loss: 2.2443 || Learning rate: lr=0.0001.
===> Epoch[13](30/324): Loss: 2.8846 || Learning rate: lr=0.0001.
===> Epoch[13](40/324): Loss: 3.2955 || Learning rate: lr=0.0001.
===> Epoch[13](50/324): Loss: 1.4048 || Learning rate: lr=0.0001.
===> Epoch[13](60/324): Loss: 1.9974 || Learning rate: lr=0.0001.
===> Epoch[13](70/324): Loss: 1.9676 || Learning rate: lr=0.0001.
===> Epoch[13](80/324): Loss: 2.1931 || Learning rate: lr=0.0001.
===> Epoch[13](90/324): Loss: 3.5524 || Learning rate: lr=0.0001.
===> Epoch[13](100/324): Loss: 1.9266 || Learning rate: lr=0.0001.
===> Epoch[13](110/324): Loss: 3.4411 || Learning rate: lr=0.0001.
===> Epoch[13](120/324): Loss: 5.8514 || Learning rate: lr=0.0001.
===> Epoch[13](130/324): Loss: 9.2450 || Learning rate: lr=0.0001.
===> Epoch[13](140/324): Loss: 8.9619 || Learning rate: lr=0.0001.
===> Epoch[13](150/324): Loss: 3.9436 || Learning rate: lr=0.0001.
===> Epoch[13](160/324): Loss: 8.2481 || Learning rate: lr=0.0001.
===> Epoch[13](170/324): Loss: 8.8695 || Learning rate: lr=0.0001.
===> Epoch[13](180/324): Loss: 4.3949 || Learning rate: lr=0.0001.
===> Epoch[13](190/324): Loss: 8.6673 || Learning rate: lr=0.0001.
===> Epoch[13](200/324): Loss: 3.5963 || Learning rate: lr=0.0001.
===> Epoch[13](210/324): Loss: 8.6781 || Learning rate: lr=0.0001.
===> Epoch[13](220/324): Loss: 3.4432 || Learning rate: lr=0.0001.
===> Epoch[13](230/324): Loss: 5.8887 || Learning rate: lr=0.0001.
===> Epoch[13](240/324): Loss: 3.6827 || Learning rate: lr=0.0001.
===> Epoch[13](250/324): Loss: 2.1488 || Learning rate: lr=0.0001.
===> Epoch[13](260/324): Loss: 4.2303 || Learning rate: lr=0.0001.
===> Epoch[13](270/324): Loss: 4.9423 || Learning rate: lr=0.0001.
===> Epoch[13](280/324): Loss: 1.2985 || Learning rate: lr=0.0001.
===> Epoch[13](290/324): Loss: 4.0267 || Learning rate: lr=0.0001.
===> Epoch[13](300/324): Loss: 2.7209 || Learning rate: lr=0.0001.
===> Epoch[13](310/324): Loss: 1.7293 || Learning rate: lr=0.0001.
===> Epoch[13](320/324): Loss: 1.4834 || Learning rate: lr=0.0001.
===> Epoch[14](10/324): Loss: 3.2042 || Learning rate: lr=0.0001.
===> Epoch[14](20/324): Loss: 3.1678 || Learning rate: lr=0.0001.
===> Epoch[14](30/324): Loss: 1.6450 || Learning rate: lr=0.0001.
===> Epoch[14](40/324): Loss: 1.8603 || Learning rate: lr=0.0001.
===> Epoch[14](50/324): Loss: 3.5152 || Learning rate: lr=0.0001.
===> Epoch[14](60/324): Loss: 2.3846 || Learning rate: lr=0.0001.
===> Epoch[14](70/324): Loss: 2.5712 || Learning rate: lr=0.0001.
===> Epoch[14](80/324): Loss: 3.7064 || Learning rate: lr=0.0001.
===> Epoch[14](90/324): Loss: 4.2904 || Learning rate: lr=0.0001.
===> Epoch[14](100/324): Loss: 2.3875 || Learning rate: lr=0.0001.
===> Epoch[14](110/324): Loss: 1.9249 || Learning rate: lr=0.0001.
===> Epoch[14](120/324): Loss: 4.1103 || Learning rate: lr=0.0001.
===> Epoch[14](130/324): Loss: 1.9359 || Learning rate: lr=0.0001.
===> Epoch[14](140/324): Loss: 2.8040 || Learning rate: lr=0.0001.
===> Epoch[14](150/324): Loss: 2.6472 || Learning rate: lr=0.0001.
===> Epoch[14](160/324): Loss: 2.9689 || Learning rate: lr=0.0001.
===> Epoch[14](170/324): Loss: 2.2661 || Learning rate: lr=0.0001.
===> Epoch[14](180/324): Loss: 1.7624 || Learning rate: lr=0.0001.
===> Epoch[14](190/324): Loss: 1.9862 || Learning rate: lr=0.0001.
===> Epoch[14](200/324): Loss: 1.3877 || Learning rate: lr=0.0001.
===> Epoch[14](210/324): Loss: 1.9309 || Learning rate: lr=0.0001.
===> Epoch[14](220/324): Loss: 2.3510 || Learning rate: lr=0.0001.
===> Epoch[14](230/324): Loss: 4.4804 || Learning rate: lr=0.0001.
===> Epoch[14](240/324): Loss: 2.4165 || Learning rate: lr=0.0001.
===> Epoch[14](250/324): Loss: 1.8773 || Learning rate: lr=0.0001.
===> Epoch[14](260/324): Loss: 3.7172 || Learning rate: lr=0.0001.
===> Epoch[14](270/324): Loss: 2.9185 || Learning rate: lr=0.0001.
===> Epoch[14](280/324): Loss: 1.9286 || Learning rate: lr=0.0001.
===> Epoch[14](290/324): Loss: 2.7405 || Learning rate: lr=0.0001.
===> Epoch[14](300/324): Loss: 2.9267 || Learning rate: lr=0.0001.
===> Epoch[14](310/324): Loss: 1.9513 || Learning rate: lr=0.0001.
===> Epoch[14](320/324): Loss: 3.0571 || Learning rate: lr=0.0001.
===> Epoch[15](10/324): Loss: 3.5699 || Learning rate: lr=0.0001.
===> Epoch[15](20/324): Loss: 1.6313 || Learning rate: lr=0.0001.
===> Epoch[15](30/324): Loss: 1.7624 || Learning rate: lr=0.0001.
===> Epoch[15](40/324): Loss: 3.5282 || Learning rate: lr=0.0001.
===> Epoch[15](50/324): Loss: 3.1729 || Learning rate: lr=0.0001.
===> Epoch[15](60/324): Loss: 3.0893 || Learning rate: lr=0.0001.
===> Epoch[15](70/324): Loss: 2.9030 || Learning rate: lr=0.0001.
===> Epoch[15](80/324): Loss: 3.7245 || Learning rate: lr=0.0001.
===> Epoch[15](90/324): Loss: 1.2527 || Learning rate: lr=0.0001.
===> Epoch[15](100/324): Loss: 1.2379 || Learning rate: lr=0.0001.
===> Epoch[15](110/324): Loss: 1.2817 || Learning rate: lr=0.0001.
===> Epoch[15](120/324): Loss: 1.1292 || Learning rate: lr=0.0001.
===> Epoch[15](130/324): Loss: 0.7338 || Learning rate: lr=0.0001.
===> Epoch[15](140/324): Loss: 5.0350 || Learning rate: lr=0.0001.
===> Epoch[15](150/324): Loss: 1.5833 || Learning rate: lr=0.0001.
===> Epoch[15](160/324): Loss: 2.3957 || Learning rate: lr=0.0001.
===> Epoch[15](170/324): Loss: 2.3222 || Learning rate: lr=0.0001.
===> Epoch[15](180/324): Loss: 2.0786 || Learning rate: lr=0.0001.
===> Epoch[15](190/324): Loss: 3.1734 || Learning rate: lr=0.0001.
===> Epoch[15](200/324): Loss: 2.3889 || Learning rate: lr=0.0001.
===> Epoch[15](210/324): Loss: 1.3866 || Learning rate: lr=0.0001.
===> Epoch[15](220/324): Loss: 2.3261 || Learning rate: lr=0.0001.
===> Epoch[15](230/324): Loss: 3.0790 || Learning rate: lr=0.0001.
===> Epoch[15](240/324): Loss: 1.4685 || Learning rate: lr=0.0001.
===> Epoch[15](250/324): Loss: 1.8254 || Learning rate: lr=0.0001.
===> Epoch[15](260/324): Loss: 2.0153 || Learning rate: lr=0.0001.
===> Epoch[15](270/324): Loss: 1.3492 || Learning rate: lr=0.0001.
===> Epoch[15](280/324): Loss: 3.8962 || Learning rate: lr=0.0001.
===> Epoch[15](290/324): Loss: 2.2902 || Learning rate: lr=0.0001.
===> Epoch[15](300/324): Loss: 1.1405 || Learning rate: lr=0.0001.
===> Epoch[15](310/324): Loss: 1.8755 || Learning rate: lr=0.0001.
===> Epoch[15](320/324): Loss: 2.4417 || Learning rate: lr=0.0001.
===> Epoch[16](10/324): Loss: 2.5488 || Learning rate: lr=0.0001.
===> Epoch[16](20/324): Loss: 1.0791 || Learning rate: lr=0.0001.
===> Epoch[16](30/324): Loss: 3.5645 || Learning rate: lr=0.0001.
===> Epoch[16](40/324): Loss: 1.0278 || Learning rate: lr=0.0001.
===> Epoch[16](50/324): Loss: 1.8308 || Learning rate: lr=0.0001.
===> Epoch[16](60/324): Loss: 3.0406 || Learning rate: lr=0.0001.
===> Epoch[16](70/324): Loss: 1.4297 || Learning rate: lr=0.0001.
===> Epoch[16](80/324): Loss: 2.6602 || Learning rate: lr=0.0001.
===> Epoch[16](90/324): Loss: 3.4592 || Learning rate: lr=0.0001.
===> Epoch[16](100/324): Loss: 1.4551 || Learning rate: lr=0.0001.
===> Epoch[16](110/324): Loss: 1.9862 || Learning rate: lr=0.0001.
===> Epoch[16](120/324): Loss: 1.6570 || Learning rate: lr=0.0001.
===> Epoch[16](130/324): Loss: 1.4538 || Learning rate: lr=0.0001.
===> Epoch[16](140/324): Loss: 1.7058 || Learning rate: lr=0.0001.
===> Epoch[16](150/324): Loss: 3.1765 || Learning rate: lr=0.0001.
===> Epoch[16](160/324): Loss: 3.7164 || Learning rate: lr=0.0001.
===> Epoch[16](170/324): Loss: 2.9930 || Learning rate: lr=0.0001.
===> Epoch[16](180/324): Loss: 2.6778 || Learning rate: lr=0.0001.
===> Epoch[16](190/324): Loss: 3.6557 || Learning rate: lr=0.0001.
===> Epoch[16](200/324): Loss: 1.6288 || Learning rate: lr=0.0001.
===> Epoch[16](210/324): Loss: 4.1603 || Learning rate: lr=0.0001.
===> Epoch[16](220/324): Loss: 7.2296 || Learning rate: lr=0.0001.
===> Epoch[16](230/324): Loss: 5.9323 || Learning rate: lr=0.0001.
===> Epoch[16](240/324): Loss: 3.0785 || Learning rate: lr=0.0001.
===> Epoch[16](250/324): Loss: 3.6369 || Learning rate: lr=0.0001.
===> Epoch[16](260/324): Loss: 1.9790 || Learning rate: lr=0.0001.
===> Epoch[16](270/324): Loss: 4.3037 || Learning rate: lr=0.0001.
===> Epoch[16](280/324): Loss: 1.3008 || Learning rate: lr=0.0001.
===> Epoch[16](290/324): Loss: 2.4256 || Learning rate: lr=0.0001.
===> Epoch[16](300/324): Loss: 3.0485 || Learning rate: lr=0.0001.
===> Epoch[16](310/324): Loss: 4.3065 || Learning rate: lr=0.0001.
===> Epoch[16](320/324): Loss: 3.4704 || Learning rate: lr=0.0001.
===> Epoch[17](10/324): Loss: 2.8740 || Learning rate: lr=0.0001.
===> Epoch[17](20/324): Loss: 1.1708 || Learning rate: lr=0.0001.
===> Epoch[17](30/324): Loss: 2.3479 || Learning rate: lr=0.0001.
===> Epoch[17](40/324): Loss: 2.3082 || Learning rate: lr=0.0001.
===> Epoch[17](50/324): Loss: 2.0922 || Learning rate: lr=0.0001.
===> Epoch[17](60/324): Loss: 1.6221 || Learning rate: lr=0.0001.
===> Epoch[17](70/324): Loss: 2.1564 || Learning rate: lr=0.0001.
===> Epoch[17](80/324): Loss: 1.4119 || Learning rate: lr=0.0001.
===> Epoch[17](90/324): Loss: 1.9319 || Learning rate: lr=0.0001.
===> Epoch[17](100/324): Loss: 1.8979 || Learning rate: lr=0.0001.
===> Epoch[17](110/324): Loss: 1.2503 || Learning rate: lr=0.0001.
===> Epoch[17](120/324): Loss: 2.1559 || Learning rate: lr=0.0001.
===> Epoch[17](130/324): Loss: 1.7865 || Learning rate: lr=0.0001.
===> Epoch[17](140/324): Loss: 1.1369 || Learning rate: lr=0.0001.
===> Epoch[17](150/324): Loss: 4.3945 || Learning rate: lr=0.0001.
===> Epoch[17](160/324): Loss: 1.7826 || Learning rate: lr=0.0001.
===> Epoch[17](170/324): Loss: 1.3892 || Learning rate: lr=0.0001.
===> Epoch[17](180/324): Loss: 2.1638 || Learning rate: lr=0.0001.
===> Epoch[17](190/324): Loss: 4.8892 || Learning rate: lr=0.0001.
===> Epoch[17](200/324): Loss: 1.9701 || Learning rate: lr=0.0001.
===> Epoch[17](210/324): Loss: 1.1577 || Learning rate: lr=0.0001.
===> Epoch[17](220/324): Loss: 2.8524 || Learning rate: lr=0.0001.
===> Epoch[17](230/324): Loss: 3.3775 || Learning rate: lr=0.0001.
===> Epoch[17](240/324): Loss: 2.6570 || Learning rate: lr=0.0001.
===> Epoch[17](250/324): Loss: 1.4462 || Learning rate: lr=0.0001.
===> Epoch[17](260/324): Loss: 1.8679 || Learning rate: lr=0.0001.
===> Epoch[17](270/324): Loss: 3.2924 || Learning rate: lr=0.0001.
===> Epoch[17](280/324): Loss: 2.4366 || Learning rate: lr=0.0001.
===> Epoch[17](290/324): Loss: 2.7701 || Learning rate: lr=0.0001.
===> Epoch[17](300/324): Loss: 2.1980 || Learning rate: lr=0.0001.
===> Epoch[17](310/324): Loss: 2.2668 || Learning rate: lr=0.0001.
===> Epoch[17](320/324): Loss: 1.5270 || Learning rate: lr=0.0001.
===> Epoch[18](10/324): Loss: 1.9937 || Learning rate: lr=0.0001.
===> Epoch[18](20/324): Loss: 1.3493 || Learning rate: lr=0.0001.
===> Epoch[18](30/324): Loss: 1.4130 || Learning rate: lr=0.0001.
===> Epoch[18](40/324): Loss: 3.0717 || Learning rate: lr=0.0001.
===> Epoch[18](50/324): Loss: 2.0543 || Learning rate: lr=0.0001.
===> Epoch[18](60/324): Loss: 1.1764 || Learning rate: lr=0.0001.
===> Epoch[18](70/324): Loss: 4.6988 || Learning rate: lr=0.0001.
===> Epoch[18](80/324): Loss: 2.4099 || Learning rate: lr=0.0001.
===> Epoch[18](90/324): Loss: 1.5292 || Learning rate: lr=0.0001.
===> Epoch[18](100/324): Loss: 1.5845 || Learning rate: lr=0.0001.
===> Epoch[18](110/324): Loss: 3.5617 || Learning rate: lr=0.0001.
===> Epoch[18](120/324): Loss: 1.3818 || Learning rate: lr=0.0001.
===> Epoch[18](130/324): Loss: 1.8075 || Learning rate: lr=0.0001.
===> Epoch[18](140/324): Loss: 1.2770 || Learning rate: lr=0.0001.
===> Epoch[18](150/324): Loss: 2.1907 || Learning rate: lr=0.0001.
===> Epoch[18](160/324): Loss: 1.5113 || Learning rate: lr=0.0001.
===> Epoch[18](170/324): Loss: 2.1658 || Learning rate: lr=0.0001.
===> Epoch[18](180/324): Loss: 1.8943 || Learning rate: lr=0.0001.
===> Epoch[18](190/324): Loss: 2.0156 || Learning rate: lr=0.0001.
===> Epoch[18](200/324): Loss: 1.4922 || Learning rate: lr=0.0001.
===> Epoch[18](210/324): Loss: 2.3431 || Learning rate: lr=0.0001.
===> Epoch[18](220/324): Loss: 1.6304 || Learning rate: lr=0.0001.
===> Epoch[18](230/324): Loss: 2.5336 || Learning rate: lr=0.0001.
===> Epoch[18](240/324): Loss: 6.2333 || Learning rate: lr=0.0001.
===> Epoch[18](250/324): Loss: 8.2454 || Learning rate: lr=0.0001.
===> Epoch[18](260/324): Loss: 2.6960 || Learning rate: lr=0.0001.
===> Epoch[18](270/324): Loss: 3.2384 || Learning rate: lr=0.0001.
===> Epoch[18](280/324): Loss: 2.4597 || Learning rate: lr=0.0001.
===> Epoch[18](290/324): Loss: 1.7800 || Learning rate: lr=0.0001.
===> Epoch[18](300/324): Loss: 2.2078 || Learning rate: lr=0.0001.
===> Epoch[18](310/324): Loss: 2.2632 || Learning rate: lr=0.0001.
===> Epoch[18](320/324): Loss: 2.6397 || Learning rate: lr=0.0001.
===> Epoch[19](10/324): Loss: 3.0364 || Learning rate: lr=0.0001.
===> Epoch[19](20/324): Loss: 0.9660 || Learning rate: lr=0.0001.
===> Epoch[19](30/324): Loss: 1.5425 || Learning rate: lr=0.0001.
===> Epoch[19](40/324): Loss: 1.9319 || Learning rate: lr=0.0001.
===> Epoch[19](50/324): Loss: 1.6058 || Learning rate: lr=0.0001.
===> Epoch[19](60/324): Loss: 3.3535 || Learning rate: lr=0.0001.
===> Epoch[19](70/324): Loss: 1.2564 || Learning rate: lr=0.0001.
===> Epoch[19](80/324): Loss: 1.5238 || Learning rate: lr=0.0001.
===> Epoch[19](90/324): Loss: 3.2398 || Learning rate: lr=0.0001.
===> Epoch[19](100/324): Loss: 2.2361 || Learning rate: lr=0.0001.
===> Epoch[19](110/324): Loss: 3.5453 || Learning rate: lr=0.0001.
===> Epoch[19](120/324): Loss: 2.4898 || Learning rate: lr=0.0001.
===> Epoch[19](130/324): Loss: 1.9898 || Learning rate: lr=0.0001.
===> Epoch[19](140/324): Loss: 1.4690 || Learning rate: lr=0.0001.
===> Epoch[19](150/324): Loss: 0.9460 || Learning rate: lr=0.0001.
===> Epoch[19](160/324): Loss: 2.2015 || Learning rate: lr=0.0001.
===> Epoch[19](170/324): Loss: 1.5172 || Learning rate: lr=0.0001.
===> Epoch[19](180/324): Loss: 3.9497 || Learning rate: lr=0.0001.
===> Epoch[19](190/324): Loss: 2.9182 || Learning rate: lr=0.0001.
===> Epoch[19](200/324): Loss: 3.8172 || Learning rate: lr=0.0001.
===> Epoch[19](210/324): Loss: 3.8820 || Learning rate: lr=0.0001.
===> Epoch[19](220/324): Loss: 2.3507 || Learning rate: lr=0.0001.
===> Epoch[19](230/324): Loss: 1.6230 || Learning rate: lr=0.0001.
===> Epoch[19](240/324): Loss: 1.5721 || Learning rate: lr=0.0001.
===> Epoch[19](250/324): Loss: 1.9857 || Learning rate: lr=0.0001.
===> Epoch[19](260/324): Loss: 1.6668 || Learning rate: lr=0.0001.
===> Epoch[19](270/324): Loss: 1.7879 || Learning rate: lr=0.0001.
===> Epoch[19](280/324): Loss: 1.9915 || Learning rate: lr=0.0001.
===> Epoch[19](290/324): Loss: 1.2198 || Learning rate: lr=0.0001.
===> Epoch[19](300/324): Loss: 1.9116 || Learning rate: lr=0.0001.
===> Epoch[19](310/324): Loss: 1.5730 || Learning rate: lr=0.0001.
===> Epoch[19](320/324): Loss: 1.2193 || Learning rate: lr=0.0001.
===> Epoch[20](10/324): Loss: 1.7041 || Learning rate: lr=0.0001.
===> Epoch[20](20/324): Loss: 3.4500 || Learning rate: lr=0.0001.
===> Epoch[20](30/324): Loss: 1.9063 || Learning rate: lr=0.0001.
===> Epoch[20](40/324): Loss: 1.9536 || Learning rate: lr=0.0001.
===> Epoch[20](50/324): Loss: 6.6076 || Learning rate: lr=0.0001.
===> Epoch[20](60/324): Loss: 6.6266 || Learning rate: lr=0.0001.
===> Epoch[20](70/324): Loss: 5.0605 || Learning rate: lr=0.0001.
===> Epoch[20](80/324): Loss: 14.9581 || Learning rate: lr=0.0001.
===> Epoch[20](90/324): Loss: 24.3776 || Learning rate: lr=0.0001.
===> Epoch[20](100/324): Loss: 7.3094 || Learning rate: lr=0.0001.
===> Epoch[20](110/324): Loss: 8.6372 || Learning rate: lr=0.0001.
===> Epoch[20](120/324): Loss: 3.6181 || Learning rate: lr=0.0001.
===> Epoch[20](130/324): Loss: 4.8408 || Learning rate: lr=0.0001.
===> Epoch[20](140/324): Loss: 2.6196 || Learning rate: lr=0.0001.
===> Epoch[20](150/324): Loss: 2.0302 || Learning rate: lr=0.0001.
===> Epoch[20](160/324): Loss: 2.4034 || Learning rate: lr=0.0001.
===> Epoch[20](170/324): Loss: 3.0464 || Learning rate: lr=0.0001.
===> Epoch[20](180/324): Loss: 3.1932 || Learning rate: lr=0.0001.
===> Epoch[20](190/324): Loss: 3.1342 || Learning rate: lr=0.0001.
===> Epoch[20](200/324): Loss: 1.1394 || Learning rate: lr=0.0001.
===> Epoch[20](210/324): Loss: 2.1687 || Learning rate: lr=0.0001.
===> Epoch[20](220/324): Loss: 2.1832 || Learning rate: lr=0.0001.
===> Epoch[20](230/324): Loss: 1.9042 || Learning rate: lr=0.0001.
===> Epoch[20](240/324): Loss: 2.1320 || Learning rate: lr=0.0001.
===> Epoch[20](250/324): Loss: 2.1680 || Learning rate: lr=0.0001.
===> Epoch[20](260/324): Loss: 1.4014 || Learning rate: lr=0.0001.
===> Epoch[20](270/324): Loss: 2.0876 || Learning rate: lr=0.0001.
===> Epoch[20](280/324): Loss: 2.3588 || Learning rate: lr=0.0001.
===> Epoch[20](290/324): Loss: 1.6023 || Learning rate: lr=0.0001.
===> Epoch[20](300/324): Loss: 2.0789 || Learning rate: lr=0.0001.
===> Epoch[20](310/324): Loss: 2.1278 || Learning rate: lr=0.0001.
===> Epoch[20](320/324): Loss: 1.2105 || Learning rate: lr=0.0001.
Checkpoint saved to weights/epoch_v2_20.pth
===> Epoch[21](10/324): Loss: 1.7143 || Learning rate: lr=0.0001.
===> Epoch[21](20/324): Loss: 1.0974 || Learning rate: lr=0.0001.
===> Epoch[21](30/324): Loss: 1.5543 || Learning rate: lr=0.0001.
===> Epoch[21](40/324): Loss: 2.4202 || Learning rate: lr=0.0001.
===> Epoch[21](50/324): Loss: 1.1030 || Learning rate: lr=0.0001.
===> Epoch[21](60/324): Loss: 1.1064 || Learning rate: lr=0.0001.
===> Epoch[21](70/324): Loss: 1.8308 || Learning rate: lr=0.0001.
===> Epoch[21](80/324): Loss: 2.2476 || Learning rate: lr=0.0001.
===> Epoch[21](90/324): Loss: 1.4442 || Learning rate: lr=0.0001.
===> Epoch[21](100/324): Loss: 1.6982 || Learning rate: lr=0.0001.
===> Epoch[21](110/324): Loss: 1.0701 || Learning rate: lr=0.0001.
===> Epoch[21](120/324): Loss: 2.0746 || Learning rate: lr=0.0001.
===> Epoch[21](130/324): Loss: 1.3843 || Learning rate: lr=0.0001.
===> Epoch[21](140/324): Loss: 3.8073 || Learning rate: lr=0.0001.
===> Epoch[21](150/324): Loss: 1.8447 || Learning rate: lr=0.0001.
===> Epoch[21](160/324): Loss: 3.4938 || Learning rate: lr=0.0001.
===> Epoch[21](170/324): Loss: 2.6541 || Learning rate: lr=0.0001.
===> Epoch[21](180/324): Loss: 2.7516 || Learning rate: lr=0.0001.
===> Epoch[21](190/324): Loss: 3.2566 || Learning rate: lr=0.0001.
===> Epoch[21](200/324): Loss: 1.6358 || Learning rate: lr=0.0001.
===> Epoch[21](210/324): Loss: 1.7321 || Learning rate: lr=0.0001.
===> Epoch[21](220/324): Loss: 4.4410 || Learning rate: lr=0.0001.
===> Epoch[21](230/324): Loss: 10.3338 || Learning rate: lr=0.0001.
===> Epoch[21](240/324): Loss: 9.8918 || Learning rate: lr=0.0001.
===> Epoch[21](250/324): Loss: 7.9628 || Learning rate: lr=0.0001.
===> Epoch[21](260/324): Loss: 7.2509 || Learning rate: lr=0.0001.
===> Epoch[21](270/324): Loss: 1.0245 || Learning rate: lr=0.0001.
===> Epoch[21](280/324): Loss: 5.4848 || Learning rate: lr=0.0001.
===> Epoch[21](290/324): Loss: 2.0037 || Learning rate: lr=0.0001.
===> Epoch[21](300/324): Loss: 2.4980 || Learning rate: lr=0.0001.
===> Epoch[21](310/324): Loss: 2.7070 || Learning rate: lr=0.0001.
===> Epoch[21](320/324): Loss: 1.3966 || Learning rate: lr=0.0001.
===> Epoch[22](10/324): Loss: 2.0698 || Learning rate: lr=0.0001.
===> Epoch[22](20/324): Loss: 2.6537 || Learning rate: lr=0.0001.
===> Epoch[22](30/324): Loss: 1.0118 || Learning rate: lr=0.0001.
===> Epoch[22](40/324): Loss: 2.1113 || Learning rate: lr=0.0001.
===> Epoch[22](50/324): Loss: 2.3374 || Learning rate: lr=0.0001.
===> Epoch[22](60/324): Loss: 1.8000 || Learning rate: lr=0.0001.
===> Epoch[22](70/324): Loss: 1.5995 || Learning rate: lr=0.0001.
===> Epoch[22](80/324): Loss: 2.2209 || Learning rate: lr=0.0001.
===> Epoch[22](90/324): Loss: 1.9692 || Learning rate: lr=0.0001.
===> Epoch[22](100/324): Loss: 1.4175 || Learning rate: lr=0.0001.
===> Epoch[22](110/324): Loss: 1.0004 || Learning rate: lr=0.0001.
===> Epoch[22](120/324): Loss: 0.9357 || Learning rate: lr=0.0001.
===> Epoch[22](130/324): Loss: 1.5562 || Learning rate: lr=0.0001.
===> Epoch[22](140/324): Loss: 2.8024 || Learning rate: lr=0.0001.
===> Epoch[22](150/324): Loss: 1.1577 || Learning rate: lr=0.0001.
===> Epoch[22](160/324): Loss: 2.0131 || Learning rate: lr=0.0001.
===> Epoch[22](170/324): Loss: 1.2051 || Learning rate: lr=0.0001.
===> Epoch[22](180/324): Loss: 0.7282 || Learning rate: lr=0.0001.
===> Epoch[22](190/324): Loss: 1.3060 || Learning rate: lr=0.0001.
===> Epoch[22](200/324): Loss: 1.5823 || Learning rate: lr=0.0001.
===> Epoch[22](210/324): Loss: 1.7318 || Learning rate: lr=0.0001.
===> Epoch[22](220/324): Loss: 3.0853 || Learning rate: lr=0.0001.
===> Epoch[22](230/324): Loss: 2.9118 || Learning rate: lr=0.0001.
===> Epoch[22](240/324): Loss: 1.9565 || Learning rate: lr=0.0001.
===> Epoch[22](250/324): Loss: 3.1508 || Learning rate: lr=0.0001.
===> Epoch[22](260/324): Loss: 1.6816 || Learning rate: lr=0.0001.
===> Epoch[22](270/324): Loss: 2.5612 || Learning rate: lr=0.0001.
===> Epoch[22](280/324): Loss: 1.1399 || Learning rate: lr=0.0001.
===> Epoch[22](290/324): Loss: 1.1928 || Learning rate: lr=0.0001.
===> Epoch[22](300/324): Loss: 2.0970 || Learning rate: lr=0.0001.
===> Epoch[22](310/324): Loss: 2.2712 || Learning rate: lr=0.0001.
===> Epoch[22](320/324): Loss: 1.2140 || Learning rate: lr=0.0001.
===> Epoch[23](10/324): Loss: 1.5499 || Learning rate: lr=0.0001.
===> Epoch[23](20/324): Loss: 1.1210 || Learning rate: lr=0.0001.
===> Epoch[23](30/324): Loss: 0.8949 || Learning rate: lr=0.0001.
===> Epoch[23](40/324): Loss: 2.4037 || Learning rate: lr=0.0001.
===> Epoch[23](50/324): Loss: 2.0987 || Learning rate: lr=0.0001.
===> Epoch[23](60/324): Loss: 2.2664 || Learning rate: lr=0.0001.
===> Epoch[23](70/324): Loss: 1.5159 || Learning rate: lr=0.0001.
===> Epoch[23](80/324): Loss: 0.9001 || Learning rate: lr=0.0001.
===> Epoch[23](90/324): Loss: 2.1984 || Learning rate: lr=0.0001.
===> Epoch[23](100/324): Loss: 1.8620 || Learning rate: lr=0.0001.
===> Epoch[23](110/324): Loss: 0.9987 || Learning rate: lr=0.0001.
===> Epoch[23](120/324): Loss: 1.2814 || Learning rate: lr=0.0001.
===> Epoch[23](130/324): Loss: 1.2351 || Learning rate: lr=0.0001.
===> Epoch[23](140/324): Loss: 1.2627 || Learning rate: lr=0.0001.
===> Epoch[23](150/324): Loss: 0.9137 || Learning rate: lr=0.0001.
===> Epoch[23](160/324): Loss: 1.8581 || Learning rate: lr=0.0001.
===> Epoch[23](170/324): Loss: 1.7293 || Learning rate: lr=0.0001.
===> Epoch[23](180/324): Loss: 1.2815 || Learning rate: lr=0.0001.
===> Epoch[23](190/324): Loss: 1.5006 || Learning rate: lr=0.0001.
===> Epoch[23](200/324): Loss: 1.5760 || Learning rate: lr=0.0001.
===> Epoch[23](210/324): Loss: 2.3744 || Learning rate: lr=0.0001.
===> Epoch[23](220/324): Loss: 1.7061 || Learning rate: lr=0.0001.
===> Epoch[23](230/324): Loss: 1.2875 || Learning rate: lr=0.0001.
===> Epoch[23](240/324): Loss: 1.7527 || Learning rate: lr=0.0001.
===> Epoch[23](250/324): Loss: 1.9632 || Learning rate: lr=0.0001.
===> Epoch[23](260/324): Loss: 1.1526 || Learning rate: lr=0.0001.
===> Epoch[23](270/324): Loss: 1.9926 || Learning rate: lr=0.0001.
===> Epoch[23](280/324): Loss: 1.6213 || Learning rate: lr=0.0001.
===> Epoch[23](290/324): Loss: 1.8953 || Learning rate: lr=0.0001.
===> Epoch[23](300/324): Loss: 1.4890 || Learning rate: lr=0.0001.
===> Epoch[23](310/324): Loss: 1.0460 || Learning rate: lr=0.0001.
===> Epoch[23](320/324): Loss: 2.4402 || Learning rate: lr=0.0001.
===> Epoch[24](10/324): Loss: 2.5059 || Learning rate: lr=0.0001.
===> Epoch[24](20/324): Loss: 1.7309 || Learning rate: lr=0.0001.
===> Epoch[24](30/324): Loss: 1.3724 || Learning rate: lr=0.0001.
===> Epoch[24](40/324): Loss: 1.3629 || Learning rate: lr=0.0001.
===> Epoch[24](50/324): Loss: 1.6483 || Learning rate: lr=0.0001.
===> Epoch[24](60/324): Loss: 1.3884 || Learning rate: lr=0.0001.
===> Epoch[24](70/324): Loss: 2.1876 || Learning rate: lr=0.0001.
===> Epoch[24](80/324): Loss: 1.5327 || Learning rate: lr=0.0001.
===> Epoch[24](90/324): Loss: 3.1461 || Learning rate: lr=0.0001.
===> Epoch[24](100/324): Loss: 1.8139 || Learning rate: lr=0.0001.
===> Epoch[24](110/324): Loss: 3.4136 || Learning rate: lr=0.0001.
===> Epoch[24](120/324): Loss: 1.6703 || Learning rate: lr=0.0001.
===> Epoch[24](130/324): Loss: 1.4239 || Learning rate: lr=0.0001.
===> Epoch[24](140/324): Loss: 1.7201 || Learning rate: lr=0.0001.
===> Epoch[24](150/324): Loss: 1.7280 || Learning rate: lr=0.0001.
===> Epoch[24](160/324): Loss: 1.5347 || Learning rate: lr=0.0001.
===> Epoch[24](170/324): Loss: 1.2098 || Learning rate: lr=0.0001.
===> Epoch[24](180/324): Loss: 1.4145 || Learning rate: lr=0.0001.
===> Epoch[24](190/324): Loss: 0.8439 || Learning rate: lr=0.0001.
===> Epoch[24](200/324): Loss: 2.0661 || Learning rate: lr=0.0001.
===> Epoch[24](210/324): Loss: 1.3629 || Learning rate: lr=0.0001.
===> Epoch[24](220/324): Loss: 2.2489 || Learning rate: lr=0.0001.
===> Epoch[24](230/324): Loss: 3.0820 || Learning rate: lr=0.0001.
===> Epoch[24](240/324): Loss: 1.3207 || Learning rate: lr=0.0001.
===> Epoch[24](250/324): Loss: 0.8772 || Learning rate: lr=0.0001.
===> Epoch[24](260/324): Loss: 3.4882 || Learning rate: lr=0.0001.
===> Epoch[24](270/324): Loss: 1.0696 || Learning rate: lr=0.0001.
===> Epoch[24](280/324): Loss: 3.8009 || Learning rate: lr=0.0001.
===> Epoch[24](290/324): Loss: 2.1916 || Learning rate: lr=0.0001.
===> Epoch[24](300/324): Loss: 1.4985 || Learning rate: lr=0.0001.
===> Epoch[24](310/324): Loss: 1.7366 || Learning rate: lr=0.0001.
===> Epoch[24](320/324): Loss: 2.0584 || Learning rate: lr=0.0001.
===> Epoch[25](10/324): Loss: 2.0222 || Learning rate: lr=0.0001.
===> Epoch[25](20/324): Loss: 1.6817 || Learning rate: lr=0.0001.
===> Epoch[25](30/324): Loss: 0.9810 || Learning rate: lr=0.0001.
===> Epoch[25](40/324): Loss: 1.8609 || Learning rate: lr=0.0001.
===> Epoch[25](50/324): Loss: 1.3050 || Learning rate: lr=0.0001.
===> Epoch[25](60/324): Loss: 1.2216 || Learning rate: lr=0.0001.
===> Epoch[25](70/324): Loss: 1.5805 || Learning rate: lr=0.0001.
===> Epoch[25](80/324): Loss: 1.2679 || Learning rate: lr=0.0001.
===> Epoch[25](90/324): Loss: 1.1075 || Learning rate: lr=0.0001.
===> Epoch[25](100/324): Loss: 0.9210 || Learning rate: lr=0.0001.
===> Epoch[25](110/324): Loss: 0.9330 || Learning rate: lr=0.0001.
===> Epoch[25](120/324): Loss: 1.3316 || Learning rate: lr=0.0001.
===> Epoch[25](130/324): Loss: 1.8431 || Learning rate: lr=0.0001.
===> Epoch[25](140/324): Loss: 1.0860 || Learning rate: lr=0.0001.
===> Epoch[25](150/324): Loss: 1.9195 || Learning rate: lr=0.0001.
===> Epoch[25](160/324): Loss: 1.1294 || Learning rate: lr=0.0001.
===> Epoch[25](170/324): Loss: 1.9870 || Learning rate: lr=0.0001.
===> Epoch[25](180/324): Loss: 1.2092 || Learning rate: lr=0.0001.
===> Epoch[25](190/324): Loss: 2.8931 || Learning rate: lr=0.0001.
===> Epoch[25](200/324): Loss: 2.2782 || Learning rate: lr=0.0001.
===> Epoch[25](210/324): Loss: 1.2206 || Learning rate: lr=0.0001.
===> Epoch[25](220/324): Loss: 2.1991 || Learning rate: lr=0.0001.
===> Epoch[25](230/324): Loss: 1.8165 || Learning rate: lr=0.0001.
===> Epoch[25](240/324): Loss: 1.2801 || Learning rate: lr=0.0001.
===> Epoch[25](250/324): Loss: 1.7945 || Learning rate: lr=0.0001.
===> Epoch[25](260/324): Loss: 2.4409 || Learning rate: lr=0.0001.
===> Epoch[25](270/324): Loss: 1.4434 || Learning rate: lr=0.0001.
===> Epoch[25](280/324): Loss: 2.2632 || Learning rate: lr=0.0001.
===> Epoch[25](290/324): Loss: 3.0269 || Learning rate: lr=0.0001.
===> Epoch[25](300/324): Loss: 1.4076 || Learning rate: lr=0.0001.
===> Epoch[25](310/324): Loss: 2.3488 || Learning rate: lr=0.0001.
===> Epoch[25](320/324): Loss: 1.2165 || Learning rate: lr=0.0001.
===> Epoch[26](10/324): Loss: 1.9618 || Learning rate: lr=0.0001.
===> Epoch[26](20/324): Loss: 1.2386 || Learning rate: lr=0.0001.
===> Epoch[26](30/324): Loss: 1.2118 || Learning rate: lr=0.0001.
===> Epoch[26](40/324): Loss: 1.2504 || Learning rate: lr=0.0001.
===> Epoch[26](50/324): Loss: 1.9998 || Learning rate: lr=0.0001.
===> Epoch[26](60/324): Loss: 2.4809 || Learning rate: lr=0.0001.
===> Epoch[26](70/324): Loss: 1.1720 || Learning rate: lr=0.0001.
===> Epoch[26](80/324): Loss: 1.4476 || Learning rate: lr=0.0001.
===> Epoch[26](90/324): Loss: 2.1943 || Learning rate: lr=0.0001.
===> Epoch[26](100/324): Loss: 2.5045 || Learning rate: lr=0.0001.
===> Epoch[26](110/324): Loss: 1.7514 || Learning rate: lr=0.0001.
===> Epoch[26](120/324): Loss: 1.0242 || Learning rate: lr=0.0001.
===> Epoch[26](130/324): Loss: 1.5932 || Learning rate: lr=0.0001.
===> Epoch[26](140/324): Loss: 1.1133 || Learning rate: lr=0.0001.
===> Epoch[26](150/324): Loss: 1.9399 || Learning rate: lr=0.0001.
===> Epoch[26](160/324): Loss: 2.6758 || Learning rate: lr=0.0001.
===> Epoch[26](170/324): Loss: 1.3332 || Learning rate: lr=0.0001.
===> Epoch[26](180/324): Loss: 1.1840 || Learning rate: lr=0.0001.
===> Epoch[26](190/324): Loss: 1.3919 || Learning rate: lr=0.0001.
===> Epoch[26](200/324): Loss: 1.5860 || Learning rate: lr=0.0001.
===> Epoch[26](210/324): Loss: 0.8171 || Learning rate: lr=0.0001.
===> Epoch[26](220/324): Loss: 1.7552 || Learning rate: lr=0.0001.
===> Epoch[26](230/324): Loss: 1.5304 || Learning rate: lr=0.0001.
===> Epoch[26](240/324): Loss: 1.8881 || Learning rate: lr=0.0001.
===> Epoch[26](250/324): Loss: 0.9642 || Learning rate: lr=0.0001.
===> Epoch[26](260/324): Loss: 1.4868 || Learning rate: lr=0.0001.
===> Epoch[26](270/324): Loss: 2.4260 || Learning rate: lr=0.0001.
===> Epoch[26](280/324): Loss: 1.1134 || Learning rate: lr=0.0001.
===> Epoch[26](290/324): Loss: 1.4853 || Learning rate: lr=0.0001.
===> Epoch[26](300/324): Loss: 3.6300 || Learning rate: lr=0.0001.
===> Epoch[26](310/324): Loss: 2.2180 || Learning rate: lr=0.0001.
===> Epoch[26](320/324): Loss: 1.8979 || Learning rate: lr=0.0001.
===> Epoch[27](10/324): Loss: 1.1497 || Learning rate: lr=0.0001.
===> Epoch[27](20/324): Loss: 1.8314 || Learning rate: lr=0.0001.
===> Epoch[27](30/324): Loss: 1.0340 || Learning rate: lr=0.0001.
===> Epoch[27](40/324): Loss: 1.2110 || Learning rate: lr=0.0001.
===> Epoch[27](50/324): Loss: 1.7048 || Learning rate: lr=0.0001.
===> Epoch[27](60/324): Loss: 1.6781 || Learning rate: lr=0.0001.
===> Epoch[27](70/324): Loss: 1.5512 || Learning rate: lr=0.0001.
===> Epoch[27](80/324): Loss: 2.4313 || Learning rate: lr=0.0001.
===> Epoch[27](90/324): Loss: 1.9358 || Learning rate: lr=0.0001.
===> Epoch[27](100/324): Loss: 1.5412 || Learning rate: lr=0.0001.
===> Epoch[27](110/324): Loss: 1.7777 || Learning rate: lr=0.0001.
===> Epoch[27](120/324): Loss: 2.0892 || Learning rate: lr=0.0001.
===> Epoch[27](130/324): Loss: 1.9150 || Learning rate: lr=0.0001.
===> Epoch[27](140/324): Loss: 1.3693 || Learning rate: lr=0.0001.
===> Epoch[27](150/324): Loss: 1.7166 || Learning rate: lr=0.0001.
===> Epoch[27](160/324): Loss: 1.6070 || Learning rate: lr=0.0001.
===> Epoch[27](170/324): Loss: 1.8301 || Learning rate: lr=0.0001.
===> Epoch[27](180/324): Loss: 1.8127 || Learning rate: lr=0.0001.
===> Epoch[27](190/324): Loss: 2.1181 || Learning rate: lr=0.0001.
===> Epoch[27](200/324): Loss: 2.6283 || Learning rate: lr=0.0001.
===> Epoch[27](210/324): Loss: 2.5368 || Learning rate: lr=0.0001.
===> Epoch[27](220/324): Loss: 1.0191 || Learning rate: lr=0.0001.
===> Epoch[27](230/324): Loss: 1.5190 || Learning rate: lr=0.0001.
===> Epoch[27](240/324): Loss: 2.2110 || Learning rate: lr=0.0001.
===> Epoch[27](250/324): Loss: 0.8834 || Learning rate: lr=0.0001.
===> Epoch[27](260/324): Loss: 1.3446 || Learning rate: lr=0.0001.
===> Epoch[27](270/324): Loss: 2.2014 || Learning rate: lr=0.0001.
===> Epoch[27](280/324): Loss: 1.5828 || Learning rate: lr=0.0001.
===> Epoch[27](290/324): Loss: 0.9428 || Learning rate: lr=0.0001.
===> Epoch[27](300/324): Loss: 2.0501 || Learning rate: lr=0.0001.
===> Epoch[27](310/324): Loss: 1.5463 || Learning rate: lr=0.0001.
===> Epoch[27](320/324): Loss: 1.8437 || Learning rate: lr=0.0001.
===> Epoch[28](10/324): Loss: 1.2005 || Learning rate: lr=0.0001.
===> Epoch[28](20/324): Loss: 0.9671 || Learning rate: lr=0.0001.
===> Epoch[28](30/324): Loss: 1.2262 || Learning rate: lr=0.0001.
===> Epoch[28](40/324): Loss: 1.5224 || Learning rate: lr=0.0001.
===> Epoch[28](50/324): Loss: 1.7040 || Learning rate: lr=0.0001.
===> Epoch[28](60/324): Loss: 1.2555 || Learning rate: lr=0.0001.
===> Epoch[28](70/324): Loss: 1.4917 || Learning rate: lr=0.0001.
===> Epoch[28](80/324): Loss: 1.2301 || Learning rate: lr=0.0001.
===> Epoch[28](90/324): Loss: 1.2459 || Learning rate: lr=0.0001.
===> Epoch[28](100/324): Loss: 1.1360 || Learning rate: lr=0.0001.
===> Epoch[28](110/324): Loss: 0.8476 || Learning rate: lr=0.0001.
===> Epoch[28](120/324): Loss: 2.2534 || Learning rate: lr=0.0001.
===> Epoch[28](130/324): Loss: 1.7329 || Learning rate: lr=0.0001.
===> Epoch[28](140/324): Loss: 1.2855 || Learning rate: lr=0.0001.
===> Epoch[28](150/324): Loss: 1.9062 || Learning rate: lr=0.0001.
===> Epoch[28](160/324): Loss: 1.2435 || Learning rate: lr=0.0001.
===> Epoch[28](170/324): Loss: 1.4885 || Learning rate: lr=0.0001.
===> Epoch[28](180/324): Loss: 1.7458 || Learning rate: lr=0.0001.
===> Epoch[28](190/324): Loss: 1.4942 || Learning rate: lr=0.0001.
===> Epoch[28](200/324): Loss: 1.3588 || Learning rate: lr=0.0001.
===> Epoch[28](210/324): Loss: 1.1380 || Learning rate: lr=0.0001.
===> Epoch[28](220/324): Loss: 1.2224 || Learning rate: lr=0.0001.
===> Epoch[28](230/324): Loss: 1.8850 || Learning rate: lr=0.0001.
===> Epoch[28](240/324): Loss: 1.1145 || Learning rate: lr=0.0001.
===> Epoch[28](250/324): Loss: 0.9872 || Learning rate: lr=0.0001.
===> Epoch[28](260/324): Loss: 1.1362 || Learning rate: lr=0.0001.
===> Epoch[28](270/324): Loss: 1.6105 || Learning rate: lr=0.0001.
===> Epoch[28](280/324): Loss: 2.0992 || Learning rate: lr=0.0001.
===> Epoch[28](290/324): Loss: 1.7310 || Learning rate: lr=0.0001.
===> Epoch[28](300/324): Loss: 1.4149 || Learning rate: lr=0.0001.
===> Epoch[28](310/324): Loss: 1.7405 || Learning rate: lr=0.0001.
===> Epoch[28](320/324): Loss: 1.0717 || Learning rate: lr=0.0001.
===> Epoch[29](10/324): Loss: 2.0087 || Learning rate: lr=0.0001.
===> Epoch[29](20/324): Loss: 1.9269 || Learning rate: lr=0.0001.
===> Epoch[29](30/324): Loss: 0.8089 || Learning rate: lr=0.0001.
===> Epoch[29](40/324): Loss: 0.9396 || Learning rate: lr=0.0001.
===> Epoch[29](50/324): Loss: 1.5812 || Learning rate: lr=0.0001.
===> Epoch[29](60/324): Loss: 2.2265 || Learning rate: lr=0.0001.
===> Epoch[29](70/324): Loss: 1.6418 || Learning rate: lr=0.0001.
===> Epoch[29](80/324): Loss: 2.0802 || Learning rate: lr=0.0001.
===> Epoch[29](90/324): Loss: 0.8772 || Learning rate: lr=0.0001.
===> Epoch[29](100/324): Loss: 1.8427 || Learning rate: lr=0.0001.
===> Epoch[29](110/324): Loss: 9.5378 || Learning rate: lr=0.0001.
===> Epoch[29](120/324): Loss: 12.7607 || Learning rate: lr=0.0001.
===> Epoch[29](130/324): Loss: 13.6610 || Learning rate: lr=0.0001.
===> Epoch[29](140/324): Loss: 4.3722 || Learning rate: lr=0.0001.
===> Epoch[29](150/324): Loss: 5.2206 || Learning rate: lr=0.0001.
===> Epoch[29](160/324): Loss: 1.9663 || Learning rate: lr=0.0001.
===> Epoch[29](170/324): Loss: 2.4243 || Learning rate: lr=0.0001.
===> Epoch[29](180/324): Loss: 3.7483 || Learning rate: lr=0.0001.
===> Epoch[29](190/324): Loss: 3.6882 || Learning rate: lr=0.0001.
===> Epoch[29](200/324): Loss: 3.7552 || Learning rate: lr=0.0001.
===> Epoch[29](210/324): Loss: 1.5205 || Learning rate: lr=0.0001.
===> Epoch[29](220/324): Loss: 2.7995 || Learning rate: lr=0.0001.
===> Epoch[29](230/324): Loss: 1.7986 || Learning rate: lr=0.0001.
===> Epoch[29](240/324): Loss: 2.9303 || Learning rate: lr=0.0001.
===> Epoch[29](250/324): Loss: 2.3122 || Learning rate: lr=0.0001.
===> Epoch[29](260/324): Loss: 2.2463 || Learning rate: lr=0.0001.
===> Epoch[29](270/324): Loss: 1.8021 || Learning rate: lr=0.0001.
===> Epoch[29](280/324): Loss: 3.7714 || Learning rate: lr=0.0001.
===> Epoch[29](290/324): Loss: 1.4610 || Learning rate: lr=0.0001.
===> Epoch[29](300/324): Loss: 1.4269 || Learning rate: lr=0.0001.
===> Epoch[29](310/324): Loss: 0.9844 || Learning rate: lr=0.0001.
===> Epoch[29](320/324): Loss: 1.9629 || Learning rate: lr=0.0001.
===> Epoch[30](10/324): Loss: 1.1859 || Learning rate: lr=0.0001.
===> Epoch[30](20/324): Loss: 1.3545 || Learning rate: lr=0.0001.
===> Epoch[30](30/324): Loss: 1.3194 || Learning rate: lr=0.0001.
===> Epoch[30](40/324): Loss: 0.8024 || Learning rate: lr=0.0001.
===> Epoch[30](50/324): Loss: 1.5282 || Learning rate: lr=0.0001.
===> Epoch[30](60/324): Loss: 1.2413 || Learning rate: lr=0.0001.
===> Epoch[30](70/324): Loss: 1.1728 || Learning rate: lr=0.0001.
===> Epoch[30](80/324): Loss: 3.3011 || Learning rate: lr=0.0001.
===> Epoch[30](90/324): Loss: 1.0755 || Learning rate: lr=0.0001.
===> Epoch[30](100/324): Loss: 0.8318 || Learning rate: lr=0.0001.
===> Epoch[30](110/324): Loss: 1.1793 || Learning rate: lr=0.0001.
===> Epoch[30](120/324): Loss: 1.2864 || Learning rate: lr=0.0001.
===> Epoch[30](130/324): Loss: 1.3979 || Learning rate: lr=0.0001.
===> Epoch[30](140/324): Loss: 1.0500 || Learning rate: lr=0.0001.
===> Epoch[30](150/324): Loss: 1.7258 || Learning rate: lr=0.0001.
===> Epoch[30](160/324): Loss: 0.8737 || Learning rate: lr=0.0001.
===> Epoch[30](170/324): Loss: 1.8938 || Learning rate: lr=0.0001.
===> Epoch[30](180/324): Loss: 1.1530 || Learning rate: lr=0.0001.
===> Epoch[30](190/324): Loss: 1.1254 || Learning rate: lr=0.0001.
===> Epoch[30](200/324): Loss: 1.7825 || Learning rate: lr=0.0001.
===> Epoch[30](210/324): Loss: 0.9749 || Learning rate: lr=0.0001.
===> Epoch[30](220/324): Loss: 1.1816 || Learning rate: lr=0.0001.
===> Epoch[30](230/324): Loss: 1.5243 || Learning rate: lr=0.0001.
===> Epoch[30](240/324): Loss: 1.5927 || Learning rate: lr=0.0001.
===> Epoch[30](250/324): Loss: 1.0658 || Learning rate: lr=0.0001.
===> Epoch[30](260/324): Loss: 1.4044 || Learning rate: lr=0.0001.
===> Epoch[30](270/324): Loss: 0.8498 || Learning rate: lr=0.0001.
===> Epoch[30](280/324): Loss: 2.0095 || Learning rate: lr=0.0001.
===> Epoch[30](290/324): Loss: 1.0864 || Learning rate: lr=0.0001.
===> Epoch[30](300/324): Loss: 1.2124 || Learning rate: lr=0.0001.
===> Epoch[30](310/324): Loss: 0.8994 || Learning rate: lr=0.0001.
===> Epoch[30](320/324): Loss: 1.7754 || Learning rate: lr=0.0001.
===> Epoch[31](10/324): Loss: 1.0210 || Learning rate: lr=0.0001.
===> Epoch[31](20/324): Loss: 1.6382 || Learning rate: lr=0.0001.
===> Epoch[31](30/324): Loss: 1.3058 || Learning rate: lr=0.0001.
===> Epoch[31](40/324): Loss: 1.5523 || Learning rate: lr=0.0001.
===> Epoch[31](50/324): Loss: 1.6478 || Learning rate: lr=0.0001.
===> Epoch[31](60/324): Loss: 1.3595 || Learning rate: lr=0.0001.
===> Epoch[31](70/324): Loss: 0.8777 || Learning rate: lr=0.0001.
===> Epoch[31](80/324): Loss: 2.1845 || Learning rate: lr=0.0001.
===> Epoch[31](90/324): Loss: 1.8267 || Learning rate: lr=0.0001.
===> Epoch[31](100/324): Loss: 1.0772 || Learning rate: lr=0.0001.
===> Epoch[31](110/324): Loss: 1.4129 || Learning rate: lr=0.0001.
===> Epoch[31](120/324): Loss: 1.6050 || Learning rate: lr=0.0001.
===> Epoch[31](130/324): Loss: 0.9759 || Learning rate: lr=0.0001.
===> Epoch[31](140/324): Loss: 1.0194 || Learning rate: lr=0.0001.
===> Epoch[31](150/324): Loss: 1.7090 || Learning rate: lr=0.0001.
===> Epoch[31](160/324): Loss: 0.8811 || Learning rate: lr=0.0001.
===> Epoch[31](170/324): Loss: 1.6067 || Learning rate: lr=0.0001.
===> Epoch[31](180/324): Loss: 1.0450 || Learning rate: lr=0.0001.
===> Epoch[31](190/324): Loss: 1.3690 || Learning rate: lr=0.0001.
===> Epoch[31](200/324): Loss: 1.1482 || Learning rate: lr=0.0001.
===> Epoch[31](210/324): Loss: 0.9292 || Learning rate: lr=0.0001.
===> Epoch[31](220/324): Loss: 1.4088 || Learning rate: lr=0.0001.
===> Epoch[31](230/324): Loss: 0.8280 || Learning rate: lr=0.0001.
===> Epoch[31](240/324): Loss: 1.7500 || Learning rate: lr=0.0001.
===> Epoch[31](250/324): Loss: 1.1960 || Learning rate: lr=0.0001.
===> Epoch[31](260/324): Loss: 1.8592 || Learning rate: lr=0.0001.
===> Epoch[31](270/324): Loss: 1.3598 || Learning rate: lr=0.0001.
===> Epoch[31](280/324): Loss: 1.2829 || Learning rate: lr=0.0001.
===> Epoch[31](290/324): Loss: 1.3043 || Learning rate: lr=0.0001.
===> Epoch[31](300/324): Loss: 1.2248 || Learning rate: lr=0.0001.
===> Epoch[31](310/324): Loss: 0.9108 || Learning rate: lr=0.0001.
===> Epoch[31](320/324): Loss: 1.1910 || Learning rate: lr=0.0001.
===> Epoch[32](10/324): Loss: 0.8744 || Learning rate: lr=0.0001.
===> Epoch[32](20/324): Loss: 1.0145 || Learning rate: lr=0.0001.
===> Epoch[32](30/324): Loss: 1.7053 || Learning rate: lr=0.0001.
===> Epoch[32](40/324): Loss: 1.6382 || Learning rate: lr=0.0001.
===> Epoch[32](50/324): Loss: 2.4324 || Learning rate: lr=0.0001.
===> Epoch[32](60/324): Loss: 1.1876 || Learning rate: lr=0.0001.
===> Epoch[32](70/324): Loss: 2.1135 || Learning rate: lr=0.0001.
===> Epoch[32](80/324): Loss: 1.3852 || Learning rate: lr=0.0001.
===> Epoch[32](90/324): Loss: 0.9364 || Learning rate: lr=0.0001.
===> Epoch[32](100/324): Loss: 1.3789 || Learning rate: lr=0.0001.
===> Epoch[32](110/324): Loss: 1.5511 || Learning rate: lr=0.0001.
===> Epoch[32](120/324): Loss: 2.0691 || Learning rate: lr=0.0001.
===> Epoch[32](130/324): Loss: 1.4467 || Learning rate: lr=0.0001.
===> Epoch[32](140/324): Loss: 1.3082 || Learning rate: lr=0.0001.
===> Epoch[32](150/324): Loss: 1.5377 || Learning rate: lr=0.0001.
===> Epoch[32](160/324): Loss: 1.1304 || Learning rate: lr=0.0001.
===> Epoch[32](170/324): Loss: 1.2889 || Learning rate: lr=0.0001.
===> Epoch[32](180/324): Loss: 1.3941 || Learning rate: lr=0.0001.
===> Epoch[32](190/324): Loss: 2.3794 || Learning rate: lr=0.0001.
===> Epoch[32](200/324): Loss: 0.7293 || Learning rate: lr=0.0001.
===> Epoch[32](210/324): Loss: 0.6982 || Learning rate: lr=0.0001.
===> Epoch[32](220/324): Loss: 0.9856 || Learning rate: lr=0.0001.
===> Epoch[32](230/324): Loss: 2.0952 || Learning rate: lr=0.0001.
===> Epoch[32](240/324): Loss: 1.3202 || Learning rate: lr=0.0001.
===> Epoch[32](250/324): Loss: 1.7536 || Learning rate: lr=0.0001.
===> Epoch[32](260/324): Loss: 0.9848 || Learning rate: lr=0.0001.
===> Epoch[32](270/324): Loss: 1.1052 || Learning rate: lr=0.0001.
===> Epoch[32](280/324): Loss: 1.1635 || Learning rate: lr=0.0001.
===> Epoch[32](290/324): Loss: 0.9682 || Learning rate: lr=0.0001.
===> Epoch[32](300/324): Loss: 0.8506 || Learning rate: lr=0.0001.
===> Epoch[32](310/324): Loss: 1.6511 || Learning rate: lr=0.0001.
===> Epoch[32](320/324): Loss: 1.7107 || Learning rate: lr=0.0001.
===> Epoch[33](10/324): Loss: 0.9936 || Learning rate: lr=0.0001.
===> Epoch[33](20/324): Loss: 1.3266 || Learning rate: lr=0.0001.
===> Epoch[33](30/324): Loss: 0.9614 || Learning rate: lr=0.0001.
===> Epoch[33](40/324): Loss: 0.7181 || Learning rate: lr=0.0001.
===> Epoch[33](50/324): Loss: 1.2441 || Learning rate: lr=0.0001.
===> Epoch[33](60/324): Loss: 0.9186 || Learning rate: lr=0.0001.
===> Epoch[33](70/324): Loss: 1.9266 || Learning rate: lr=0.0001.
===> Epoch[33](80/324): Loss: 1.2721 || Learning rate: lr=0.0001.
===> Epoch[33](90/324): Loss: 0.9705 || Learning rate: lr=0.0001.
===> Epoch[33](100/324): Loss: 1.7994 || Learning rate: lr=0.0001.
===> Epoch[33](110/324): Loss: 1.6159 || Learning rate: lr=0.0001.
===> Epoch[33](120/324): Loss: 0.8797 || Learning rate: lr=0.0001.
===> Epoch[33](130/324): Loss: 1.9628 || Learning rate: lr=0.0001.
===> Epoch[33](140/324): Loss: 0.9963 || Learning rate: lr=0.0001.
===> Epoch[33](150/324): Loss: 1.4282 || Learning rate: lr=0.0001.
===> Epoch[33](160/324): Loss: 1.0221 || Learning rate: lr=0.0001.
===> Epoch[33](170/324): Loss: 0.9407 || Learning rate: lr=0.0001.
===> Epoch[33](180/324): Loss: 1.4619 || Learning rate: lr=0.0001.
===> Epoch[33](190/324): Loss: 1.1434 || Learning rate: lr=0.0001.
===> Epoch[33](200/324): Loss: 2.3537 || Learning rate: lr=0.0001.
===> Epoch[33](210/324): Loss: 0.9913 || Learning rate: lr=0.0001.
===> Epoch[33](220/324): Loss: 1.8612 || Learning rate: lr=0.0001.
===> Epoch[33](230/324): Loss: 2.7504 || Learning rate: lr=0.0001.
===> Epoch[33](240/324): Loss: 0.8709 || Learning rate: lr=0.0001.
===> Epoch[33](250/324): Loss: 1.4191 || Learning rate: lr=0.0001.
===> Epoch[33](260/324): Loss: 0.9132 || Learning rate: lr=0.0001.
===> Epoch[33](270/324): Loss: 1.1676 || Learning rate: lr=0.0001.
===> Epoch[33](280/324): Loss: 1.7306 || Learning rate: lr=0.0001.
===> Epoch[33](290/324): Loss: 1.0828 || Learning rate: lr=0.0001.
===> Epoch[33](300/324): Loss: 1.4336 || Learning rate: lr=0.0001.
===> Epoch[33](310/324): Loss: 1.5358 || Learning rate: lr=0.0001.
===> Epoch[33](320/324): Loss: 0.7546 || Learning rate: lr=0.0001.
===> Epoch[34](10/324): Loss: 1.2758 || Learning rate: lr=0.0001.
===> Epoch[34](20/324): Loss: 1.2315 || Learning rate: lr=0.0001.
===> Epoch[34](30/324): Loss: 1.4303 || Learning rate: lr=0.0001.
===> Epoch[34](40/324): Loss: 1.1338 || Learning rate: lr=0.0001.
===> Epoch[34](50/324): Loss: 0.9761 || Learning rate: lr=0.0001.
===> Epoch[34](60/324): Loss: 1.0532 || Learning rate: lr=0.0001.
===> Epoch[34](70/324): Loss: 1.4049 || Learning rate: lr=0.0001.
===> Epoch[34](80/324): Loss: 1.1073 || Learning rate: lr=0.0001.
===> Epoch[34](90/324): Loss: 1.1414 || Learning rate: lr=0.0001.
===> Epoch[34](100/324): Loss: 0.8023 || Learning rate: lr=0.0001.
===> Epoch[34](110/324): Loss: 1.0741 || Learning rate: lr=0.0001.
===> Epoch[34](120/324): Loss: 1.8313 || Learning rate: lr=0.0001.
===> Epoch[34](130/324): Loss: 1.3799 || Learning rate: lr=0.0001.
===> Epoch[34](140/324): Loss: 1.3013 || Learning rate: lr=0.0001.
===> Epoch[34](150/324): Loss: 1.5333 || Learning rate: lr=0.0001.
===> Epoch[34](160/324): Loss: 1.4054 || Learning rate: lr=0.0001.
===> Epoch[34](170/324): Loss: 0.6913 || Learning rate: lr=0.0001.
===> Epoch[34](180/324): Loss: 1.0897 || Learning rate: lr=0.0001.
===> Epoch[34](190/324): Loss: 0.9024 || Learning rate: lr=0.0001.
===> Epoch[34](200/324): Loss: 1.8767 || Learning rate: lr=0.0001.
===> Epoch[34](210/324): Loss: 1.3092 || Learning rate: lr=0.0001.
===> Epoch[34](220/324): Loss: 1.3972 || Learning rate: lr=0.0001.
===> Epoch[34](230/324): Loss: 1.5277 || Learning rate: lr=0.0001.
===> Epoch[34](240/324): Loss: 1.0962 || Learning rate: lr=0.0001.
===> Epoch[34](250/324): Loss: 1.8895 || Learning rate: lr=0.0001.
===> Epoch[34](260/324): Loss: 0.9505 || Learning rate: lr=0.0001.
===> Epoch[34](270/324): Loss: 1.7812 || Learning rate: lr=0.0001.
===> Epoch[34](280/324): Loss: 1.7871 || Learning rate: lr=0.0001.
===> Epoch[34](290/324): Loss: 1.5175 || Learning rate: lr=0.0001.
===> Epoch[34](300/324): Loss: 0.9999 || Learning rate: lr=0.0001.
===> Epoch[34](310/324): Loss: 1.1932 || Learning rate: lr=0.0001.
===> Epoch[34](320/324): Loss: 1.1299 || Learning rate: lr=0.0001.
===> Epoch[35](10/324): Loss: 1.1197 || Learning rate: lr=0.0001.
===> Epoch[35](20/324): Loss: 1.7707 || Learning rate: lr=0.0001.
===> Epoch[35](30/324): Loss: 1.7693 || Learning rate: lr=0.0001.
===> Epoch[35](40/324): Loss: 1.0411 || Learning rate: lr=0.0001.
===> Epoch[35](50/324): Loss: 1.8448 || Learning rate: lr=0.0001.
===> Epoch[35](60/324): Loss: 1.7918 || Learning rate: lr=0.0001.
===> Epoch[35](70/324): Loss: 1.9312 || Learning rate: lr=0.0001.
===> Epoch[35](80/324): Loss: 1.8752 || Learning rate: lr=0.0001.
===> Epoch[35](90/324): Loss: 2.8035 || Learning rate: lr=0.0001.
===> Epoch[35](100/324): Loss: 2.0501 || Learning rate: lr=0.0001.
===> Epoch[35](110/324): Loss: 1.6733 || Learning rate: lr=0.0001.
===> Epoch[35](120/324): Loss: 2.4993 || Learning rate: lr=0.0001.
===> Epoch[35](130/324): Loss: 1.8225 || Learning rate: lr=0.0001.
===> Epoch[35](140/324): Loss: 1.1158 || Learning rate: lr=0.0001.
===> Epoch[35](150/324): Loss: 1.8263 || Learning rate: lr=0.0001.
===> Epoch[35](160/324): Loss: 2.8411 || Learning rate: lr=0.0001.
===> Epoch[35](170/324): Loss: 3.7707 || Learning rate: lr=0.0001.
===> Epoch[35](180/324): Loss: 1.2355 || Learning rate: lr=0.0001.
===> Epoch[35](190/324): Loss: 2.0724 || Learning rate: lr=0.0001.
===> Epoch[35](200/324): Loss: 1.9629 || Learning rate: lr=0.0001.
===> Epoch[35](210/324): Loss: 1.8616 || Learning rate: lr=0.0001.
===> Epoch[35](220/324): Loss: 1.2185 || Learning rate: lr=0.0001.
===> Epoch[35](230/324): Loss: 1.4649 || Learning rate: lr=0.0001.
===> Epoch[35](240/324): Loss: 1.6226 || Learning rate: lr=0.0001.
===> Epoch[35](250/324): Loss: 1.5729 || Learning rate: lr=0.0001.
===> Epoch[35](260/324): Loss: 1.4304 || Learning rate: lr=0.0001.
===> Epoch[35](270/324): Loss: 1.2197 || Learning rate: lr=0.0001.
===> Epoch[35](280/324): Loss: 1.0705 || Learning rate: lr=0.0001.
===> Epoch[35](290/324): Loss: 1.9051 || Learning rate: lr=0.0001.
===> Epoch[35](300/324): Loss: 1.1597 || Learning rate: lr=0.0001.
===> Epoch[35](310/324): Loss: 1.8043 || Learning rate: lr=0.0001.
===> Epoch[35](320/324): Loss: 1.0076 || Learning rate: lr=0.0001.
===> Epoch[36](10/324): Loss: 1.1264 || Learning rate: lr=0.0001.
===> Epoch[36](20/324): Loss: 1.0756 || Learning rate: lr=0.0001.
===> Epoch[36](30/324): Loss: 0.8676 || Learning rate: lr=0.0001.
===> Epoch[36](40/324): Loss: 1.1516 || Learning rate: lr=0.0001.
===> Epoch[36](50/324): Loss: 1.0242 || Learning rate: lr=0.0001.
===> Epoch[36](60/324): Loss: 1.1552 || Learning rate: lr=0.0001.
===> Epoch[36](70/324): Loss: 2.0378 || Learning rate: lr=0.0001.
===> Epoch[36](80/324): Loss: 1.0207 || Learning rate: lr=0.0001.
===> Epoch[36](90/324): Loss: 1.5420 || Learning rate: lr=0.0001.
===> Epoch[36](100/324): Loss: 1.5874 || Learning rate: lr=0.0001.
===> Epoch[36](110/324): Loss: 1.2762 || Learning rate: lr=0.0001.
===> Epoch[36](120/324): Loss: 1.0510 || Learning rate: lr=0.0001.
===> Epoch[36](130/324): Loss: 1.2954 || Learning rate: lr=0.0001.
===> Epoch[36](140/324): Loss: 1.8603 || Learning rate: lr=0.0001.
===> Epoch[36](150/324): Loss: 1.8266 || Learning rate: lr=0.0001.
===> Epoch[36](160/324): Loss: 2.0440 || Learning rate: lr=0.0001.
===> Epoch[36](170/324): Loss: 1.7013 || Learning rate: lr=0.0001.
===> Epoch[36](180/324): Loss: 1.5400 || Learning rate: lr=0.0001.
===> Epoch[36](190/324): Loss: 1.1937 || Learning rate: lr=0.0001.
===> Epoch[36](200/324): Loss: 1.9848 || Learning rate: lr=0.0001.
===> Epoch[36](210/324): Loss: 2.7163 || Learning rate: lr=0.0001.
===> Epoch[36](220/324): Loss: 1.9814 || Learning rate: lr=0.0001.
===> Epoch[36](230/324): Loss: 1.8568 || Learning rate: lr=0.0001.
===> Epoch[36](240/324): Loss: 1.4139 || Learning rate: lr=0.0001.
===> Epoch[36](250/324): Loss: 1.6843 || Learning rate: lr=0.0001.
===> Epoch[36](260/324): Loss: 1.4100 || Learning rate: lr=0.0001.
===> Epoch[36](270/324): Loss: 1.0740 || Learning rate: lr=0.0001.
===> Epoch[36](280/324): Loss: 1.3703 || Learning rate: lr=0.0001.
===> Epoch[36](290/324): Loss: 1.2999 || Learning rate: lr=0.0001.
===> Epoch[36](300/324): Loss: 1.0301 || Learning rate: lr=0.0001.
===> Epoch[36](310/324): Loss: 0.8452 || Learning rate: lr=0.0001.
===> Epoch[36](320/324): Loss: 1.7887 || Learning rate: lr=0.0001.
===> Epoch[37](10/324): Loss: 0.8563 || Learning rate: lr=0.0001.
===> Epoch[37](20/324): Loss: 0.9746 || Learning rate: lr=0.0001.
===> Epoch[37](30/324): Loss: 1.1224 || Learning rate: lr=0.0001.
===> Epoch[37](40/324): Loss: 1.2118 || Learning rate: lr=0.0001.
===> Epoch[37](50/324): Loss: 1.2966 || Learning rate: lr=0.0001.
===> Epoch[37](60/324): Loss: 0.9557 || Learning rate: lr=0.0001.
===> Epoch[37](70/324): Loss: 1.0575 || Learning rate: lr=0.0001.
===> Epoch[37](80/324): Loss: 0.9265 || Learning rate: lr=0.0001.
===> Epoch[37](90/324): Loss: 1.0000 || Learning rate: lr=0.0001.
===> Epoch[37](100/324): Loss: 0.9563 || Learning rate: lr=0.0001.
===> Epoch[37](110/324): Loss: 1.4317 || Learning rate: lr=0.0001.
===> Epoch[37](120/324): Loss: 1.5622 || Learning rate: lr=0.0001.
===> Epoch[37](130/324): Loss: 0.6916 || Learning rate: lr=0.0001.
===> Epoch[37](140/324): Loss: 1.9681 || Learning rate: lr=0.0001.
===> Epoch[37](150/324): Loss: 0.7592 || Learning rate: lr=0.0001.
===> Epoch[37](160/324): Loss: 0.8779 || Learning rate: lr=0.0001.
===> Epoch[37](170/324): Loss: 0.7866 || Learning rate: lr=0.0001.
===> Epoch[37](180/324): Loss: 1.1336 || Learning rate: lr=0.0001.
===> Epoch[37](190/324): Loss: 0.7385 || Learning rate: lr=0.0001.
===> Epoch[37](200/324): Loss: 0.9902 || Learning rate: lr=0.0001.
===> Epoch[37](210/324): Loss: 2.0658 || Learning rate: lr=0.0001.
===> Epoch[37](220/324): Loss: 1.1766 || Learning rate: lr=0.0001.
===> Epoch[37](230/324): Loss: 2.2844 || Learning rate: lr=0.0001.
===> Epoch[37](240/324): Loss: 0.6625 || Learning rate: lr=0.0001.
===> Epoch[37](250/324): Loss: 0.9106 || Learning rate: lr=0.0001.
===> Epoch[37](260/324): Loss: 1.4221 || Learning rate: lr=0.0001.
===> Epoch[37](270/324): Loss: 1.0537 || Learning rate: lr=0.0001.
===> Epoch[37](280/324): Loss: 1.8482 || Learning rate: lr=0.0001.
===> Epoch[37](290/324): Loss: 1.3003 || Learning rate: lr=0.0001.
===> Epoch[37](300/324): Loss: 1.1173 || Learning rate: lr=0.0001.
===> Epoch[37](310/324): Loss: 0.7909 || Learning rate: lr=0.0001.
===> Epoch[37](320/324): Loss: 1.1150 || Learning rate: lr=0.0001.
===> Epoch[38](10/324): Loss: 1.9810 || Learning rate: lr=0.0001.
===> Epoch[38](20/324): Loss: 1.2481 || Learning rate: lr=0.0001.
===> Epoch[38](30/324): Loss: 1.1681 || Learning rate: lr=0.0001.
===> Epoch[38](40/324): Loss: 0.6774 || Learning rate: lr=0.0001.
===> Epoch[38](50/324): Loss: 1.3329 || Learning rate: lr=0.0001.
===> Epoch[38](60/324): Loss: 1.0615 || Learning rate: lr=0.0001.
===> Epoch[38](70/324): Loss: 1.5579 || Learning rate: lr=0.0001.
===> Epoch[38](80/324): Loss: 0.9834 || Learning rate: lr=0.0001.
===> Epoch[38](90/324): Loss: 0.8189 || Learning rate: lr=0.0001.
===> Epoch[38](100/324): Loss: 0.8850 || Learning rate: lr=0.0001.
===> Epoch[38](110/324): Loss: 0.8410 || Learning rate: lr=0.0001.
===> Epoch[38](120/324): Loss: 0.9139 || Learning rate: lr=0.0001.
===> Epoch[38](130/324): Loss: 1.2089 || Learning rate: lr=0.0001.
===> Epoch[38](140/324): Loss: 1.2700 || Learning rate: lr=0.0001.
===> Epoch[38](150/324): Loss: 1.1950 || Learning rate: lr=0.0001.
===> Epoch[38](160/324): Loss: 0.7816 || Learning rate: lr=0.0001.
===> Epoch[38](170/324): Loss: 1.8504 || Learning rate: lr=0.0001.
===> Epoch[38](180/324): Loss: 1.4938 || Learning rate: lr=0.0001.
===> Epoch[38](190/324): Loss: 1.7284 || Learning rate: lr=0.0001.
===> Epoch[38](200/324): Loss: 0.7362 || Learning rate: lr=0.0001.
===> Epoch[38](210/324): Loss: 0.8480 || Learning rate: lr=0.0001.
===> Epoch[38](220/324): Loss: 1.6728 || Learning rate: lr=0.0001.
===> Epoch[38](230/324): Loss: 1.2085 || Learning rate: lr=0.0001.
===> Epoch[38](240/324): Loss: 1.2523 || Learning rate: lr=0.0001.
===> Epoch[38](250/324): Loss: 1.0029 || Learning rate: lr=0.0001.
===> Epoch[38](260/324): Loss: 1.1102 || Learning rate: lr=0.0001.
===> Epoch[38](270/324): Loss: 2.0589 || Learning rate: lr=0.0001.
===> Epoch[38](280/324): Loss: 4.7441 || Learning rate: lr=0.0001.
===> Epoch[38](290/324): Loss: 6.5847 || Learning rate: lr=0.0001.
===> Epoch[38](300/324): Loss: 4.7081 || Learning rate: lr=0.0001.
===> Epoch[38](310/324): Loss: 7.5628 || Learning rate: lr=0.0001.
===> Epoch[38](320/324): Loss: 5.6601 || Learning rate: lr=0.0001.
===> Epoch[39](10/324): Loss: 3.3760 || Learning rate: lr=0.0001.
===> Epoch[39](20/324): Loss: 1.8002 || Learning rate: lr=0.0001.
===> Epoch[39](30/324): Loss: 1.6365 || Learning rate: lr=0.0001.
===> Epoch[39](40/324): Loss: 1.7327 || Learning rate: lr=0.0001.
===> Epoch[39](50/324): Loss: 1.4168 || Learning rate: lr=0.0001.
===> Epoch[39](60/324): Loss: 1.5059 || Learning rate: lr=0.0001.
===> Epoch[39](70/324): Loss: 1.1614 || Learning rate: lr=0.0001.
===> Epoch[39](80/324): Loss: 1.3879 || Learning rate: lr=0.0001.
===> Epoch[39](90/324): Loss: 1.7328 || Learning rate: lr=0.0001.
===> Epoch[39](100/324): Loss: 0.6998 || Learning rate: lr=0.0001.
===> Epoch[39](110/324): Loss: 1.4138 || Learning rate: lr=0.0001.
===> Epoch[39](120/324): Loss: 1.9779 || Learning rate: lr=0.0001.
===> Epoch[39](130/324): Loss: 1.8662 || Learning rate: lr=0.0001.
===> Epoch[39](140/324): Loss: 1.9929 || Learning rate: lr=0.0001.
===> Epoch[39](150/324): Loss: 1.3684 || Learning rate: lr=0.0001.
===> Epoch[39](160/324): Loss: 1.3024 || Learning rate: lr=0.0001.
===> Epoch[39](170/324): Loss: 0.8338 || Learning rate: lr=0.0001.
===> Epoch[39](180/324): Loss: 1.3363 || Learning rate: lr=0.0001.
===> Epoch[39](190/324): Loss: 1.5510 || Learning rate: lr=0.0001.
===> Epoch[39](200/324): Loss: 1.8037 || Learning rate: lr=0.0001.
===> Epoch[39](210/324): Loss: 0.8296 || Learning rate: lr=0.0001.
===> Epoch[39](220/324): Loss: 1.1575 || Learning rate: lr=0.0001.
===> Epoch[39](230/324): Loss: 1.7926 || Learning rate: lr=0.0001.
===> Epoch[39](240/324): Loss: 1.7378 || Learning rate: lr=0.0001.
===> Epoch[39](250/324): Loss: 1.1898 || Learning rate: lr=0.0001.
===> Epoch[39](260/324): Loss: 1.0007 || Learning rate: lr=0.0001.
===> Epoch[39](270/324): Loss: 1.1461 || Learning rate: lr=0.0001.
===> Epoch[39](280/324): Loss: 0.8096 || Learning rate: lr=0.0001.
===> Epoch[39](290/324): Loss: 0.8401 || Learning rate: lr=0.0001.
===> Epoch[39](300/324): Loss: 1.3240 || Learning rate: lr=0.0001.
===> Epoch[39](310/324): Loss: 2.5276 || Learning rate: lr=0.0001.
===> Epoch[39](320/324): Loss: 1.5580 || Learning rate: lr=0.0001.
===> Epoch[40](10/324): Loss: 1.0737 || Learning rate: lr=0.0001.
===> Epoch[40](20/324): Loss: 1.6857 || Learning rate: lr=0.0001.
===> Epoch[40](30/324): Loss: 1.8883 || Learning rate: lr=0.0001.
===> Epoch[40](40/324): Loss: 1.6488 || Learning rate: lr=0.0001.
===> Epoch[40](50/324): Loss: 1.3431 || Learning rate: lr=0.0001.
===> Epoch[40](60/324): Loss: 1.0187 || Learning rate: lr=0.0001.
===> Epoch[40](70/324): Loss: 1.4285 || Learning rate: lr=0.0001.
===> Epoch[40](80/324): Loss: 1.5084 || Learning rate: lr=0.0001.
===> Epoch[40](90/324): Loss: 1.7095 || Learning rate: lr=0.0001.
===> Epoch[40](100/324): Loss: 1.5079 || Learning rate: lr=0.0001.
===> Epoch[40](110/324): Loss: 1.6968 || Learning rate: lr=0.0001.
===> Epoch[40](120/324): Loss: 1.0921 || Learning rate: lr=0.0001.
===> Epoch[40](130/324): Loss: 1.0707 || Learning rate: lr=0.0001.
===> Epoch[40](140/324): Loss: 1.3157 || Learning rate: lr=0.0001.
===> Epoch[40](150/324): Loss: 0.6938 || Learning rate: lr=0.0001.
===> Epoch[40](160/324): Loss: 1.5147 || Learning rate: lr=0.0001.
===> Epoch[40](170/324): Loss: 1.1831 || Learning rate: lr=0.0001.
===> Epoch[40](180/324): Loss: 1.5671 || Learning rate: lr=0.0001.
===> Epoch[40](190/324): Loss: 1.3264 || Learning rate: lr=0.0001.
===> Epoch[40](200/324): Loss: 1.3000 || Learning rate: lr=0.0001.
===> Epoch[40](210/324): Loss: 1.0312 || Learning rate: lr=0.0001.
===> Epoch[40](220/324): Loss: 1.0749 || Learning rate: lr=0.0001.
===> Epoch[40](230/324): Loss: 0.9859 || Learning rate: lr=0.0001.
===> Epoch[40](240/324): Loss: 1.1095 || Learning rate: lr=0.0001.
===> Epoch[40](250/324): Loss: 0.7881 || Learning rate: lr=0.0001.
===> Epoch[40](260/324): Loss: 0.9093 || Learning rate: lr=0.0001.
===> Epoch[40](270/324): Loss: 0.8658 || Learning rate: lr=0.0001.
===> Epoch[40](280/324): Loss: 1.4266 || Learning rate: lr=0.0001.
===> Epoch[40](290/324): Loss: 1.7294 || Learning rate: lr=0.0001.
===> Epoch[40](300/324): Loss: 1.5136 || Learning rate: lr=0.0001.
===> Epoch[40](310/324): Loss: 1.6422 || Learning rate: lr=0.0001.
===> Epoch[40](320/324): Loss: 1.7668 || Learning rate: lr=0.0001.
Checkpoint saved to weights/epoch_v2_40.pth
===> Epoch[41](10/324): Loss: 0.8561 || Learning rate: lr=0.0001.
===> Epoch[41](20/324): Loss: 1.9841 || Learning rate: lr=0.0001.
===> Epoch[41](30/324): Loss: 1.5480 || Learning rate: lr=0.0001.
===> Epoch[41](40/324): Loss: 1.4461 || Learning rate: lr=0.0001.
===> Epoch[41](50/324): Loss: 1.4838 || Learning rate: lr=0.0001.
===> Epoch[41](60/324): Loss: 0.8210 || Learning rate: lr=0.0001.
===> Epoch[41](70/324): Loss: 0.9947 || Learning rate: lr=0.0001.
===> Epoch[41](80/324): Loss: 1.1194 || Learning rate: lr=0.0001.
===> Epoch[41](90/324): Loss: 1.7044 || Learning rate: lr=0.0001.
===> Epoch[41](100/324): Loss: 1.0364 || Learning rate: lr=0.0001.
===> Epoch[41](110/324): Loss: 1.3779 || Learning rate: lr=0.0001.
===> Epoch[41](120/324): Loss: 1.2797 || Learning rate: lr=0.0001.
===> Epoch[41](130/324): Loss: 0.9980 || Learning rate: lr=0.0001.
===> Epoch[41](140/324): Loss: 1.0739 || Learning rate: lr=0.0001.
===> Epoch[41](150/324): Loss: 1.1064 || Learning rate: lr=0.0001.
===> Epoch[41](160/324): Loss: 1.6961 || Learning rate: lr=0.0001.
===> Epoch[41](170/324): Loss: 1.7383 || Learning rate: lr=0.0001.
===> Epoch[41](180/324): Loss: 1.1743 || Learning rate: lr=0.0001.
===> Epoch[41](190/324): Loss: 1.4925 || Learning rate: lr=0.0001.
===> Epoch[41](200/324): Loss: 2.1247 || Learning rate: lr=0.0001.
===> Epoch[41](210/324): Loss: 1.9698 || Learning rate: lr=0.0001.
===> Epoch[41](220/324): Loss: 3.9826 || Learning rate: lr=0.0001.
===> Epoch[41](230/324): Loss: 1.3894 || Learning rate: lr=0.0001.
===> Epoch[41](240/324): Loss: 3.2130 || Learning rate: lr=0.0001.
===> Epoch[41](250/324): Loss: 3.4053 || Learning rate: lr=0.0001.
===> Epoch[41](260/324): Loss: 1.9434 || Learning rate: lr=0.0001.
===> Epoch[41](270/324): Loss: 1.6627 || Learning rate: lr=0.0001.
===> Epoch[41](280/324): Loss: 1.6568 || Learning rate: lr=0.0001.
===> Epoch[41](290/324): Loss: 1.2744 || Learning rate: lr=0.0001.
===> Epoch[41](300/324): Loss: 1.1336 || Learning rate: lr=0.0001.
===> Epoch[41](310/324): Loss: 1.1061 || Learning rate: lr=0.0001.
===> Epoch[41](320/324): Loss: 1.0631 || Learning rate: lr=0.0001.
===> Epoch[42](10/324): Loss: 1.5502 || Learning rate: lr=0.0001.
===> Epoch[42](20/324): Loss: 1.2499 || Learning rate: lr=0.0001.
===> Epoch[42](30/324): Loss: 1.0057 || Learning rate: lr=0.0001.
===> Epoch[42](40/324): Loss: 0.7271 || Learning rate: lr=0.0001.
===> Epoch[42](50/324): Loss: 0.8721 || Learning rate: lr=0.0001.
===> Epoch[42](60/324): Loss: 1.2152 || Learning rate: lr=0.0001.
===> Epoch[42](70/324): Loss: 0.8182 || Learning rate: lr=0.0001.
===> Epoch[42](80/324): Loss: 1.4443 || Learning rate: lr=0.0001.
===> Epoch[42](90/324): Loss: 0.6532 || Learning rate: lr=0.0001.
===> Epoch[42](100/324): Loss: 1.3888 || Learning rate: lr=0.0001.
===> Epoch[42](110/324): Loss: 0.9471 || Learning rate: lr=0.0001.
===> Epoch[42](120/324): Loss: 0.9764 || Learning rate: lr=0.0001.
===> Epoch[42](130/324): Loss: 0.9088 || Learning rate: lr=0.0001.
===> Epoch[42](140/324): Loss: 1.0195 || Learning rate: lr=0.0001.
===> Epoch[42](150/324): Loss: 2.5152 || Learning rate: lr=0.0001.
===> Epoch[42](160/324): Loss: 1.1201 || Learning rate: lr=0.0001.
===> Epoch[42](170/324): Loss: 1.6294 || Learning rate: lr=0.0001.
===> Epoch[42](180/324): Loss: 1.1566 || Learning rate: lr=0.0001.
===> Epoch[42](190/324): Loss: 0.9694 || Learning rate: lr=0.0001.
===> Epoch[42](200/324): Loss: 1.0233 || Learning rate: lr=0.0001.
===> Epoch[42](210/324): Loss: 1.4552 || Learning rate: lr=0.0001.
===> Epoch[42](220/324): Loss: 0.9695 || Learning rate: lr=0.0001.
===> Epoch[42](230/324): Loss: 1.8095 || Learning rate: lr=0.0001.
===> Epoch[42](240/324): Loss: 1.3894 || Learning rate: lr=0.0001.
===> Epoch[42](250/324): Loss: 1.3074 || Learning rate: lr=0.0001.
===> Epoch[42](260/324): Loss: 1.1949 || Learning rate: lr=0.0001.
===> Epoch[42](270/324): Loss: 0.7332 || Learning rate: lr=0.0001.
===> Epoch[42](280/324): Loss: 0.8517 || Learning rate: lr=0.0001.
===> Epoch[42](290/324): Loss: 1.3272 || Learning rate: lr=0.0001.
===> Epoch[42](300/324): Loss: 1.0615 || Learning rate: lr=0.0001.
===> Epoch[42](310/324): Loss: 0.8814 || Learning rate: lr=0.0001.
===> Epoch[42](320/324): Loss: 0.8627 || Learning rate: lr=0.0001.
===> Epoch[43](10/324): Loss: 0.9985 || Learning rate: lr=0.0001.
===> Epoch[43](20/324): Loss: 0.9217 || Learning rate: lr=0.0001.
===> Epoch[43](30/324): Loss: 0.8332 || Learning rate: lr=0.0001.
===> Epoch[43](40/324): Loss: 1.0803 || Learning rate: lr=0.0001.
===> Epoch[43](50/324): Loss: 0.8365 || Learning rate: lr=0.0001.
===> Epoch[43](60/324): Loss: 1.5247 || Learning rate: lr=0.0001.
===> Epoch[43](70/324): Loss: 1.0975 || Learning rate: lr=0.0001.
===> Epoch[43](80/324): Loss: 1.5361 || Learning rate: lr=0.0001.
===> Epoch[43](90/324): Loss: 0.9460 || Learning rate: lr=0.0001.
===> Epoch[43](100/324): Loss: 1.3560 || Learning rate: lr=0.0001.
===> Epoch[43](110/324): Loss: 0.9477 || Learning rate: lr=0.0001.
===> Epoch[43](120/324): Loss: 0.7347 || Learning rate: lr=0.0001.
===> Epoch[43](130/324): Loss: 0.9288 || Learning rate: lr=0.0001.
===> Epoch[43](140/324): Loss: 1.0965 || Learning rate: lr=0.0001.
===> Epoch[43](150/324): Loss: 1.2047 || Learning rate: lr=0.0001.
===> Epoch[43](160/324): Loss: 0.6826 || Learning rate: lr=0.0001.
===> Epoch[43](170/324): Loss: 1.0838 || Learning rate: lr=0.0001.
===> Epoch[43](180/324): Loss: 1.3137 || Learning rate: lr=0.0001.
===> Epoch[43](190/324): Loss: 1.0890 || Learning rate: lr=0.0001.
===> Epoch[43](200/324): Loss: 1.1933 || Learning rate: lr=0.0001.
===> Epoch[43](210/324): Loss: 0.8648 || Learning rate: lr=0.0001.
===> Epoch[43](220/324): Loss: 0.8004 || Learning rate: lr=0.0001.
===> Epoch[43](230/324): Loss: 0.8709 || Learning rate: lr=0.0001.
===> Epoch[43](240/324): Loss: 1.0797 || Learning rate: lr=0.0001.
===> Epoch[43](250/324): Loss: 1.5019 || Learning rate: lr=0.0001.
===> Epoch[43](260/324): Loss: 0.9507 || Learning rate: lr=0.0001.
===> Epoch[43](270/324): Loss: 1.1089 || Learning rate: lr=0.0001.
===> Epoch[43](280/324): Loss: 1.5593 || Learning rate: lr=0.0001.
===> Epoch[43](290/324): Loss: 0.6810 || Learning rate: lr=0.0001.
===> Epoch[43](300/324): Loss: 2.0309 || Learning rate: lr=0.0001.
===> Epoch[43](310/324): Loss: 0.8344 || Learning rate: lr=0.0001.
===> Epoch[43](320/324): Loss: 2.0270 || Learning rate: lr=0.0001.
===> Epoch[44](10/324): Loss: 0.8586 || Learning rate: lr=0.0001.
===> Epoch[44](20/324): Loss: 1.3282 || Learning rate: lr=0.0001.
===> Epoch[44](30/324): Loss: 1.0473 || Learning rate: lr=0.0001.
===> Epoch[44](40/324): Loss: 1.2671 || Learning rate: lr=0.0001.
===> Epoch[44](50/324): Loss: 1.3414 || Learning rate: lr=0.0001.
===> Epoch[44](60/324): Loss: 1.3718 || Learning rate: lr=0.0001.
===> Epoch[44](70/324): Loss: 0.9978 || Learning rate: lr=0.0001.
===> Epoch[44](80/324): Loss: 1.1228 || Learning rate: lr=0.0001.
===> Epoch[44](90/324): Loss: 0.9757 || Learning rate: lr=0.0001.
===> Epoch[44](100/324): Loss: 1.2660 || Learning rate: lr=0.0001.
===> Epoch[44](110/324): Loss: 0.8143 || Learning rate: lr=0.0001.
===> Epoch[44](120/324): Loss: 0.9693 || Learning rate: lr=0.0001.
===> Epoch[44](130/324): Loss: 0.8090 || Learning rate: lr=0.0001.
===> Epoch[44](140/324): Loss: 1.2265 || Learning rate: lr=0.0001.
===> Epoch[44](150/324): Loss: 1.4154 || Learning rate: lr=0.0001.
===> Epoch[44](160/324): Loss: 1.0832 || Learning rate: lr=0.0001.
===> Epoch[44](170/324): Loss: 0.6987 || Learning rate: lr=0.0001.
===> Epoch[44](180/324): Loss: 1.3222 || Learning rate: lr=0.0001.
===> Epoch[44](190/324): Loss: 0.8692 || Learning rate: lr=0.0001.
===> Epoch[44](200/324): Loss: 0.9993 || Learning rate: lr=0.0001.
===> Epoch[44](210/324): Loss: 1.2845 || Learning rate: lr=0.0001.
===> Epoch[44](220/324): Loss: 1.7677 || Learning rate: lr=0.0001.
===> Epoch[44](230/324): Loss: 1.4747 || Learning rate: lr=0.0001.
===> Epoch[44](240/324): Loss: 1.0410 || Learning rate: lr=0.0001.
===> Epoch[44](250/324): Loss: 0.9572 || Learning rate: lr=0.0001.
===> Epoch[44](260/324): Loss: 1.0946 || Learning rate: lr=0.0001.
===> Epoch[44](270/324): Loss: 0.9708 || Learning rate: lr=0.0001.
===> Epoch[44](280/324): Loss: 1.0297 || Learning rate: lr=0.0001.
===> Epoch[44](290/324): Loss: 1.7969 || Learning rate: lr=0.0001.
===> Epoch[44](300/324): Loss: 1.9151 || Learning rate: lr=0.0001.
===> Epoch[44](310/324): Loss: 3.1181 || Learning rate: lr=0.0001.
===> Epoch[44](320/324): Loss: 2.8971 || Learning rate: lr=0.0001.
===> Epoch[45](10/324): Loss: 1.5873 || Learning rate: lr=0.0001.
===> Epoch[45](20/324): Loss: 3.2234 || Learning rate: lr=0.0001.
===> Epoch[45](30/324): Loss: 1.4064 || Learning rate: lr=0.0001.
===> Epoch[45](40/324): Loss: 3.1204 || Learning rate: lr=0.0001.
===> Epoch[45](50/324): Loss: 2.4862 || Learning rate: lr=0.0001.
===> Epoch[45](60/324): Loss: 2.2228 || Learning rate: lr=0.0001.
===> Epoch[45](70/324): Loss: 4.8432 || Learning rate: lr=0.0001.
===> Epoch[45](80/324): Loss: 2.7354 || Learning rate: lr=0.0001.
===> Epoch[45](90/324): Loss: 2.1182 || Learning rate: lr=0.0001.
===> Epoch[45](100/324): Loss: 1.8382 || Learning rate: lr=0.0001.
===> Epoch[45](110/324): Loss: 1.9759 || Learning rate: lr=0.0001.
===> Epoch[45](120/324): Loss: 2.0171 || Learning rate: lr=0.0001.
===> Epoch[45](130/324): Loss: 1.2795 || Learning rate: lr=0.0001.
===> Epoch[45](140/324): Loss: 1.3488 || Learning rate: lr=0.0001.
===> Epoch[45](150/324): Loss: 1.0216 || Learning rate: lr=0.0001.
===> Epoch[45](160/324): Loss: 0.9361 || Learning rate: lr=0.0001.
===> Epoch[45](170/324): Loss: 1.5145 || Learning rate: lr=0.0001.
===> Epoch[45](180/324): Loss: 1.0338 || Learning rate: lr=0.0001.
===> Epoch[45](190/324): Loss: 2.6092 || Learning rate: lr=0.0001.
===> Epoch[45](200/324): Loss: 1.3522 || Learning rate: lr=0.0001.
===> Epoch[45](210/324): Loss: 1.9406 || Learning rate: lr=0.0001.
===> Epoch[45](220/324): Loss: 0.9558 || Learning rate: lr=0.0001.
===> Epoch[45](230/324): Loss: 1.1212 || Learning rate: lr=0.0001.
===> Epoch[45](240/324): Loss: 1.0913 || Learning rate: lr=0.0001.
===> Epoch[45](250/324): Loss: 0.9803 || Learning rate: lr=0.0001.
===> Epoch[45](260/324): Loss: 2.2769 || Learning rate: lr=0.0001.
===> Epoch[45](270/324): Loss: 1.5563 || Learning rate: lr=0.0001.
===> Epoch[45](280/324): Loss: 1.7357 || Learning rate: lr=0.0001.
===> Epoch[45](290/324): Loss: 1.5736 || Learning rate: lr=0.0001.
===> Epoch[45](300/324): Loss: 0.8055 || Learning rate: lr=0.0001.
===> Epoch[45](310/324): Loss: 2.0953 || Learning rate: lr=0.0001.
===> Epoch[45](320/324): Loss: 0.9814 || Learning rate: lr=0.0001.
===> Epoch[46](10/324): Loss: 1.2023 || Learning rate: lr=0.0001.
===> Epoch[46](20/324): Loss: 1.4184 || Learning rate: lr=0.0001.
===> Epoch[46](30/324): Loss: 0.9527 || Learning rate: lr=0.0001.
===> Epoch[46](40/324): Loss: 0.6157 || Learning rate: lr=0.0001.
===> Epoch[46](50/324): Loss: 1.3481 || Learning rate: lr=0.0001.
===> Epoch[46](60/324): Loss: 1.3424 || Learning rate: lr=0.0001.
===> Epoch[46](70/324): Loss: 1.3921 || Learning rate: lr=0.0001.
===> Epoch[46](80/324): Loss: 1.0884 || Learning rate: lr=0.0001.
===> Epoch[46](90/324): Loss: 1.2854 || Learning rate: lr=0.0001.
===> Epoch[46](100/324): Loss: 1.2620 || Learning rate: lr=0.0001.
===> Epoch[46](110/324): Loss: 1.4461 || Learning rate: lr=0.0001.
===> Epoch[46](120/324): Loss: 1.0502 || Learning rate: lr=0.0001.
===> Epoch[46](130/324): Loss: 1.5962 || Learning rate: lr=0.0001.
===> Epoch[46](140/324): Loss: 0.5432 || Learning rate: lr=0.0001.
===> Epoch[46](150/324): Loss: 1.2718 || Learning rate: lr=0.0001.
===> Epoch[46](160/324): Loss: 1.2473 || Learning rate: lr=0.0001.
===> Epoch[46](170/324): Loss: 1.1577 || Learning rate: lr=0.0001.
===> Epoch[46](180/324): Loss: 1.2869 || Learning rate: lr=0.0001.
===> Epoch[46](190/324): Loss: 0.8015 || Learning rate: lr=0.0001.
===> Epoch[46](200/324): Loss: 1.1592 || Learning rate: lr=0.0001.
===> Epoch[46](210/324): Loss: 1.0660 || Learning rate: lr=0.0001.
===> Epoch[46](220/324): Loss: 1.7453 || Learning rate: lr=0.0001.
===> Epoch[46](230/324): Loss: 0.8874 || Learning rate: lr=0.0001.
===> Epoch[46](240/324): Loss: 1.2341 || Learning rate: lr=0.0001.
===> Epoch[46](250/324): Loss: 1.0363 || Learning rate: lr=0.0001.
===> Epoch[46](260/324): Loss: 1.7902 || Learning rate: lr=0.0001.
===> Epoch[46](270/324): Loss: 1.2305 || Learning rate: lr=0.0001.
===> Epoch[46](280/324): Loss: 0.7326 || Learning rate: lr=0.0001.
===> Epoch[46](290/324): Loss: 1.1113 || Learning rate: lr=0.0001.
===> Epoch[46](300/324): Loss: 0.9392 || Learning rate: lr=0.0001.
===> Epoch[46](310/324): Loss: 0.6880 || Learning rate: lr=0.0001.
===> Epoch[46](320/324): Loss: 1.0615 || Learning rate: lr=0.0001.
===> Epoch[47](10/324): Loss: 0.7491 || Learning rate: lr=0.0001.
===> Epoch[47](20/324): Loss: 1.0611 || Learning rate: lr=0.0001.
===> Epoch[47](30/324): Loss: 0.6502 || Learning rate: lr=0.0001.
===> Epoch[47](40/324): Loss: 1.3828 || Learning rate: lr=0.0001.
===> Epoch[47](50/324): Loss: 0.7841 || Learning rate: lr=0.0001.
===> Epoch[47](60/324): Loss: 0.8184 || Learning rate: lr=0.0001.
===> Epoch[47](70/324): Loss: 1.3649 || Learning rate: lr=0.0001.
===> Epoch[47](80/324): Loss: 1.1124 || Learning rate: lr=0.0001.
===> Epoch[47](90/324): Loss: 1.2169 || Learning rate: lr=0.0001.
===> Epoch[47](100/324): Loss: 1.2262 || Learning rate: lr=0.0001.
===> Epoch[47](110/324): Loss: 0.6730 || Learning rate: lr=0.0001.
===> Epoch[47](120/324): Loss: 1.3804 || Learning rate: lr=0.0001.
===> Epoch[47](130/324): Loss: 1.0319 || Learning rate: lr=0.0001.
===> Epoch[47](140/324): Loss: 1.1163 || Learning rate: lr=0.0001.
===> Epoch[47](150/324): Loss: 1.7679 || Learning rate: lr=0.0001.
===> Epoch[47](160/324): Loss: 0.8956 || Learning rate: lr=0.0001.
===> Epoch[47](170/324): Loss: 1.0709 || Learning rate: lr=0.0001.
===> Epoch[47](180/324): Loss: 1.4291 || Learning rate: lr=0.0001.
===> Epoch[47](190/324): Loss: 1.8426 || Learning rate: lr=0.0001.
===> Epoch[47](200/324): Loss: 0.8806 || Learning rate: lr=0.0001.
===> Epoch[47](210/324): Loss: 0.9201 || Learning rate: lr=0.0001.
===> Epoch[47](220/324): Loss: 1.0697 || Learning rate: lr=0.0001.
===> Epoch[47](230/324): Loss: 1.1759 || Learning rate: lr=0.0001.
===> Epoch[47](240/324): Loss: 1.3780 || Learning rate: lr=0.0001.
===> Epoch[47](250/324): Loss: 1.0065 || Learning rate: lr=0.0001.
===> Epoch[47](260/324): Loss: 0.7356 || Learning rate: lr=0.0001.
===> Epoch[47](270/324): Loss: 0.9747 || Learning rate: lr=0.0001.
===> Epoch[47](280/324): Loss: 1.1102 || Learning rate: lr=0.0001.
===> Epoch[47](290/324): Loss: 0.7142 || Learning rate: lr=0.0001.
===> Epoch[47](300/324): Loss: 1.3421 || Learning rate: lr=0.0001.
===> Epoch[47](310/324): Loss: 1.3498 || Learning rate: lr=0.0001.
===> Epoch[47](320/324): Loss: 0.8108 || Learning rate: lr=0.0001.
===> Epoch[48](10/324): Loss: 0.8617 || Learning rate: lr=0.0001.
===> Epoch[48](20/324): Loss: 1.2349 || Learning rate: lr=0.0001.
===> Epoch[48](30/324): Loss: 1.2824 || Learning rate: lr=0.0001.
===> Epoch[48](40/324): Loss: 0.8597 || Learning rate: lr=0.0001.
===> Epoch[48](50/324): Loss: 1.0274 || Learning rate: lr=0.0001.
===> Epoch[48](60/324): Loss: 1.1494 || Learning rate: lr=0.0001.
===> Epoch[48](70/324): Loss: 0.7986 || Learning rate: lr=0.0001.
===> Epoch[48](80/324): Loss: 0.9951 || Learning rate: lr=0.0001.
===> Epoch[48](90/324): Loss: 1.3658 || Learning rate: lr=0.0001.
===> Epoch[48](100/324): Loss: 1.3416 || Learning rate: lr=0.0001.
===> Epoch[48](110/324): Loss: 1.0301 || Learning rate: lr=0.0001.
===> Epoch[48](120/324): Loss: 0.8175 || Learning rate: lr=0.0001.
===> Epoch[48](130/324): Loss: 1.4067 || Learning rate: lr=0.0001.
===> Epoch[48](140/324): Loss: 1.4359 || Learning rate: lr=0.0001.
===> Epoch[48](150/324): Loss: 1.1842 || Learning rate: lr=0.0001.
===> Epoch[48](160/324): Loss: 1.3304 || Learning rate: lr=0.0001.
===> Epoch[48](170/324): Loss: 1.6296 || Learning rate: lr=0.0001.
===> Epoch[48](180/324): Loss: 0.7165 || Learning rate: lr=0.0001.
===> Epoch[48](190/324): Loss: 1.2203 || Learning rate: lr=0.0001.
===> Epoch[48](200/324): Loss: 1.2133 || Learning rate: lr=0.0001.
===> Epoch[48](210/324): Loss: 2.2228 || Learning rate: lr=0.0001.
===> Epoch[48](220/324): Loss: 2.3461 || Learning rate: lr=0.0001.
===> Epoch[48](230/324): Loss: 3.1144 || Learning rate: lr=0.0001.
===> Epoch[48](240/324): Loss: 3.0748 || Learning rate: lr=0.0001.
===> Epoch[48](250/324): Loss: 2.1661 || Learning rate: lr=0.0001.
===> Epoch[48](260/324): Loss: 0.7901 || Learning rate: lr=0.0001.
===> Epoch[48](270/324): Loss: 1.4193 || Learning rate: lr=0.0001.
===> Epoch[48](280/324): Loss: 0.7632 || Learning rate: lr=0.0001.
===> Epoch[48](290/324): Loss: 1.6086 || Learning rate: lr=0.0001.
===> Epoch[48](300/324): Loss: 0.7582 || Learning rate: lr=0.0001.
===> Epoch[48](310/324): Loss: 1.0173 || Learning rate: lr=0.0001.
===> Epoch[48](320/324): Loss: 0.8277 || Learning rate: lr=0.0001.
===> Epoch[49](10/324): Loss: 1.4707 || Learning rate: lr=0.0001.
===> Epoch[49](20/324): Loss: 1.3055 || Learning rate: lr=0.0001.
===> Epoch[49](30/324): Loss: 1.0028 || Learning rate: lr=0.0001.
===> Epoch[49](40/324): Loss: 1.0724 || Learning rate: lr=0.0001.
===> Epoch[49](50/324): Loss: 1.1416 || Learning rate: lr=0.0001.
===> Epoch[49](60/324): Loss: 1.2816 || Learning rate: lr=0.0001.
===> Epoch[49](70/324): Loss: 1.0293 || Learning rate: lr=0.0001.
===> Epoch[49](80/324): Loss: 1.4077 || Learning rate: lr=0.0001.
===> Epoch[49](90/324): Loss: 1.1472 || Learning rate: lr=0.0001.
===> Epoch[49](100/324): Loss: 0.8278 || Learning rate: lr=0.0001.
===> Epoch[49](110/324): Loss: 1.0727 || Learning rate: lr=0.0001.
===> Epoch[49](120/324): Loss: 0.5403 || Learning rate: lr=0.0001.
===> Epoch[49](130/324): Loss: 0.8644 || Learning rate: lr=0.0001.
===> Epoch[49](140/324): Loss: 0.9413 || Learning rate: lr=0.0001.
===> Epoch[49](150/324): Loss: 1.5644 || Learning rate: lr=0.0001.
===> Epoch[49](160/324): Loss: 1.0388 || Learning rate: lr=0.0001.
===> Epoch[49](170/324): Loss: 1.0593 || Learning rate: lr=0.0001.
===> Epoch[49](180/324): Loss: 1.0710 || Learning rate: lr=0.0001.
===> Epoch[49](190/324): Loss: 1.1421 || Learning rate: lr=0.0001.
===> Epoch[49](200/324): Loss: 1.3325 || Learning rate: lr=0.0001.
===> Epoch[49](210/324): Loss: 0.7525 || Learning rate: lr=0.0001.
===> Epoch[49](220/324): Loss: 1.1177 || Learning rate: lr=0.0001.
===> Epoch[49](230/324): Loss: 1.0621 || Learning rate: lr=0.0001.
===> Epoch[49](240/324): Loss: 0.7510 || Learning rate: lr=0.0001.
===> Epoch[49](250/324): Loss: 1.4582 || Learning rate: lr=0.0001.
===> Epoch[49](260/324): Loss: 1.3629 || Learning rate: lr=0.0001.
===> Epoch[49](270/324): Loss: 0.7352 || Learning rate: lr=0.0001.
===> Epoch[49](280/324): Loss: 0.9830 || Learning rate: lr=0.0001.
===> Epoch[49](290/324): Loss: 0.7843 || Learning rate: lr=0.0001.
===> Epoch[49](300/324): Loss: 0.9056 || Learning rate: lr=0.0001.
===> Epoch[49](310/324): Loss: 1.4928 || Learning rate: lr=0.0001.
===> Epoch[49](320/324): Loss: 1.2586 || Learning rate: lr=0.0001.
===> Epoch[50](10/324): Loss: 0.8124 || Learning rate: lr=0.0001.
===> Epoch[50](20/324): Loss: 1.2345 || Learning rate: lr=0.0001.
===> Epoch[50](30/324): Loss: 1.0417 || Learning rate: lr=0.0001.
===> Epoch[50](40/324): Loss: 1.5748 || Learning rate: lr=0.0001.
===> Epoch[50](50/324): Loss: 0.9900 || Learning rate: lr=0.0001.
===> Epoch[50](60/324): Loss: 1.2269 || Learning rate: lr=0.0001.
===> Epoch[50](70/324): Loss: 1.1144 || Learning rate: lr=0.0001.
===> Epoch[50](80/324): Loss: 1.1595 || Learning rate: lr=0.0001.
===> Epoch[50](90/324): Loss: 1.0243 || Learning rate: lr=0.0001.
===> Epoch[50](100/324): Loss: 1.2113 || Learning rate: lr=0.0001.
===> Epoch[50](110/324): Loss: 0.7526 || Learning rate: lr=0.0001.
===> Epoch[50](120/324): Loss: 0.9765 || Learning rate: lr=0.0001.
===> Epoch[50](130/324): Loss: 0.9845 || Learning rate: lr=0.0001.
===> Epoch[50](140/324): Loss: 1.9240 || Learning rate: lr=0.0001.
===> Epoch[50](150/324): Loss: 1.5050 || Learning rate: lr=0.0001.
===> Epoch[50](160/324): Loss: 0.7525 || Learning rate: lr=0.0001.
===> Epoch[50](170/324): Loss: 1.1239 || Learning rate: lr=0.0001.
===> Epoch[50](180/324): Loss: 1.0506 || Learning rate: lr=0.0001.
===> Epoch[50](190/324): Loss: 0.9455 || Learning rate: lr=0.0001.
===> Epoch[50](200/324): Loss: 0.6197 || Learning rate: lr=0.0001.
===> Epoch[50](210/324): Loss: 1.3395 || Learning rate: lr=0.0001.
===> Epoch[50](220/324): Loss: 0.9075 || Learning rate: lr=0.0001.
===> Epoch[50](230/324): Loss: 1.1179 || Learning rate: lr=0.0001.
===> Epoch[50](240/324): Loss: 1.0033 || Learning rate: lr=0.0001.
===> Epoch[50](250/324): Loss: 0.7493 || Learning rate: lr=0.0001.
===> Epoch[50](260/324): Loss: 0.9410 || Learning rate: lr=0.0001.
===> Epoch[50](270/324): Loss: 0.9657 || Learning rate: lr=0.0001.
===> Epoch[50](280/324): Loss: 1.0380 || Learning rate: lr=0.0001.
===> Epoch[50](290/324): Loss: 1.2064 || Learning rate: lr=0.0001.
===> Epoch[50](300/324): Loss: 1.0719 || Learning rate: lr=0.0001.
===> Epoch[50](310/324): Loss: 0.8399 || Learning rate: lr=0.0001.
===> Epoch[50](320/324): Loss: 1.4737 || Learning rate: lr=0.0001.
===> Epoch[51](10/324): Loss: 0.9560 || Learning rate: lr=0.0001.
===> Epoch[51](20/324): Loss: 1.2181 || Learning rate: lr=0.0001.
===> Epoch[51](30/324): Loss: 0.5140 || Learning rate: lr=0.0001.
===> Epoch[51](40/324): Loss: 1.2651 || Learning rate: lr=0.0001.
===> Epoch[51](50/324): Loss: 0.9620 || Learning rate: lr=0.0001.
===> Epoch[51](60/324): Loss: 0.5845 || Learning rate: lr=0.0001.
===> Epoch[51](70/324): Loss: 1.0465 || Learning rate: lr=0.0001.
===> Epoch[51](80/324): Loss: 1.6185 || Learning rate: lr=0.0001.
===> Epoch[51](90/324): Loss: 1.3621 || Learning rate: lr=0.0001.
===> Epoch[51](100/324): Loss: 2.0444 || Learning rate: lr=0.0001.
===> Epoch[51](110/324): Loss: 1.4284 || Learning rate: lr=0.0001.
===> Epoch[51](120/324): Loss: 1.0987 || Learning rate: lr=0.0001.
===> Epoch[51](130/324): Loss: 1.6824 || Learning rate: lr=0.0001.
===> Epoch[51](140/324): Loss: 1.0873 || Learning rate: lr=0.0001.
===> Epoch[51](150/324): Loss: 1.5004 || Learning rate: lr=0.0001.
===> Epoch[51](160/324): Loss: 1.6068 || Learning rate: lr=0.0001.
===> Epoch[51](170/324): Loss: 1.7503 || Learning rate: lr=0.0001.
===> Epoch[51](180/324): Loss: 1.3276 || Learning rate: lr=0.0001.
===> Epoch[51](190/324): Loss: 0.9056 || Learning rate: lr=0.0001.
===> Epoch[51](200/324): Loss: 1.4201 || Learning rate: lr=0.0001.
===> Epoch[51](210/324): Loss: 1.0002 || Learning rate: lr=0.0001.
===> Epoch[51](220/324): Loss: 0.9961 || Learning rate: lr=0.0001.
===> Epoch[51](230/324): Loss: 0.7615 || Learning rate: lr=0.0001.
===> Epoch[51](240/324): Loss: 1.2518 || Learning rate: lr=0.0001.
===> Epoch[51](250/324): Loss: 1.2959 || Learning rate: lr=0.0001.
===> Epoch[51](260/324): Loss: 1.0266 || Learning rate: lr=0.0001.
===> Epoch[51](270/324): Loss: 2.0678 || Learning rate: lr=0.0001.
===> Epoch[51](280/324): Loss: 1.0421 || Learning rate: lr=0.0001.
===> Epoch[51](290/324): Loss: 1.3478 || Learning rate: lr=0.0001.
===> Epoch[51](300/324): Loss: 1.3702 || Learning rate: lr=0.0001.
===> Epoch[51](310/324): Loss: 0.8974 || Learning rate: lr=0.0001.
===> Epoch[51](320/324): Loss: 0.7266 || Learning rate: lr=0.0001.
===> Epoch[52](10/324): Loss: 1.2624 || Learning rate: lr=0.0001.
===> Epoch[52](20/324): Loss: 0.9256 || Learning rate: lr=0.0001.
===> Epoch[52](30/324): Loss: 0.8668 || Learning rate: lr=0.0001.
===> Epoch[52](40/324): Loss: 1.1130 || Learning rate: lr=0.0001.
===> Epoch[52](50/324): Loss: 0.9959 || Learning rate: lr=0.0001.
===> Epoch[52](60/324): Loss: 1.1593 || Learning rate: lr=0.0001.
===> Epoch[52](70/324): Loss: 1.6747 || Learning rate: lr=0.0001.
===> Epoch[52](80/324): Loss: 0.9907 || Learning rate: lr=0.0001.
===> Epoch[52](90/324): Loss: 1.0353 || Learning rate: lr=0.0001.
===> Epoch[52](100/324): Loss: 0.8621 || Learning rate: lr=0.0001.
===> Epoch[52](110/324): Loss: 1.2186 || Learning rate: lr=0.0001.
===> Epoch[52](120/324): Loss: 1.1517 || Learning rate: lr=0.0001.
===> Epoch[52](130/324): Loss: 0.8468 || Learning rate: lr=0.0001.
===> Epoch[52](140/324): Loss: 0.9616 || Learning rate: lr=0.0001.
===> Epoch[52](150/324): Loss: 1.1975 || Learning rate: lr=0.0001.
===> Epoch[52](160/324): Loss: 0.9675 || Learning rate: lr=0.0001.
===> Epoch[52](170/324): Loss: 0.8162 || Learning rate: lr=0.0001.
===> Epoch[52](180/324): Loss: 1.0632 || Learning rate: lr=0.0001.
===> Epoch[52](190/324): Loss: 0.8230 || Learning rate: lr=0.0001.
===> Epoch[52](200/324): Loss: 0.9947 || Learning rate: lr=0.0001.
===> Epoch[52](210/324): Loss: 1.4890 || Learning rate: lr=0.0001.
===> Epoch[52](220/324): Loss: 1.0709 || Learning rate: lr=0.0001.
===> Epoch[52](230/324): Loss: 1.0148 || Learning rate: lr=0.0001.
===> Epoch[52](240/324): Loss: 0.9382 || Learning rate: lr=0.0001.
===> Epoch[52](250/324): Loss: 0.8770 || Learning rate: lr=0.0001.
===> Epoch[52](260/324): Loss: 1.4377 || Learning rate: lr=0.0001.
===> Epoch[52](270/324): Loss: 1.3988 || Learning rate: lr=0.0001.
===> Epoch[52](280/324): Loss: 0.6688 || Learning rate: lr=0.0001.
===> Epoch[52](290/324): Loss: 0.8389 || Learning rate: lr=0.0001.
===> Epoch[52](300/324): Loss: 1.3923 || Learning rate: lr=0.0001.
===> Epoch[52](310/324): Loss: 0.7183 || Learning rate: lr=0.0001.
===> Epoch[52](320/324): Loss: 0.8798 || Learning rate: lr=0.0001.
===> Epoch[53](10/324): Loss: 1.2035 || Learning rate: lr=0.0001.
===> Epoch[53](20/324): Loss: 0.9835 || Learning rate: lr=0.0001.
===> Epoch[53](30/324): Loss: 1.4413 || Learning rate: lr=0.0001.
===> Epoch[53](40/324): Loss: 0.6330 || Learning rate: lr=0.0001.
===> Epoch[53](50/324): Loss: 1.3100 || Learning rate: lr=0.0001.
===> Epoch[53](60/324): Loss: 1.4417 || Learning rate: lr=0.0001.
===> Epoch[53](70/324): Loss: 1.0179 || Learning rate: lr=0.0001.
===> Epoch[53](80/324): Loss: 1.2141 || Learning rate: lr=0.0001.
===> Epoch[53](90/324): Loss: 0.7955 || Learning rate: lr=0.0001.
===> Epoch[53](100/324): Loss: 1.2500 || Learning rate: lr=0.0001.
===> Epoch[53](110/324): Loss: 1.7278 || Learning rate: lr=0.0001.
===> Epoch[53](120/324): Loss: 1.6207 || Learning rate: lr=0.0001.
===> Epoch[53](130/324): Loss: 1.2995 || Learning rate: lr=0.0001.
===> Epoch[53](140/324): Loss: 3.2492 || Learning rate: lr=0.0001.
===> Epoch[53](150/324): Loss: 1.9398 || Learning rate: lr=0.0001.
===> Epoch[53](160/324): Loss: 1.6442 || Learning rate: lr=0.0001.
===> Epoch[53](170/324): Loss: 2.0661 || Learning rate: lr=0.0001.
===> Epoch[53](180/324): Loss: 1.0305 || Learning rate: lr=0.0001.
===> Epoch[53](190/324): Loss: 1.8074 || Learning rate: lr=0.0001.
===> Epoch[53](200/324): Loss: 1.3866 || Learning rate: lr=0.0001.
===> Epoch[53](210/324): Loss: 0.9187 || Learning rate: lr=0.0001.
===> Epoch[53](220/324): Loss: 0.8632 || Learning rate: lr=0.0001.
===> Epoch[53](230/324): Loss: 1.3016 || Learning rate: lr=0.0001.
===> Epoch[53](240/324): Loss: 1.4063 || Learning rate: lr=0.0001.
===> Epoch[53](250/324): Loss: 1.0653 || Learning rate: lr=0.0001.
===> Epoch[53](260/324): Loss: 2.4445 || Learning rate: lr=0.0001.
===> Epoch[53](270/324): Loss: 1.5686 || Learning rate: lr=0.0001.
===> Epoch[53](280/324): Loss: 1.4389 || Learning rate: lr=0.0001.
===> Epoch[53](290/324): Loss: 1.3424 || Learning rate: lr=0.0001.
===> Epoch[53](300/324): Loss: 1.6230 || Learning rate: lr=0.0001.
===> Epoch[53](310/324): Loss: 1.0311 || Learning rate: lr=0.0001.
===> Epoch[53](320/324): Loss: 1.2731 || Learning rate: lr=0.0001.
===> Epoch[54](10/324): Loss: 1.7207 || Learning rate: lr=0.0001.
===> Epoch[54](20/324): Loss: 1.1149 || Learning rate: lr=0.0001.
===> Epoch[54](30/324): Loss: 1.4814 || Learning rate: lr=0.0001.
===> Epoch[54](40/324): Loss: 1.0236 || Learning rate: lr=0.0001.
===> Epoch[54](50/324): Loss: 1.0986 || Learning rate: lr=0.0001.
===> Epoch[54](60/324): Loss: 1.9300 || Learning rate: lr=0.0001.
===> Epoch[54](70/324): Loss: 0.7767 || Learning rate: lr=0.0001.
===> Epoch[54](80/324): Loss: 1.0364 || Learning rate: lr=0.0001.
===> Epoch[54](90/324): Loss: 0.8613 || Learning rate: lr=0.0001.
===> Epoch[54](100/324): Loss: 1.1649 || Learning rate: lr=0.0001.
===> Epoch[54](110/324): Loss: 1.5717 || Learning rate: lr=0.0001.
===> Epoch[54](120/324): Loss: 1.1605 || Learning rate: lr=0.0001.
===> Epoch[54](130/324): Loss: 1.2545 || Learning rate: lr=0.0001.
===> Epoch[54](140/324): Loss: 0.9170 || Learning rate: lr=0.0001.
===> Epoch[54](150/324): Loss: 0.8512 || Learning rate: lr=0.0001.
===> Epoch[54](160/324): Loss: 1.3318 || Learning rate: lr=0.0001.
===> Epoch[54](170/324): Loss: 0.8333 || Learning rate: lr=0.0001.
===> Epoch[54](180/324): Loss: 1.1169 || Learning rate: lr=0.0001.
===> Epoch[54](190/324): Loss: 1.2522 || Learning rate: lr=0.0001.
===> Epoch[54](200/324): Loss: 2.0565 || Learning rate: lr=0.0001.
===> Epoch[54](210/324): Loss: 1.5669 || Learning rate: lr=0.0001.
===> Epoch[54](220/324): Loss: 2.4661 || Learning rate: lr=0.0001.
===> Epoch[54](230/324): Loss: 1.0408 || Learning rate: lr=0.0001.
===> Epoch[54](240/324): Loss: 2.0972 || Learning rate: lr=0.0001.
===> Epoch[54](250/324): Loss: 3.2419 || Learning rate: lr=0.0001.
===> Epoch[54](260/324): Loss: 1.8666 || Learning rate: lr=0.0001.
===> Epoch[54](270/324): Loss: 0.8276 || Learning rate: lr=0.0001.
===> Epoch[54](280/324): Loss: 1.1820 || Learning rate: lr=0.0001.
===> Epoch[54](290/324): Loss: 1.0315 || Learning rate: lr=0.0001.
===> Epoch[54](300/324): Loss: 1.3290 || Learning rate: lr=0.0001.
===> Epoch[54](310/324): Loss: 0.9418 || Learning rate: lr=0.0001.
===> Epoch[54](320/324): Loss: 2.0198 || Learning rate: lr=0.0001.
===> Epoch[55](10/324): Loss: 0.5989 || Learning rate: lr=0.0001.
===> Epoch[55](20/324): Loss: 1.1933 || Learning rate: lr=0.0001.
===> Epoch[55](30/324): Loss: 0.6658 || Learning rate: lr=0.0001.
===> Epoch[55](40/324): Loss: 0.7260 || Learning rate: lr=0.0001.
===> Epoch[55](50/324): Loss: 1.0085 || Learning rate: lr=0.0001.
===> Epoch[55](60/324): Loss: 0.9120 || Learning rate: lr=0.0001.
===> Epoch[55](70/324): Loss: 0.6816 || Learning rate: lr=0.0001.
===> Epoch[55](80/324): Loss: 1.0104 || Learning rate: lr=0.0001.
===> Epoch[55](90/324): Loss: 0.9138 || Learning rate: lr=0.0001.
===> Epoch[55](100/324): Loss: 1.3561 || Learning rate: lr=0.0001.
===> Epoch[55](110/324): Loss: 0.7258 || Learning rate: lr=0.0001.
===> Epoch[55](120/324): Loss: 0.8121 || Learning rate: lr=0.0001.
===> Epoch[55](130/324): Loss: 1.3618 || Learning rate: lr=0.0001.
===> Epoch[55](140/324): Loss: 1.0111 || Learning rate: lr=0.0001.
===> Epoch[55](150/324): Loss: 0.7468 || Learning rate: lr=0.0001.
===> Epoch[55](160/324): Loss: 1.2481 || Learning rate: lr=0.0001.
===> Epoch[55](170/324): Loss: 0.9824 || Learning rate: lr=0.0001.
===> Epoch[55](180/324): Loss: 0.9329 || Learning rate: lr=0.0001.
===> Epoch[55](190/324): Loss: 1.0176 || Learning rate: lr=0.0001.
===> Epoch[55](200/324): Loss: 1.4484 || Learning rate: lr=0.0001.
===> Epoch[55](210/324): Loss: 0.5020 || Learning rate: lr=0.0001.
===> Epoch[55](220/324): Loss: 1.3060 || Learning rate: lr=0.0001.
===> Epoch[55](230/324): Loss: 0.8281 || Learning rate: lr=0.0001.
===> Epoch[55](240/324): Loss: 0.9243 || Learning rate: lr=0.0001.
===> Epoch[55](250/324): Loss: 1.1018 || Learning rate: lr=0.0001.
===> Epoch[55](260/324): Loss: 1.2397 || Learning rate: lr=0.0001.
===> Epoch[55](270/324): Loss: 1.0140 || Learning rate: lr=0.0001.
===> Epoch[55](280/324): Loss: 1.0582 || Learning rate: lr=0.0001.
===> Epoch[55](290/324): Loss: 1.3506 || Learning rate: lr=0.0001.
===> Epoch[55](300/324): Loss: 0.9639 || Learning rate: lr=0.0001.
===> Epoch[55](310/324): Loss: 1.9087 || Learning rate: lr=0.0001.
===> Epoch[55](320/324): Loss: 1.0689 || Learning rate: lr=0.0001.
===> Epoch[56](10/324): Loss: 0.6421 || Learning rate: lr=0.0001.
===> Epoch[56](20/324): Loss: 0.8984 || Learning rate: lr=0.0001.
===> Epoch[56](30/324): Loss: 1.0064 || Learning rate: lr=0.0001.
===> Epoch[56](40/324): Loss: 0.9375 || Learning rate: lr=0.0001.
===> Epoch[56](50/324): Loss: 1.2253 || Learning rate: lr=0.0001.
===> Epoch[56](60/324): Loss: 0.9309 || Learning rate: lr=0.0001.
===> Epoch[56](70/324): Loss: 0.6694 || Learning rate: lr=0.0001.
===> Epoch[56](80/324): Loss: 0.7950 || Learning rate: lr=0.0001.
===> Epoch[56](90/324): Loss: 1.1051 || Learning rate: lr=0.0001.
===> Epoch[56](100/324): Loss: 0.8899 || Learning rate: lr=0.0001.
===> Epoch[56](110/324): Loss: 1.2285 || Learning rate: lr=0.0001.
===> Epoch[56](120/324): Loss: 1.3199 || Learning rate: lr=0.0001.
===> Epoch[56](130/324): Loss: 0.9747 || Learning rate: lr=0.0001.
===> Epoch[56](140/324): Loss: 0.8303 || Learning rate: lr=0.0001.
===> Epoch[56](150/324): Loss: 1.0581 || Learning rate: lr=0.0001.
===> Epoch[56](160/324): Loss: 0.8477 || Learning rate: lr=0.0001.
===> Epoch[56](170/324): Loss: 1.4264 || Learning rate: lr=0.0001.
===> Epoch[56](180/324): Loss: 1.7424 || Learning rate: lr=0.0001.
===> Epoch[56](190/324): Loss: 1.0890 || Learning rate: lr=0.0001.
===> Epoch[56](200/324): Loss: 1.6764 || Learning rate: lr=0.0001.
===> Epoch[56](210/324): Loss: 1.2672 || Learning rate: lr=0.0001.
===> Epoch[56](220/324): Loss: 1.3311 || Learning rate: lr=0.0001.
===> Epoch[56](230/324): Loss: 1.7871 || Learning rate: lr=0.0001.
===> Epoch[56](240/324): Loss: 1.3308 || Learning rate: lr=0.0001.
===> Epoch[56](250/324): Loss: 2.0327 || Learning rate: lr=0.0001.
===> Epoch[56](260/324): Loss: 1.1457 || Learning rate: lr=0.0001.
===> Epoch[56](270/324): Loss: 1.3035 || Learning rate: lr=0.0001.
===> Epoch[56](280/324): Loss: 0.7762 || Learning rate: lr=0.0001.
===> Epoch[56](290/324): Loss: 0.5077 || Learning rate: lr=0.0001.
===> Epoch[56](300/324): Loss: 1.2535 || Learning rate: lr=0.0001.
===> Epoch[56](310/324): Loss: 1.1880 || Learning rate: lr=0.0001.
===> Epoch[56](320/324): Loss: 2.0234 || Learning rate: lr=0.0001.
===> Epoch[57](10/324): Loss: 1.1442 || Learning rate: lr=0.0001.
===> Epoch[57](20/324): Loss: 1.6407 || Learning rate: lr=0.0001.
===> Epoch[57](30/324): Loss: 2.1618 || Learning rate: lr=0.0001.
===> Epoch[57](40/324): Loss: 1.2337 || Learning rate: lr=0.0001.
===> Epoch[57](50/324): Loss: 1.7222 || Learning rate: lr=0.0001.
===> Epoch[57](60/324): Loss: 1.8722 || Learning rate: lr=0.0001.
===> Epoch[57](70/324): Loss: 1.2767 || Learning rate: lr=0.0001.
===> Epoch[57](80/324): Loss: 0.8926 || Learning rate: lr=0.0001.
===> Epoch[57](90/324): Loss: 1.2622 || Learning rate: lr=0.0001.
===> Epoch[57](100/324): Loss: 1.3653 || Learning rate: lr=0.0001.
===> Epoch[57](110/324): Loss: 1.1873 || Learning rate: lr=0.0001.
===> Epoch[57](120/324): Loss: 1.0744 || Learning rate: lr=0.0001.
===> Epoch[57](130/324): Loss: 0.9736 || Learning rate: lr=0.0001.
===> Epoch[57](140/324): Loss: 1.1519 || Learning rate: lr=0.0001.
===> Epoch[57](150/324): Loss: 1.6275 || Learning rate: lr=0.0001.
===> Epoch[57](160/324): Loss: 0.9761 || Learning rate: lr=0.0001.
===> Epoch[57](170/324): Loss: 1.0768 || Learning rate: lr=0.0001.
===> Epoch[57](180/324): Loss: 1.5029 || Learning rate: lr=0.0001.
===> Epoch[57](190/324): Loss: 0.6752 || Learning rate: lr=0.0001.
===> Epoch[57](200/324): Loss: 0.8225 || Learning rate: lr=0.0001.
===> Epoch[57](210/324): Loss: 1.5009 || Learning rate: lr=0.0001.
===> Epoch[57](220/324): Loss: 2.0688 || Learning rate: lr=0.0001.
===> Epoch[57](230/324): Loss: 3.5605 || Learning rate: lr=0.0001.
===> Epoch[57](240/324): Loss: 1.4456 || Learning rate: lr=0.0001.
===> Epoch[57](250/324): Loss: 2.5724 || Learning rate: lr=0.0001.
===> Epoch[57](260/324): Loss: 1.7234 || Learning rate: lr=0.0001.
===> Epoch[57](270/324): Loss: 1.0217 || Learning rate: lr=0.0001.
===> Epoch[57](280/324): Loss: 1.6969 || Learning rate: lr=0.0001.
===> Epoch[57](290/324): Loss: 1.4883 || Learning rate: lr=0.0001.
===> Epoch[57](300/324): Loss: 0.7806 || Learning rate: lr=0.0001.
===> Epoch[57](310/324): Loss: 1.4218 || Learning rate: lr=0.0001.
===> Epoch[57](320/324): Loss: 0.5980 || Learning rate: lr=0.0001.
===> Epoch[58](10/324): Loss: 1.3379 || Learning rate: lr=0.0001.
===> Epoch[58](20/324): Loss: 2.0828 || Learning rate: lr=0.0001.
===> Epoch[58](30/324): Loss: 1.8857 || Learning rate: lr=0.0001.
===> Epoch[58](40/324): Loss: 1.6054 || Learning rate: lr=0.0001.
===> Epoch[58](50/324): Loss: 0.8221 || Learning rate: lr=0.0001.
===> Epoch[58](60/324): Loss: 1.3591 || Learning rate: lr=0.0001.
===> Epoch[58](70/324): Loss: 1.1497 || Learning rate: lr=0.0001.
===> Epoch[58](80/324): Loss: 0.8392 || Learning rate: lr=0.0001.
===> Epoch[58](90/324): Loss: 1.4927 || Learning rate: lr=0.0001.
===> Epoch[58](100/324): Loss: 1.2096 || Learning rate: lr=0.0001.
===> Epoch[58](110/324): Loss: 0.9923 || Learning rate: lr=0.0001.
===> Epoch[58](120/324): Loss: 1.0238 || Learning rate: lr=0.0001.
===> Epoch[58](130/324): Loss: 1.6842 || Learning rate: lr=0.0001.
===> Epoch[58](140/324): Loss: 1.0029 || Learning rate: lr=0.0001.
===> Epoch[58](150/324): Loss: 1.0001 || Learning rate: lr=0.0001.
===> Epoch[58](160/324): Loss: 1.4911 || Learning rate: lr=0.0001.
===> Epoch[58](170/324): Loss: 1.2053 || Learning rate: lr=0.0001.
===> Epoch[58](180/324): Loss: 1.3915 || Learning rate: lr=0.0001.
===> Epoch[58](190/324): Loss: 0.8525 || Learning rate: lr=0.0001.
===> Epoch[58](200/324): Loss: 0.6366 || Learning rate: lr=0.0001.
===> Epoch[58](210/324): Loss: 1.1728 || Learning rate: lr=0.0001.
===> Epoch[58](220/324): Loss: 1.0936 || Learning rate: lr=0.0001.
===> Epoch[58](230/324): Loss: 1.1739 || Learning rate: lr=0.0001.
===> Epoch[58](240/324): Loss: 1.0229 || Learning rate: lr=0.0001.
===> Epoch[58](250/324): Loss: 1.0115 || Learning rate: lr=0.0001.
===> Epoch[58](260/324): Loss: 1.1407 || Learning rate: lr=0.0001.
===> Epoch[58](270/324): Loss: 1.1732 || Learning rate: lr=0.0001.
===> Epoch[58](280/324): Loss: 3.2590 || Learning rate: lr=0.0001.
===> Epoch[58](290/324): Loss: 2.0881 || Learning rate: lr=0.0001.
===> Epoch[58](300/324): Loss: 1.1473 || Learning rate: lr=0.0001.
===> Epoch[58](310/324): Loss: 1.3816 || Learning rate: lr=0.0001.
===> Epoch[58](320/324): Loss: 1.3102 || Learning rate: lr=0.0001.
===> Epoch[59](10/324): Loss: 1.1993 || Learning rate: lr=0.0001.
===> Epoch[59](20/324): Loss: 1.3526 || Learning rate: lr=0.0001.
===> Epoch[59](30/324): Loss: 0.7908 || Learning rate: lr=0.0001.
===> Epoch[59](40/324): Loss: 0.6626 || Learning rate: lr=0.0001.
===> Epoch[59](50/324): Loss: 1.0229 || Learning rate: lr=0.0001.
===> Epoch[59](60/324): Loss: 1.7080 || Learning rate: lr=0.0001.
===> Epoch[59](70/324): Loss: 0.8408 || Learning rate: lr=0.0001.
===> Epoch[59](80/324): Loss: 1.7504 || Learning rate: lr=0.0001.
===> Epoch[59](90/324): Loss: 1.1722 || Learning rate: lr=0.0001.
===> Epoch[59](100/324): Loss: 1.6879 || Learning rate: lr=0.0001.
===> Epoch[59](110/324): Loss: 1.4634 || Learning rate: lr=0.0001.
===> Epoch[59](120/324): Loss: 1.2034 || Learning rate: lr=0.0001.
===> Epoch[59](130/324): Loss: 1.0914 || Learning rate: lr=0.0001.
===> Epoch[59](140/324): Loss: 1.4223 || Learning rate: lr=0.0001.
===> Epoch[59](150/324): Loss: 0.7643 || Learning rate: lr=0.0001.
===> Epoch[59](160/324): Loss: 1.1043 || Learning rate: lr=0.0001.
===> Epoch[59](170/324): Loss: 1.1138 || Learning rate: lr=0.0001.
===> Epoch[59](180/324): Loss: 0.5240 || Learning rate: lr=0.0001.
===> Epoch[59](190/324): Loss: 1.3956 || Learning rate: lr=0.0001.
===> Epoch[59](200/324): Loss: 0.9768 || Learning rate: lr=0.0001.
===> Epoch[59](210/324): Loss: 0.9626 || Learning rate: lr=0.0001.
===> Epoch[59](220/324): Loss: 1.3619 || Learning rate: lr=0.0001.
===> Epoch[59](230/324): Loss: 1.6468 || Learning rate: lr=0.0001.
===> Epoch[59](240/324): Loss: 0.9721 || Learning rate: lr=0.0001.
===> Epoch[59](250/324): Loss: 0.9564 || Learning rate: lr=0.0001.
===> Epoch[59](260/324): Loss: 0.8270 || Learning rate: lr=0.0001.
===> Epoch[59](270/324): Loss: 1.0424 || Learning rate: lr=0.0001.
===> Epoch[59](280/324): Loss: 1.5962 || Learning rate: lr=0.0001.
===> Epoch[59](290/324): Loss: 0.5982 || Learning rate: lr=0.0001.
===> Epoch[59](300/324): Loss: 1.0833 || Learning rate: lr=0.0001.
===> Epoch[59](310/324): Loss: 1.1658 || Learning rate: lr=0.0001.
===> Epoch[59](320/324): Loss: 1.6964 || Learning rate: lr=0.0001.
===> Epoch[60](10/324): Loss: 1.1066 || Learning rate: lr=0.0001.
===> Epoch[60](20/324): Loss: 1.1141 || Learning rate: lr=0.0001.
===> Epoch[60](30/324): Loss: 0.5505 || Learning rate: lr=0.0001.
===> Epoch[60](40/324): Loss: 0.6207 || Learning rate: lr=0.0001.
===> Epoch[60](50/324): Loss: 1.3187 || Learning rate: lr=0.0001.
===> Epoch[60](60/324): Loss: 1.0666 || Learning rate: lr=0.0001.
===> Epoch[60](70/324): Loss: 1.1628 || Learning rate: lr=0.0001.
===> Epoch[60](80/324): Loss: 0.7285 || Learning rate: lr=0.0001.
===> Epoch[60](90/324): Loss: 1.2728 || Learning rate: lr=0.0001.
===> Epoch[60](100/324): Loss: 0.9784 || Learning rate: lr=0.0001.
===> Epoch[60](110/324): Loss: 1.4538 || Learning rate: lr=0.0001.
===> Epoch[60](120/324): Loss: 0.8457 || Learning rate: lr=0.0001.
===> Epoch[60](130/324): Loss: 1.3900 || Learning rate: lr=0.0001.
===> Epoch[60](140/324): Loss: 1.3915 || Learning rate: lr=0.0001.
===> Epoch[60](150/324): Loss: 0.7173 || Learning rate: lr=0.0001.
===> Epoch[60](160/324): Loss: 1.1768 || Learning rate: lr=0.0001.
===> Epoch[60](170/324): Loss: 1.1563 || Learning rate: lr=0.0001.
===> Epoch[60](180/324): Loss: 0.8344 || Learning rate: lr=0.0001.
===> Epoch[60](190/324): Loss: 1.1920 || Learning rate: lr=0.0001.
===> Epoch[60](200/324): Loss: 0.7159 || Learning rate: lr=0.0001.
===> Epoch[60](210/324): Loss: 0.7161 || Learning rate: lr=0.0001.
===> Epoch[60](220/324): Loss: 1.1505 || Learning rate: lr=0.0001.
===> Epoch[60](230/324): Loss: 1.1767 || Learning rate: lr=0.0001.
===> Epoch[60](240/324): Loss: 1.0680 || Learning rate: lr=0.0001.
===> Epoch[60](250/324): Loss: 0.8646 || Learning rate: lr=0.0001.
===> Epoch[60](260/324): Loss: 1.2245 || Learning rate: lr=0.0001.
===> Epoch[60](270/324): Loss: 0.6418 || Learning rate: lr=0.0001.
===> Epoch[60](280/324): Loss: 1.1972 || Learning rate: lr=0.0001.
===> Epoch[60](290/324): Loss: 1.0138 || Learning rate: lr=0.0001.
===> Epoch[60](300/324): Loss: 1.3683 || Learning rate: lr=0.0001.
===> Epoch[60](310/324): Loss: 1.4193 || Learning rate: lr=0.0001.
===> Epoch[60](320/324): Loss: 1.1028 || Learning rate: lr=0.0001.
Checkpoint saved to weights/epoch_v2_60.pth
===> Epoch[61](10/324): Loss: 0.7814 || Learning rate: lr=0.0001.
===> Epoch[61](20/324): Loss: 1.3343 || Learning rate: lr=0.0001.
===> Epoch[61](30/324): Loss: 1.0828 || Learning rate: lr=0.0001.
===> Epoch[61](40/324): Loss: 0.7482 || Learning rate: lr=0.0001.
===> Epoch[61](50/324): Loss: 1.1504 || Learning rate: lr=0.0001.
===> Epoch[61](60/324): Loss: 1.3856 || Learning rate: lr=0.0001.
===> Epoch[61](70/324): Loss: 5.5026 || Learning rate: lr=0.0001.
===> Epoch[61](80/324): Loss: 4.3693 || Learning rate: lr=0.0001.
===> Epoch[61](90/324): Loss: 1.3939 || Learning rate: lr=0.0001.
===> Epoch[61](100/324): Loss: 1.5379 || Learning rate: lr=0.0001.
===> Epoch[61](110/324): Loss: 1.1472 || Learning rate: lr=0.0001.
===> Epoch[61](120/324): Loss: 1.4897 || Learning rate: lr=0.0001.
===> Epoch[61](130/324): Loss: 1.1117 || Learning rate: lr=0.0001.
===> Epoch[61](140/324): Loss: 1.2931 || Learning rate: lr=0.0001.
===> Epoch[61](150/324): Loss: 0.9054 || Learning rate: lr=0.0001.
===> Epoch[61](160/324): Loss: 0.7592 || Learning rate: lr=0.0001.
===> Epoch[61](170/324): Loss: 0.9309 || Learning rate: lr=0.0001.
===> Epoch[61](180/324): Loss: 1.0947 || Learning rate: lr=0.0001.
===> Epoch[61](190/324): Loss: 0.9698 || Learning rate: lr=0.0001.
===> Epoch[61](200/324): Loss: 1.0914 || Learning rate: lr=0.0001.
===> Epoch[61](210/324): Loss: 1.0326 || Learning rate: lr=0.0001.
===> Epoch[61](220/324): Loss: 1.0041 || Learning rate: lr=0.0001.
===> Epoch[61](230/324): Loss: 0.8943 || Learning rate: lr=0.0001.
===> Epoch[61](240/324): Loss: 0.9981 || Learning rate: lr=0.0001.
===> Epoch[61](250/324): Loss: 0.7495 || Learning rate: lr=0.0001.
===> Epoch[61](260/324): Loss: 0.9182 || Learning rate: lr=0.0001.
===> Epoch[61](270/324): Loss: 1.1973 || Learning rate: lr=0.0001.
===> Epoch[61](280/324): Loss: 1.2646 || Learning rate: lr=0.0001.
===> Epoch[61](290/324): Loss: 0.6235 || Learning rate: lr=0.0001.
===> Epoch[61](300/324): Loss: 1.1085 || Learning rate: lr=0.0001.
===> Epoch[61](310/324): Loss: 0.7467 || Learning rate: lr=0.0001.
===> Epoch[61](320/324): Loss: 1.0518 || Learning rate: lr=0.0001.
===> Epoch[62](10/324): Loss: 0.6701 || Learning rate: lr=0.0001.
===> Epoch[62](20/324): Loss: 1.0971 || Learning rate: lr=0.0001.
===> Epoch[62](30/324): Loss: 0.7981 || Learning rate: lr=0.0001.
===> Epoch[62](40/324): Loss: 0.9844 || Learning rate: lr=0.0001.
===> Epoch[62](50/324): Loss: 0.8872 || Learning rate: lr=0.0001.
===> Epoch[62](60/324): Loss: 1.1688 || Learning rate: lr=0.0001.
===> Epoch[62](70/324): Loss: 1.2322 || Learning rate: lr=0.0001.
===> Epoch[62](80/324): Loss: 0.6007 || Learning rate: lr=0.0001.
===> Epoch[62](90/324): Loss: 0.5112 || Learning rate: lr=0.0001.
===> Epoch[62](100/324): Loss: 1.3265 || Learning rate: lr=0.0001.
===> Epoch[62](110/324): Loss: 0.8503 || Learning rate: lr=0.0001.
===> Epoch[62](120/324): Loss: 1.3094 || Learning rate: lr=0.0001.
===> Epoch[62](130/324): Loss: 0.8708 || Learning rate: lr=0.0001.
===> Epoch[62](140/324): Loss: 0.6079 || Learning rate: lr=0.0001.
===> Epoch[62](150/324): Loss: 0.9949 || Learning rate: lr=0.0001.
===> Epoch[62](160/324): Loss: 1.0761 || Learning rate: lr=0.0001.
===> Epoch[62](170/324): Loss: 1.2463 || Learning rate: lr=0.0001.
===> Epoch[62](180/324): Loss: 0.6889 || Learning rate: lr=0.0001.
===> Epoch[62](190/324): Loss: 1.2881 || Learning rate: lr=0.0001.
===> Epoch[62](200/324): Loss: 1.0200 || Learning rate: lr=0.0001.
===> Epoch[62](210/324): Loss: 2.0129 || Learning rate: lr=0.0001.
===> Epoch[62](220/324): Loss: 1.0486 || Learning rate: lr=0.0001.
===> Epoch[62](230/324): Loss: 1.3451 || Learning rate: lr=0.0001.
===> Epoch[62](240/324): Loss: 1.1881 || Learning rate: lr=0.0001.
===> Epoch[62](250/324): Loss: 0.7394 || Learning rate: lr=0.0001.
===> Epoch[62](260/324): Loss: 0.7455 || Learning rate: lr=0.0001.
===> Epoch[62](270/324): Loss: 0.9571 || Learning rate: lr=0.0001.
===> Epoch[62](280/324): Loss: 0.8321 || Learning rate: lr=0.0001.
===> Epoch[62](290/324): Loss: 0.8437 || Learning rate: lr=0.0001.
===> Epoch[62](300/324): Loss: 1.1252 || Learning rate: lr=0.0001.
===> Epoch[62](310/324): Loss: 0.9268 || Learning rate: lr=0.0001.
===> Epoch[62](320/324): Loss: 0.8059 || Learning rate: lr=0.0001.
===> Epoch[63](10/324): Loss: 1.3357 || Learning rate: lr=0.0001.
===> Epoch[63](20/324): Loss: 1.2872 || Learning rate: lr=0.0001.
===> Epoch[63](30/324): Loss: 0.9911 || Learning rate: lr=0.0001.
===> Epoch[63](40/324): Loss: 0.8932 || Learning rate: lr=0.0001.
===> Epoch[63](50/324): Loss: 0.7704 || Learning rate: lr=0.0001.
===> Epoch[63](60/324): Loss: 1.8585 || Learning rate: lr=0.0001.
===> Epoch[63](70/324): Loss: 0.7622 || Learning rate: lr=0.0001.
===> Epoch[63](80/324): Loss: 1.0704 || Learning rate: lr=0.0001.
===> Epoch[63](90/324): Loss: 0.7874 || Learning rate: lr=0.0001.
===> Epoch[63](100/324): Loss: 1.2387 || Learning rate: lr=0.0001.
===> Epoch[63](110/324): Loss: 0.8554 || Learning rate: lr=0.0001.
===> Epoch[63](120/324): Loss: 1.2299 || Learning rate: lr=0.0001.
===> Epoch[63](130/324): Loss: 0.7811 || Learning rate: lr=0.0001.
===> Epoch[63](140/324): Loss: 0.9854 || Learning rate: lr=0.0001.
===> Epoch[63](150/324): Loss: 1.4781 || Learning rate: lr=0.0001.
===> Epoch[63](160/324): Loss: 0.9475 || Learning rate: lr=0.0001.
===> Epoch[63](170/324): Loss: 0.9056 || Learning rate: lr=0.0001.
===> Epoch[63](180/324): Loss: 0.7139 || Learning rate: lr=0.0001.
===> Epoch[63](190/324): Loss: 1.3586 || Learning rate: lr=0.0001.
===> Epoch[63](200/324): Loss: 1.3172 || Learning rate: lr=0.0001.
===> Epoch[63](210/324): Loss: 5.2314 || Learning rate: lr=0.0001.
===> Epoch[63](220/324): Loss: 3.6984 || Learning rate: lr=0.0001.
===> Epoch[63](230/324): Loss: 5.9978 || Learning rate: lr=0.0001.
===> Epoch[63](240/324): Loss: 3.8409 || Learning rate: lr=0.0001.
===> Epoch[63](250/324): Loss: 1.8644 || Learning rate: lr=0.0001.
===> Epoch[63](260/324): Loss: 1.8686 || Learning rate: lr=0.0001.
===> Epoch[63](270/324): Loss: 1.0651 || Learning rate: lr=0.0001.
===> Epoch[63](280/324): Loss: 1.1315 || Learning rate: lr=0.0001.
===> Epoch[63](290/324): Loss: 1.6227 || Learning rate: lr=0.0001.
===> Epoch[63](300/324): Loss: 1.6728 || Learning rate: lr=0.0001.
===> Epoch[63](310/324): Loss: 0.9607 || Learning rate: lr=0.0001.
===> Epoch[63](320/324): Loss: 1.0191 || Learning rate: lr=0.0001.
===> Epoch[64](10/324): Loss: 0.9330 || Learning rate: lr=0.0001.
===> Epoch[64](20/324): Loss: 1.1622 || Learning rate: lr=0.0001.
===> Epoch[64](30/324): Loss: 1.1846 || Learning rate: lr=0.0001.
===> Epoch[64](40/324): Loss: 1.5339 || Learning rate: lr=0.0001.
===> Epoch[64](50/324): Loss: 0.6877 || Learning rate: lr=0.0001.
===> Epoch[64](60/324): Loss: 1.4495 || Learning rate: lr=0.0001.
===> Epoch[64](70/324): Loss: 0.8476 || Learning rate: lr=0.0001.
===> Epoch[64](80/324): Loss: 1.0250 || Learning rate: lr=0.0001.
===> Epoch[64](90/324): Loss: 1.6292 || Learning rate: lr=0.0001.
===> Epoch[64](100/324): Loss: 0.7305 || Learning rate: lr=0.0001.
===> Epoch[64](110/324): Loss: 1.0632 || Learning rate: lr=0.0001.
===> Epoch[64](120/324): Loss: 1.1461 || Learning rate: lr=0.0001.
===> Epoch[64](130/324): Loss: 0.9697 || Learning rate: lr=0.0001.
===> Epoch[64](140/324): Loss: 0.9411 || Learning rate: lr=0.0001.
===> Epoch[64](150/324): Loss: 0.8525 || Learning rate: lr=0.0001.
===> Epoch[64](160/324): Loss: 0.8271 || Learning rate: lr=0.0001.
===> Epoch[64](170/324): Loss: 0.8463 || Learning rate: lr=0.0001.
===> Epoch[64](180/324): Loss: 1.0993 || Learning rate: lr=0.0001.
===> Epoch[64](190/324): Loss: 1.3749 || Learning rate: lr=0.0001.
===> Epoch[64](200/324): Loss: 0.6668 || Learning rate: lr=0.0001.
===> Epoch[64](210/324): Loss: 1.4046 || Learning rate: lr=0.0001.
===> Epoch[64](220/324): Loss: 1.1006 || Learning rate: lr=0.0001.
===> Epoch[64](230/324): Loss: 0.6908 || Learning rate: lr=0.0001.
===> Epoch[64](240/324): Loss: 0.8709 || Learning rate: lr=0.0001.
===> Epoch[64](250/324): Loss: 0.9208 || Learning rate: lr=0.0001.
===> Epoch[64](260/324): Loss: 1.2176 || Learning rate: lr=0.0001.
===> Epoch[64](270/324): Loss: 1.2857 || Learning rate: lr=0.0001.
===> Epoch[64](280/324): Loss: 0.8672 || Learning rate: lr=0.0001.
===> Epoch[64](290/324): Loss: 1.0696 || Learning rate: lr=0.0001.
===> Epoch[64](300/324): Loss: 1.1520 || Learning rate: lr=0.0001.
===> Epoch[64](310/324): Loss: 1.2004 || Learning rate: lr=0.0001.
===> Epoch[64](320/324): Loss: 0.6041 || Learning rate: lr=0.0001.
===> Epoch[65](10/324): Loss: 0.8086 || Learning rate: lr=0.0001.
===> Epoch[65](20/324): Loss: 0.8897 || Learning rate: lr=0.0001.
===> Epoch[65](30/324): Loss: 0.7315 || Learning rate: lr=0.0001.
===> Epoch[65](40/324): Loss: 0.9774 || Learning rate: lr=0.0001.
===> Epoch[65](50/324): Loss: 1.0978 || Learning rate: lr=0.0001.
===> Epoch[65](60/324): Loss: 0.7251 || Learning rate: lr=0.0001.
===> Epoch[65](70/324): Loss: 0.8195 || Learning rate: lr=0.0001.
===> Epoch[65](80/324): Loss: 0.7257 || Learning rate: lr=0.0001.
===> Epoch[65](90/324): Loss: 1.2355 || Learning rate: lr=0.0001.
===> Epoch[65](100/324): Loss: 0.9147 || Learning rate: lr=0.0001.
===> Epoch[65](110/324): Loss: 0.8644 || Learning rate: lr=0.0001.
===> Epoch[65](120/324): Loss: 0.9674 || Learning rate: lr=0.0001.
===> Epoch[65](130/324): Loss: 0.8900 || Learning rate: lr=0.0001.
===> Epoch[65](140/324): Loss: 1.1983 || Learning rate: lr=0.0001.
===> Epoch[65](150/324): Loss: 0.9423 || Learning rate: lr=0.0001.
===> Epoch[65](160/324): Loss: 0.9903 || Learning rate: lr=0.0001.
===> Epoch[65](170/324): Loss: 0.9032 || Learning rate: lr=0.0001.
===> Epoch[65](180/324): Loss: 1.6270 || Learning rate: lr=0.0001.
===> Epoch[65](190/324): Loss: 0.8809 || Learning rate: lr=0.0001.
===> Epoch[65](200/324): Loss: 1.2731 || Learning rate: lr=0.0001.
===> Epoch[65](210/324): Loss: 0.5812 || Learning rate: lr=0.0001.
===> Epoch[65](220/324): Loss: 1.1065 || Learning rate: lr=0.0001.
===> Epoch[65](230/324): Loss: 1.3185 || Learning rate: lr=0.0001.
===> Epoch[65](240/324): Loss: 1.3806 || Learning rate: lr=0.0001.
===> Epoch[65](250/324): Loss: 0.9324 || Learning rate: lr=0.0001.
===> Epoch[65](260/324): Loss: 1.1693 || Learning rate: lr=0.0001.
===> Epoch[65](270/324): Loss: 1.1282 || Learning rate: lr=0.0001.
===> Epoch[65](280/324): Loss: 1.1981 || Learning rate: lr=0.0001.
===> Epoch[65](290/324): Loss: 0.8371 || Learning rate: lr=0.0001.
===> Epoch[65](300/324): Loss: 1.5924 || Learning rate: lr=0.0001.
===> Epoch[65](310/324): Loss: 0.8544 || Learning rate: lr=0.0001.
===> Epoch[65](320/324): Loss: 0.8759 || Learning rate: lr=0.0001.
===> Epoch[66](10/324): Loss: 0.9466 || Learning rate: lr=0.0001.
===> Epoch[66](20/324): Loss: 0.7250 || Learning rate: lr=0.0001.
===> Epoch[66](30/324): Loss: 0.9314 || Learning rate: lr=0.0001.
===> Epoch[66](40/324): Loss: 0.8980 || Learning rate: lr=0.0001.
===> Epoch[66](50/324): Loss: 1.0010 || Learning rate: lr=0.0001.
===> Epoch[66](60/324): Loss: 0.7494 || Learning rate: lr=0.0001.
===> Epoch[66](70/324): Loss: 0.9479 || Learning rate: lr=0.0001.
===> Epoch[66](80/324): Loss: 1.0249 || Learning rate: lr=0.0001.
===> Epoch[66](90/324): Loss: 0.6572 || Learning rate: lr=0.0001.
===> Epoch[66](100/324): Loss: 0.5028 || Learning rate: lr=0.0001.
===> Epoch[66](110/324): Loss: 0.8972 || Learning rate: lr=0.0001.
===> Epoch[66](120/324): Loss: 1.2671 || Learning rate: lr=0.0001.
===> Epoch[66](130/324): Loss: 0.9796 || Learning rate: lr=0.0001.
===> Epoch[66](140/324): Loss: 1.1783 || Learning rate: lr=0.0001.
===> Epoch[66](150/324): Loss: 0.8275 || Learning rate: lr=0.0001.
===> Epoch[66](160/324): Loss: 1.2465 || Learning rate: lr=0.0001.
===> Epoch[66](170/324): Loss: 1.4330 || Learning rate: lr=0.0001.
===> Epoch[66](180/324): Loss: 1.0197 || Learning rate: lr=0.0001.
===> Epoch[66](190/324): Loss: 0.8537 || Learning rate: lr=0.0001.
===> Epoch[66](200/324): Loss: 0.8412 || Learning rate: lr=0.0001.
===> Epoch[66](210/324): Loss: 0.7903 || Learning rate: lr=0.0001.
===> Epoch[66](220/324): Loss: 1.0446 || Learning rate: lr=0.0001.
===> Epoch[66](230/324): Loss: 1.3996 || Learning rate: lr=0.0001.
===> Epoch[66](240/324): Loss: 0.9599 || Learning rate: lr=0.0001.
===> Epoch[66](250/324): Loss: 0.9811 || Learning rate: lr=0.0001.
===> Epoch[66](260/324): Loss: 0.9945 || Learning rate: lr=0.0001.
===> Epoch[66](270/324): Loss: 0.9311 || Learning rate: lr=0.0001.
===> Epoch[66](280/324): Loss: 1.2538 || Learning rate: lr=0.0001.
===> Epoch[66](290/324): Loss: 1.0402 || Learning rate: lr=0.0001.
===> Epoch[66](300/324): Loss: 1.2241 || Learning rate: lr=0.0001.
===> Epoch[66](310/324): Loss: 0.8895 || Learning rate: lr=0.0001.
===> Epoch[66](320/324): Loss: 0.9721 || Learning rate: lr=0.0001.
===> Epoch[67](10/324): Loss: 1.3467 || Learning rate: lr=0.0001.
===> Epoch[67](20/324): Loss: 0.8646 || Learning rate: lr=0.0001.
===> Epoch[67](30/324): Loss: 1.0169 || Learning rate: lr=0.0001.
===> Epoch[67](40/324): Loss: 1.5080 || Learning rate: lr=0.0001.
===> Epoch[67](50/324): Loss: 0.9250 || Learning rate: lr=0.0001.
===> Epoch[67](60/324): Loss: 0.8757 || Learning rate: lr=0.0001.
===> Epoch[67](70/324): Loss: 0.9599 || Learning rate: lr=0.0001.
===> Epoch[67](80/324): Loss: 0.7061 || Learning rate: lr=0.0001.
===> Epoch[67](90/324): Loss: 0.7466 || Learning rate: lr=0.0001.
===> Epoch[67](100/324): Loss: 1.6346 || Learning rate: lr=0.0001.
===> Epoch[67](110/324): Loss: 0.6458 || Learning rate: lr=0.0001.
===> Epoch[67](120/324): Loss: 0.8758 || Learning rate: lr=0.0001.
===> Epoch[67](130/324): Loss: 0.9283 || Learning rate: lr=0.0001.
===> Epoch[67](140/324): Loss: 0.9842 || Learning rate: lr=0.0001.
===> Epoch[67](150/324): Loss: 0.8798 || Learning rate: lr=0.0001.
===> Epoch[67](160/324): Loss: 0.5946 || Learning rate: lr=0.0001.
===> Epoch[67](170/324): Loss: 0.8841 || Learning rate: lr=0.0001.
===> Epoch[67](180/324): Loss: 0.9464 || Learning rate: lr=0.0001.
===> Epoch[67](190/324): Loss: 0.7154 || Learning rate: lr=0.0001.
===> Epoch[67](200/324): Loss: 0.9966 || Learning rate: lr=0.0001.
===> Epoch[67](210/324): Loss: 1.0897 || Learning rate: lr=0.0001.
===> Epoch[67](220/324): Loss: 0.9819 || Learning rate: lr=0.0001.
===> Epoch[67](230/324): Loss: 0.8169 || Learning rate: lr=0.0001.
===> Epoch[67](240/324): Loss: 0.9644 || Learning rate: lr=0.0001.
===> Epoch[67](250/324): Loss: 1.1367 || Learning rate: lr=0.0001.
===> Epoch[67](260/324): Loss: 0.9597 || Learning rate: lr=0.0001.
===> Epoch[67](270/324): Loss: 0.5299 || Learning rate: lr=0.0001.
===> Epoch[67](280/324): Loss: 1.2408 || Learning rate: lr=0.0001.
===> Epoch[67](290/324): Loss: 1.1800 || Learning rate: lr=0.0001.
===> Epoch[67](300/324): Loss: 0.7630 || Learning rate: lr=0.0001.
===> Epoch[67](310/324): Loss: 1.6379 || Learning rate: lr=0.0001.
===> Epoch[67](320/324): Loss: 0.6662 || Learning rate: lr=0.0001.
===> Epoch[68](10/324): Loss: 1.3229 || Learning rate: lr=0.0001.
===> Epoch[68](20/324): Loss: 0.7931 || Learning rate: lr=0.0001.
===> Epoch[68](30/324): Loss: 0.7647 || Learning rate: lr=0.0001.
===> Epoch[68](40/324): Loss: 0.5857 || Learning rate: lr=0.0001.
===> Epoch[68](50/324): Loss: 0.9567 || Learning rate: lr=0.0001.
===> Epoch[68](60/324): Loss: 1.2536 || Learning rate: lr=0.0001.
===> Epoch[68](70/324): Loss: 0.9389 || Learning rate: lr=0.0001.
===> Epoch[68](80/324): Loss: 0.7197 || Learning rate: lr=0.0001.
===> Epoch[68](90/324): Loss: 1.3537 || Learning rate: lr=0.0001.
===> Epoch[68](100/324): Loss: 0.7032 || Learning rate: lr=0.0001.
===> Epoch[68](110/324): Loss: 0.7450 || Learning rate: lr=0.0001.
===> Epoch[68](120/324): Loss: 0.8916 || Learning rate: lr=0.0001.
===> Epoch[68](130/324): Loss: 1.6463 || Learning rate: lr=0.0001.
===> Epoch[68](140/324): Loss: 0.8367 || Learning rate: lr=0.0001.
===> Epoch[68](150/324): Loss: 1.2223 || Learning rate: lr=0.0001.
===> Epoch[68](160/324): Loss: 0.7189 || Learning rate: lr=0.0001.
===> Epoch[68](170/324): Loss: 1.6428 || Learning rate: lr=0.0001.
===> Epoch[68](180/324): Loss: 1.2514 || Learning rate: lr=0.0001.
===> Epoch[68](190/324): Loss: 1.1793 || Learning rate: lr=0.0001.
===> Epoch[68](200/324): Loss: 0.8868 || Learning rate: lr=0.0001.
===> Epoch[68](210/324): Loss: 0.8444 || Learning rate: lr=0.0001.
===> Epoch[68](220/324): Loss: 0.7190 || Learning rate: lr=0.0001.
===> Epoch[68](230/324): Loss: 1.3429 || Learning rate: lr=0.0001.
===> Epoch[68](240/324): Loss: 1.0513 || Learning rate: lr=0.0001.
===> Epoch[68](250/324): Loss: 0.7580 || Learning rate: lr=0.0001.
===> Epoch[68](260/324): Loss: 1.0250 || Learning rate: lr=0.0001.
===> Epoch[68](270/324): Loss: 1.0407 || Learning rate: lr=0.0001.
===> Epoch[68](280/324): Loss: 0.6536 || Learning rate: lr=0.0001.
===> Epoch[68](290/324): Loss: 0.9766 || Learning rate: lr=0.0001.
===> Epoch[68](300/324): Loss: 1.0175 || Learning rate: lr=0.0001.
===> Epoch[68](310/324): Loss: 0.7301 || Learning rate: lr=0.0001.
===> Epoch[68](320/324): Loss: 1.0651 || Learning rate: lr=0.0001.
===> Epoch[69](10/324): Loss: 0.9116 || Learning rate: lr=0.0001.
===> Epoch[69](20/324): Loss: 1.1865 || Learning rate: lr=0.0001.
===> Epoch[69](30/324): Loss: 0.7486 || Learning rate: lr=0.0001.
===> Epoch[69](40/324): Loss: 1.0735 || Learning rate: lr=0.0001.
===> Epoch[69](50/324): Loss: 1.0804 || Learning rate: lr=0.0001.
===> Epoch[69](60/324): Loss: 0.9233 || Learning rate: lr=0.0001.
===> Epoch[69](70/324): Loss: 1.3925 || Learning rate: lr=0.0001.
===> Epoch[69](80/324): Loss: 0.9239 || Learning rate: lr=0.0001.
===> Epoch[69](90/324): Loss: 1.4019 || Learning rate: lr=0.0001.
===> Epoch[69](100/324): Loss: 0.8562 || Learning rate: lr=0.0001.
===> Epoch[69](110/324): Loss: 1.4339 || Learning rate: lr=0.0001.
===> Epoch[69](120/324): Loss: 1.8175 || Learning rate: lr=0.0001.
===> Epoch[69](130/324): Loss: 1.6952 || Learning rate: lr=0.0001.
===> Epoch[69](140/324): Loss: 1.2298 || Learning rate: lr=0.0001.
===> Epoch[69](150/324): Loss: 1.8326 || Learning rate: lr=0.0001.
===> Epoch[69](160/324): Loss: 1.8350 || Learning rate: lr=0.0001.
===> Epoch[69](170/324): Loss: 1.3817 || Learning rate: lr=0.0001.
===> Epoch[69](180/324): Loss: 3.0576 || Learning rate: lr=0.0001.
===> Epoch[69](190/324): Loss: 2.0149 || Learning rate: lr=0.0001.
===> Epoch[69](200/324): Loss: 1.5758 || Learning rate: lr=0.0001.
===> Epoch[69](210/324): Loss: 1.8890 || Learning rate: lr=0.0001.
===> Epoch[69](220/324): Loss: 1.9801 || Learning rate: lr=0.0001.
===> Epoch[69](230/324): Loss: 1.6092 || Learning rate: lr=0.0001.
===> Epoch[69](240/324): Loss: 1.6123 || Learning rate: lr=0.0001.
===> Epoch[69](250/324): Loss: 1.3072 || Learning rate: lr=0.0001.
===> Epoch[69](260/324): Loss: 1.8491 || Learning rate: lr=0.0001.
===> Epoch[69](270/324): Loss: 1.2526 || Learning rate: lr=0.0001.
===> Epoch[69](280/324): Loss: 0.8540 || Learning rate: lr=0.0001.
===> Epoch[69](290/324): Loss: 4.8530 || Learning rate: lr=0.0001.
===> Epoch[69](300/324): Loss: 5.3401 || Learning rate: lr=0.0001.
===> Epoch[69](310/324): Loss: 6.2071 || Learning rate: lr=0.0001.
===> Epoch[69](320/324): Loss: 2.2052 || Learning rate: lr=0.0001.
===> Epoch[70](10/324): Loss: 3.6665 || Learning rate: lr=0.0001.
===> Epoch[70](20/324): Loss: 2.2071 || Learning rate: lr=0.0001.
===> Epoch[70](30/324): Loss: 1.4384 || Learning rate: lr=0.0001.
===> Epoch[70](40/324): Loss: 1.3947 || Learning rate: lr=0.0001.
===> Epoch[70](50/324): Loss: 2.7236 || Learning rate: lr=0.0001.
===> Epoch[70](60/324): Loss: 0.9693 || Learning rate: lr=0.0001.
===> Epoch[70](70/324): Loss: 2.2739 || Learning rate: lr=0.0001.
===> Epoch[70](80/324): Loss: 1.4311 || Learning rate: lr=0.0001.
===> Epoch[70](90/324): Loss: 1.2312 || Learning rate: lr=0.0001.
===> Epoch[70](100/324): Loss: 1.4288 || Learning rate: lr=0.0001.
===> Epoch[70](110/324): Loss: 1.0168 || Learning rate: lr=0.0001.
===> Epoch[70](120/324): Loss: 1.1977 || Learning rate: lr=0.0001.
===> Epoch[70](130/324): Loss: 1.0344 || Learning rate: lr=0.0001.
===> Epoch[70](140/324): Loss: 0.9281 || Learning rate: lr=0.0001.
===> Epoch[70](150/324): Loss: 1.0502 || Learning rate: lr=0.0001.
===> Epoch[70](160/324): Loss: 1.3589 || Learning rate: lr=0.0001.
===> Epoch[70](170/324): Loss: 1.0097 || Learning rate: lr=0.0001.
===> Epoch[70](180/324): Loss: 1.3298 || Learning rate: lr=0.0001.
===> Epoch[70](190/324): Loss: 0.7954 || Learning rate: lr=0.0001.
===> Epoch[70](200/324): Loss: 1.5408 || Learning rate: lr=0.0001.
===> Epoch[70](210/324): Loss: 0.6048 || Learning rate: lr=0.0001.
===> Epoch[70](220/324): Loss: 1.0004 || Learning rate: lr=0.0001.
===> Epoch[70](230/324): Loss: 1.2572 || Learning rate: lr=0.0001.
===> Epoch[70](240/324): Loss: 0.7445 || Learning rate: lr=0.0001.
===> Epoch[70](250/324): Loss: 1.1080 || Learning rate: lr=0.0001.
===> Epoch[70](260/324): Loss: 1.0351 || Learning rate: lr=0.0001.
===> Epoch[70](270/324): Loss: 0.6584 || Learning rate: lr=0.0001.
===> Epoch[70](280/324): Loss: 1.8341 || Learning rate: lr=0.0001.
===> Epoch[70](290/324): Loss: 0.8480 || Learning rate: lr=0.0001.
===> Epoch[70](300/324): Loss: 0.7796 || Learning rate: lr=0.0001.
===> Epoch[70](310/324): Loss: 1.1929 || Learning rate: lr=0.0001.
===> Epoch[70](320/324): Loss: 0.7044 || Learning rate: lr=0.0001.
===> Epoch[71](10/324): Loss: 1.2818 || Learning rate: lr=0.0001.
===> Epoch[71](20/324): Loss: 0.9262 || Learning rate: lr=0.0001.
===> Epoch[71](30/324): Loss: 0.6478 || Learning rate: lr=0.0001.
===> Epoch[71](40/324): Loss: 0.9770 || Learning rate: lr=0.0001.
===> Epoch[71](50/324): Loss: 0.9505 || Learning rate: lr=0.0001.
===> Epoch[71](60/324): Loss: 1.2002 || Learning rate: lr=0.0001.
===> Epoch[71](70/324): Loss: 1.0492 || Learning rate: lr=0.0001.
===> Epoch[71](80/324): Loss: 1.0755 || Learning rate: lr=0.0001.
===> Epoch[71](90/324): Loss: 1.0524 || Learning rate: lr=0.0001.
===> Epoch[71](100/324): Loss: 0.5966 || Learning rate: lr=0.0001.
===> Epoch[71](110/324): Loss: 0.7224 || Learning rate: lr=0.0001.
===> Epoch[71](120/324): Loss: 1.0067 || Learning rate: lr=0.0001.
===> Epoch[71](130/324): Loss: 1.3713 || Learning rate: lr=0.0001.
===> Epoch[71](140/324): Loss: 1.0004 || Learning rate: lr=0.0001.
===> Epoch[71](150/324): Loss: 1.1477 || Learning rate: lr=0.0001.
===> Epoch[71](160/324): Loss: 0.5390 || Learning rate: lr=0.0001.
===> Epoch[71](170/324): Loss: 0.8314 || Learning rate: lr=0.0001.
===> Epoch[71](180/324): Loss: 0.9852 || Learning rate: lr=0.0001.
===> Epoch[71](190/324): Loss: 0.7347 || Learning rate: lr=0.0001.
===> Epoch[71](200/324): Loss: 1.2506 || Learning rate: lr=0.0001.
===> Epoch[71](210/324): Loss: 0.7631 || Learning rate: lr=0.0001.
===> Epoch[71](220/324): Loss: 0.9216 || Learning rate: lr=0.0001.
===> Epoch[71](230/324): Loss: 0.7337 || Learning rate: lr=0.0001.
===> Epoch[71](240/324): Loss: 0.6166 || Learning rate: lr=0.0001.
===> Epoch[71](250/324): Loss: 1.0788 || Learning rate: lr=0.0001.
===> Epoch[71](260/324): Loss: 1.3137 || Learning rate: lr=0.0001.
===> Epoch[71](270/324): Loss: 0.8119 || Learning rate: lr=0.0001.
===> Epoch[71](280/324): Loss: 1.3784 || Learning rate: lr=0.0001.
===> Epoch[71](290/324): Loss: 0.9535 || Learning rate: lr=0.0001.
===> Epoch[71](300/324): Loss: 1.2283 || Learning rate: lr=0.0001.
===> Epoch[71](310/324): Loss: 1.3483 || Learning rate: lr=0.0001.
===> Epoch[71](320/324): Loss: 1.0445 || Learning rate: lr=0.0001.
===> Epoch[72](10/324): Loss: 1.0172 || Learning rate: lr=0.0001.
===> Epoch[72](20/324): Loss: 0.5243 || Learning rate: lr=0.0001.
===> Epoch[72](30/324): Loss: 0.7090 || Learning rate: lr=0.0001.
===> Epoch[72](40/324): Loss: 1.0999 || Learning rate: lr=0.0001.
===> Epoch[72](50/324): Loss: 0.9441 || Learning rate: lr=0.0001.
===> Epoch[72](60/324): Loss: 1.0287 || Learning rate: lr=0.0001.
===> Epoch[72](70/324): Loss: 1.0495 || Learning rate: lr=0.0001.
===> Epoch[72](80/324): Loss: 0.6345 || Learning rate: lr=0.0001.
===> Epoch[72](90/324): Loss: 1.0230 || Learning rate: lr=0.0001.
===> Epoch[72](100/324): Loss: 1.2540 || Learning rate: lr=0.0001.
===> Epoch[72](110/324): Loss: 1.2944 || Learning rate: lr=0.0001.
===> Epoch[72](120/324): Loss: 0.7512 || Learning rate: lr=0.0001.
===> Epoch[72](130/324): Loss: 0.8542 || Learning rate: lr=0.0001.
===> Epoch[72](140/324): Loss: 1.1172 || Learning rate: lr=0.0001.
===> Epoch[72](150/324): Loss: 1.1275 || Learning rate: lr=0.0001.
===> Epoch[72](160/324): Loss: 0.6967 || Learning rate: lr=0.0001.
===> Epoch[72](170/324): Loss: 1.2289 || Learning rate: lr=0.0001.
===> Epoch[72](180/324): Loss: 0.8409 || Learning rate: lr=0.0001.
===> Epoch[72](190/324): Loss: 0.8490 || Learning rate: lr=0.0001.
===> Epoch[72](200/324): Loss: 1.1108 || Learning rate: lr=0.0001.
===> Epoch[72](210/324): Loss: 1.2452 || Learning rate: lr=0.0001.
===> Epoch[72](220/324): Loss: 0.9848 || Learning rate: lr=0.0001.
===> Epoch[72](230/324): Loss: 0.8755 || Learning rate: lr=0.0001.
===> Epoch[72](240/324): Loss: 0.8958 || Learning rate: lr=0.0001.
===> Epoch[72](250/324): Loss: 0.6662 || Learning rate: lr=0.0001.
===> Epoch[72](260/324): Loss: 0.7487 || Learning rate: lr=0.0001.
===> Epoch[72](270/324): Loss: 0.7799 || Learning rate: lr=0.0001.
===> Epoch[72](280/324): Loss: 1.1620 || Learning rate: lr=0.0001.
===> Epoch[72](290/324): Loss: 0.8179 || Learning rate: lr=0.0001.
===> Epoch[72](300/324): Loss: 1.1535 || Learning rate: lr=0.0001.
===> Epoch[72](310/324): Loss: 0.7858 || Learning rate: lr=0.0001.
===> Epoch[72](320/324): Loss: 0.7481 || Learning rate: lr=0.0001.
===> Epoch[73](10/324): Loss: 1.7926 || Learning rate: lr=0.0001.
===> Epoch[73](20/324): Loss: 0.8244 || Learning rate: lr=0.0001.
===> Epoch[73](30/324): Loss: 1.0536 || Learning rate: lr=0.0001.
===> Epoch[73](40/324): Loss: 0.8740 || Learning rate: lr=0.0001.
===> Epoch[73](50/324): Loss: 1.0387 || Learning rate: lr=0.0001.
===> Epoch[73](60/324): Loss: 0.6720 || Learning rate: lr=0.0001.
===> Epoch[73](70/324): Loss: 0.8453 || Learning rate: lr=0.0001.
===> Epoch[73](80/324): Loss: 0.7227 || Learning rate: lr=0.0001.
===> Epoch[73](90/324): Loss: 0.5405 || Learning rate: lr=0.0001.
===> Epoch[73](100/324): Loss: 0.6949 || Learning rate: lr=0.0001.
===> Epoch[73](110/324): Loss: 0.7536 || Learning rate: lr=0.0001.
===> Epoch[73](120/324): Loss: 0.8543 || Learning rate: lr=0.0001.
===> Epoch[73](130/324): Loss: 0.6185 || Learning rate: lr=0.0001.
===> Epoch[73](140/324): Loss: 1.0135 || Learning rate: lr=0.0001.
===> Epoch[73](150/324): Loss: 1.3182 || Learning rate: lr=0.0001.
===> Epoch[73](160/324): Loss: 0.9998 || Learning rate: lr=0.0001.
===> Epoch[73](170/324): Loss: 1.3060 || Learning rate: lr=0.0001.
===> Epoch[73](180/324): Loss: 0.8046 || Learning rate: lr=0.0001.
===> Epoch[73](190/324): Loss: 1.3912 || Learning rate: lr=0.0001.
===> Epoch[73](200/324): Loss: 1.0990 || Learning rate: lr=0.0001.
===> Epoch[73](210/324): Loss: 1.5603 || Learning rate: lr=0.0001.
===> Epoch[73](220/324): Loss: 1.4295 || Learning rate: lr=0.0001.
===> Epoch[73](230/324): Loss: 0.6641 || Learning rate: lr=0.0001.
===> Epoch[73](240/324): Loss: 0.8350 || Learning rate: lr=0.0001.
===> Epoch[73](250/324): Loss: 0.8180 || Learning rate: lr=0.0001.
===> Epoch[73](260/324): Loss: 0.5882 || Learning rate: lr=0.0001.
===> Epoch[73](270/324): Loss: 0.6730 || Learning rate: lr=0.0001.
===> Epoch[73](280/324): Loss: 0.5962 || Learning rate: lr=0.0001.
===> Epoch[73](290/324): Loss: 1.5037 || Learning rate: lr=0.0001.
===> Epoch[73](300/324): Loss: 0.8377 || Learning rate: lr=0.0001.
===> Epoch[73](310/324): Loss: 1.0392 || Learning rate: lr=0.0001.
===> Epoch[73](320/324): Loss: 0.6099 || Learning rate: lr=0.0001.
===> Epoch[74](10/324): Loss: 1.1767 || Learning rate: lr=0.0001.
===> Epoch[74](20/324): Loss: 1.1717 || Learning rate: lr=0.0001.
===> Epoch[74](30/324): Loss: 0.6302 || Learning rate: lr=0.0001.
===> Epoch[74](40/324): Loss: 1.0154 || Learning rate: lr=0.0001.
===> Epoch[74](50/324): Loss: 2.4633 || Learning rate: lr=0.0001.
===> Epoch[74](60/324): Loss: 1.1547 || Learning rate: lr=0.0001.
===> Epoch[74](70/324): Loss: 0.9104 || Learning rate: lr=0.0001.
===> Epoch[74](80/324): Loss: 0.7123 || Learning rate: lr=0.0001.
===> Epoch[74](90/324): Loss: 0.6538 || Learning rate: lr=0.0001.
===> Epoch[74](100/324): Loss: 1.1987 || Learning rate: lr=0.0001.
===> Epoch[74](110/324): Loss: 0.7687 || Learning rate: lr=0.0001.
===> Epoch[74](120/324): Loss: 1.0237 || Learning rate: lr=0.0001.
===> Epoch[74](130/324): Loss: 1.1558 || Learning rate: lr=0.0001.
===> Epoch[74](140/324): Loss: 1.0472 || Learning rate: lr=0.0001.
===> Epoch[74](150/324): Loss: 0.9888 || Learning rate: lr=0.0001.
===> Epoch[74](160/324): Loss: 0.6890 || Learning rate: lr=0.0001.
===> Epoch[74](170/324): Loss: 1.2940 || Learning rate: lr=0.0001.
===> Epoch[74](180/324): Loss: 1.1672 || Learning rate: lr=0.0001.
===> Epoch[74](190/324): Loss: 0.6407 || Learning rate: lr=0.0001.
===> Epoch[74](200/324): Loss: 1.2618 || Learning rate: lr=0.0001.
===> Epoch[74](210/324): Loss: 1.0070 || Learning rate: lr=0.0001.
===> Epoch[74](220/324): Loss: 0.8348 || Learning rate: lr=0.0001.
===> Epoch[74](230/324): Loss: 1.1663 || Learning rate: lr=0.0001.
===> Epoch[74](240/324): Loss: 0.6073 || Learning rate: lr=0.0001.
===> Epoch[74](250/324): Loss: 1.4833 || Learning rate: lr=0.0001.
===> Epoch[74](260/324): Loss: 0.9985 || Learning rate: lr=0.0001.
===> Epoch[74](270/324): Loss: 1.3376 || Learning rate: lr=0.0001.
===> Epoch[74](280/324): Loss: 1.1003 || Learning rate: lr=0.0001.
===> Epoch[74](290/324): Loss: 0.8465 || Learning rate: lr=0.0001.
===> Epoch[74](300/324): Loss: 0.8471 || Learning rate: lr=0.0001.
===> Epoch[74](310/324): Loss: 1.3511 || Learning rate: lr=0.0001.
===> Epoch[74](320/324): Loss: 0.8721 || Learning rate: lr=0.0001.
===> Epoch[75](10/324): Loss: 1.3659 || Learning rate: lr=0.0001.
===> Epoch[75](20/324): Loss: 1.2211 || Learning rate: lr=0.0001.
===> Epoch[75](30/324): Loss: 0.9124 || Learning rate: lr=0.0001.
===> Epoch[75](40/324): Loss: 0.7933 || Learning rate: lr=0.0001.
===> Epoch[75](50/324): Loss: 1.0094 || Learning rate: lr=0.0001.
===> Epoch[75](60/324): Loss: 1.4356 || Learning rate: lr=0.0001.
===> Epoch[75](70/324): Loss: 0.7216 || Learning rate: lr=0.0001.
===> Epoch[75](80/324): Loss: 0.7241 || Learning rate: lr=0.0001.
===> Epoch[75](90/324): Loss: 1.0243 || Learning rate: lr=0.0001.
===> Epoch[75](100/324): Loss: 1.0945 || Learning rate: lr=0.0001.
===> Epoch[75](110/324): Loss: 0.9914 || Learning rate: lr=0.0001.
===> Epoch[75](120/324): Loss: 1.4151 || Learning rate: lr=0.0001.
===> Epoch[75](130/324): Loss: 1.5761 || Learning rate: lr=0.0001.
===> Epoch[75](140/324): Loss: 0.7065 || Learning rate: lr=0.0001.
===> Epoch[75](150/324): Loss: 1.2718 || Learning rate: lr=0.0001.
===> Epoch[75](160/324): Loss: 1.1715 || Learning rate: lr=0.0001.
===> Epoch[75](170/324): Loss: 1.3129 || Learning rate: lr=0.0001.
===> Epoch[75](180/324): Loss: 0.7085 || Learning rate: lr=0.0001.
===> Epoch[75](190/324): Loss: 1.1406 || Learning rate: lr=0.0001.
===> Epoch[75](200/324): Loss: 1.5749 || Learning rate: lr=0.0001.
===> Epoch[75](210/324): Loss: 0.9483 || Learning rate: lr=0.0001.
===> Epoch[75](220/324): Loss: 0.9309 || Learning rate: lr=0.0001.
===> Epoch[75](230/324): Loss: 1.1764 || Learning rate: lr=0.0001.
===> Epoch[75](240/324): Loss: 0.7326 || Learning rate: lr=0.0001.
===> Epoch[75](250/324): Loss: 1.3023 || Learning rate: lr=0.0001.
===> Epoch[75](260/324): Loss: 1.5156 || Learning rate: lr=0.0001.
===> Epoch[75](270/324): Loss: 0.6774 || Learning rate: lr=0.0001.
===> Epoch[75](280/324): Loss: 1.1012 || Learning rate: lr=0.0001.
===> Epoch[75](290/324): Loss: 0.7440 || Learning rate: lr=0.0001.
===> Epoch[75](300/324): Loss: 0.9861 || Learning rate: lr=0.0001.
===> Epoch[75](310/324): Loss: 1.0512 || Learning rate: lr=0.0001.
===> Epoch[75](320/324): Loss: 0.7950 || Learning rate: lr=0.0001.
===> Epoch[76](10/324): Loss: 0.7139 || Learning rate: lr=0.0001.
===> Epoch[76](20/324): Loss: 0.9021 || Learning rate: lr=0.0001.
===> Epoch[76](30/324): Loss: 0.7747 || Learning rate: lr=0.0001.
===> Epoch[76](40/324): Loss: 0.8013 || Learning rate: lr=0.0001.
===> Epoch[76](50/324): Loss: 0.9578 || Learning rate: lr=0.0001.
===> Epoch[76](60/324): Loss: 1.3008 || Learning rate: lr=0.0001.
===> Epoch[76](70/324): Loss: 1.1325 || Learning rate: lr=0.0001.
===> Epoch[76](80/324): Loss: 0.9791 || Learning rate: lr=0.0001.
===> Epoch[76](90/324): Loss: 0.8693 || Learning rate: lr=0.0001.
===> Epoch[76](100/324): Loss: 0.7887 || Learning rate: lr=0.0001.
===> Epoch[76](110/324): Loss: 1.1067 || Learning rate: lr=0.0001.
===> Epoch[76](120/324): Loss: 0.9374 || Learning rate: lr=0.0001.
===> Epoch[76](130/324): Loss: 1.2061 || Learning rate: lr=0.0001.
===> Epoch[76](140/324): Loss: 1.2992 || Learning rate: lr=0.0001.
===> Epoch[76](150/324): Loss: 1.1534 || Learning rate: lr=0.0001.
===> Epoch[76](160/324): Loss: 1.0595 || Learning rate: lr=0.0001.
===> Epoch[76](170/324): Loss: 1.2427 || Learning rate: lr=0.0001.
===> Epoch[76](180/324): Loss: 1.6869 || Learning rate: lr=0.0001.
===> Epoch[76](190/324): Loss: 0.9046 || Learning rate: lr=0.0001.
===> Epoch[76](200/324): Loss: 0.9777 || Learning rate: lr=0.0001.
===> Epoch[76](210/324): Loss: 0.9175 || Learning rate: lr=0.0001.
===> Epoch[76](220/324): Loss: 0.9499 || Learning rate: lr=0.0001.
===> Epoch[76](230/324): Loss: 0.7482 || Learning rate: lr=0.0001.
===> Epoch[76](240/324): Loss: 1.1033 || Learning rate: lr=0.0001.
===> Epoch[76](250/324): Loss: 0.9439 || Learning rate: lr=0.0001.
===> Epoch[76](260/324): Loss: 0.9836 || Learning rate: lr=0.0001.
===> Epoch[76](270/324): Loss: 1.4233 || Learning rate: lr=0.0001.
===> Epoch[76](280/324): Loss: 1.2606 || Learning rate: lr=0.0001.
===> Epoch[76](290/324): Loss: 1.0991 || Learning rate: lr=0.0001.
===> Epoch[76](300/324): Loss: 1.0376 || Learning rate: lr=0.0001.
===> Epoch[76](310/324): Loss: 0.6880 || Learning rate: lr=0.0001.
===> Epoch[76](320/324): Loss: 0.7814 || Learning rate: lr=0.0001.
===> Epoch[77](10/324): Loss: 1.2125 || Learning rate: lr=0.0001.
===> Epoch[77](20/324): Loss: 0.9725 || Learning rate: lr=0.0001.
===> Epoch[77](30/324): Loss: 0.9658 || Learning rate: lr=0.0001.
===> Epoch[77](40/324): Loss: 0.8374 || Learning rate: lr=0.0001.
===> Epoch[77](50/324): Loss: 0.9336 || Learning rate: lr=0.0001.
===> Epoch[77](60/324): Loss: 1.4211 || Learning rate: lr=0.0001.
===> Epoch[77](70/324): Loss: 1.3659 || Learning rate: lr=0.0001.
===> Epoch[77](80/324): Loss: 1.4162 || Learning rate: lr=0.0001.
===> Epoch[77](90/324): Loss: 1.0910 || Learning rate: lr=0.0001.
===> Epoch[77](100/324): Loss: 1.3291 || Learning rate: lr=0.0001.
===> Epoch[77](110/324): Loss: 1.3372 || Learning rate: lr=0.0001.
===> Epoch[77](120/324): Loss: 1.3005 || Learning rate: lr=0.0001.
===> Epoch[77](130/324): Loss: 0.7152 || Learning rate: lr=0.0001.
===> Epoch[77](140/324): Loss: 1.1333 || Learning rate: lr=0.0001.
===> Epoch[77](150/324): Loss: 0.6449 || Learning rate: lr=0.0001.
===> Epoch[77](160/324): Loss: 0.8128 || Learning rate: lr=0.0001.
===> Epoch[77](170/324): Loss: 0.7083 || Learning rate: lr=0.0001.
===> Epoch[77](180/324): Loss: 1.4577 || Learning rate: lr=0.0001.
===> Epoch[77](190/324): Loss: 0.8146 || Learning rate: lr=0.0001.
===> Epoch[77](200/324): Loss: 1.2676 || Learning rate: lr=0.0001.
===> Epoch[77](210/324): Loss: 1.0454 || Learning rate: lr=0.0001.
===> Epoch[77](220/324): Loss: 0.9323 || Learning rate: lr=0.0001.
===> Epoch[77](230/324): Loss: 1.9325 || Learning rate: lr=0.0001.
===> Epoch[77](240/324): Loss: 1.4781 || Learning rate: lr=0.0001.
===> Epoch[77](250/324): Loss: 0.9694 || Learning rate: lr=0.0001.
===> Epoch[77](260/324): Loss: 1.3768 || Learning rate: lr=0.0001.
===> Epoch[77](270/324): Loss: 1.0809 || Learning rate: lr=0.0001.
===> Epoch[77](280/324): Loss: 0.9057 || Learning rate: lr=0.0001.
===> Epoch[77](290/324): Loss: 0.9110 || Learning rate: lr=0.0001.
===> Epoch[77](300/324): Loss: 1.4307 || Learning rate: lr=0.0001.
===> Epoch[77](310/324): Loss: 1.0075 || Learning rate: lr=0.0001.
===> Epoch[77](320/324): Loss: 0.6533 || Learning rate: lr=0.0001.
===> Epoch[78](10/324): Loss: 1.2733 || Learning rate: lr=0.0001.
===> Epoch[78](20/324): Loss: 1.2191 || Learning rate: lr=0.0001.
===> Epoch[78](30/324): Loss: 0.7493 || Learning rate: lr=0.0001.
===> Epoch[78](40/324): Loss: 1.5951 || Learning rate: lr=0.0001.
===> Epoch[78](50/324): Loss: 1.0083 || Learning rate: lr=0.0001.
===> Epoch[78](60/324): Loss: 0.9708 || Learning rate: lr=0.0001.
===> Epoch[78](70/324): Loss: 1.2134 || Learning rate: lr=0.0001.
===> Epoch[78](80/324): Loss: 1.0759 || Learning rate: lr=0.0001.
===> Epoch[78](90/324): Loss: 1.3159 || Learning rate: lr=0.0001.
===> Epoch[78](100/324): Loss: 0.7960 || Learning rate: lr=0.0001.
===> Epoch[78](110/324): Loss: 1.5063 || Learning rate: lr=0.0001.
===> Epoch[78](120/324): Loss: 0.5431 || Learning rate: lr=0.0001.
===> Epoch[78](130/324): Loss: 0.7978 || Learning rate: lr=0.0001.
===> Epoch[78](140/324): Loss: 1.4100 || Learning rate: lr=0.0001.
===> Epoch[78](150/324): Loss: 0.9228 || Learning rate: lr=0.0001.
===> Epoch[78](160/324): Loss: 0.7287 || Learning rate: lr=0.0001.
===> Epoch[78](170/324): Loss: 0.5037 || Learning rate: lr=0.0001.
===> Epoch[78](180/324): Loss: 1.3869 || Learning rate: lr=0.0001.
===> Epoch[78](190/324): Loss: 0.9561 || Learning rate: lr=0.0001.
===> Epoch[78](200/324): Loss: 0.7998 || Learning rate: lr=0.0001.
===> Epoch[78](210/324): Loss: 2.0632 || Learning rate: lr=0.0001.
===> Epoch[78](220/324): Loss: 2.3164 || Learning rate: lr=0.0001.
===> Epoch[78](230/324): Loss: 0.9097 || Learning rate: lr=0.0001.
===> Epoch[78](240/324): Loss: 3.0278 || Learning rate: lr=0.0001.
===> Epoch[78](250/324): Loss: 1.7628 || Learning rate: lr=0.0001.
===> Epoch[78](260/324): Loss: 1.9762 || Learning rate: lr=0.0001.
===> Epoch[78](270/324): Loss: 1.9920 || Learning rate: lr=0.0001.
===> Epoch[78](280/324): Loss: 2.2030 || Learning rate: lr=0.0001.
===> Epoch[78](290/324): Loss: 2.9871 || Learning rate: lr=0.0001.
===> Epoch[78](300/324): Loss: 1.5483 || Learning rate: lr=0.0001.
===> Epoch[78](310/324): Loss: 1.4643 || Learning rate: lr=0.0001.
===> Epoch[78](320/324): Loss: 1.1155 || Learning rate: lr=0.0001.
===> Epoch[79](10/324): Loss: 1.0990 || Learning rate: lr=0.0001.
===> Epoch[79](20/324): Loss: 1.1239 || Learning rate: lr=0.0001.
===> Epoch[79](30/324): Loss: 1.1160 || Learning rate: lr=0.0001.
===> Epoch[79](40/324): Loss: 1.0312 || Learning rate: lr=0.0001.
===> Epoch[79](50/324): Loss: 1.1064 || Learning rate: lr=0.0001.
===> Epoch[79](60/324): Loss: 0.8659 || Learning rate: lr=0.0001.
===> Epoch[79](70/324): Loss: 2.1238 || Learning rate: lr=0.0001.
===> Epoch[79](80/324): Loss: 2.4016 || Learning rate: lr=0.0001.
===> Epoch[79](90/324): Loss: 1.9670 || Learning rate: lr=0.0001.
===> Epoch[79](100/324): Loss: 1.2051 || Learning rate: lr=0.0001.
===> Epoch[79](110/324): Loss: 1.6901 || Learning rate: lr=0.0001.
===> Epoch[79](120/324): Loss: 1.4255 || Learning rate: lr=0.0001.
===> Epoch[79](130/324): Loss: 0.7507 || Learning rate: lr=0.0001.
===> Epoch[79](140/324): Loss: 1.5988 || Learning rate: lr=0.0001.
===> Epoch[79](150/324): Loss: 0.5830 || Learning rate: lr=0.0001.
===> Epoch[79](160/324): Loss: 1.0709 || Learning rate: lr=0.0001.
===> Epoch[79](170/324): Loss: 0.9751 || Learning rate: lr=0.0001.
===> Epoch[79](180/324): Loss: 0.9786 || Learning rate: lr=0.0001.
===> Epoch[79](190/324): Loss: 1.5208 || Learning rate: lr=0.0001.
===> Epoch[79](200/324): Loss: 1.1406 || Learning rate: lr=0.0001.
===> Epoch[79](210/324): Loss: 0.8411 || Learning rate: lr=0.0001.
===> Epoch[79](220/324): Loss: 0.7939 || Learning rate: lr=0.0001.
===> Epoch[79](230/324): Loss: 0.6653 || Learning rate: lr=0.0001.
===> Epoch[79](240/324): Loss: 0.8939 || Learning rate: lr=0.0001.
===> Epoch[79](250/324): Loss: 0.9767 || Learning rate: lr=0.0001.
===> Epoch[79](260/324): Loss: 1.6075 || Learning rate: lr=0.0001.
===> Epoch[79](270/324): Loss: 0.6717 || Learning rate: lr=0.0001.
===> Epoch[79](280/324): Loss: 0.9287 || Learning rate: lr=0.0001.
===> Epoch[79](290/324): Loss: 0.9515 || Learning rate: lr=0.0001.
===> Epoch[79](300/324): Loss: 0.7511 || Learning rate: lr=0.0001.
===> Epoch[79](310/324): Loss: 0.7751 || Learning rate: lr=0.0001.
===> Epoch[79](320/324): Loss: 1.1915 || Learning rate: lr=0.0001.
===> Epoch[80](10/324): Loss: 0.7780 || Learning rate: lr=0.0001.
===> Epoch[80](20/324): Loss: 1.1636 || Learning rate: lr=0.0001.
===> Epoch[80](30/324): Loss: 0.8826 || Learning rate: lr=0.0001.
===> Epoch[80](40/324): Loss: 0.7768 || Learning rate: lr=0.0001.
===> Epoch[80](50/324): Loss: 0.9191 || Learning rate: lr=0.0001.
===> Epoch[80](60/324): Loss: 1.2923 || Learning rate: lr=0.0001.
===> Epoch[80](70/324): Loss: 0.7261 || Learning rate: lr=0.0001.
===> Epoch[80](80/324): Loss: 0.9160 || Learning rate: lr=0.0001.
===> Epoch[80](90/324): Loss: 1.1211 || Learning rate: lr=0.0001.
===> Epoch[80](100/324): Loss: 1.3286 || Learning rate: lr=0.0001.
===> Epoch[80](110/324): Loss: 1.2518 || Learning rate: lr=0.0001.
===> Epoch[80](120/324): Loss: 0.8087 || Learning rate: lr=0.0001.
===> Epoch[80](130/324): Loss: 0.8000 || Learning rate: lr=0.0001.
===> Epoch[80](140/324): Loss: 0.6263 || Learning rate: lr=0.0001.
===> Epoch[80](150/324): Loss: 1.1893 || Learning rate: lr=0.0001.
===> Epoch[80](160/324): Loss: 0.7719 || Learning rate: lr=0.0001.
===> Epoch[80](170/324): Loss: 1.0494 || Learning rate: lr=0.0001.
===> Epoch[80](180/324): Loss: 1.0998 || Learning rate: lr=0.0001.
===> Epoch[80](190/324): Loss: 1.0095 || Learning rate: lr=0.0001.
===> Epoch[80](200/324): Loss: 0.8667 || Learning rate: lr=0.0001.
===> Epoch[80](210/324): Loss: 1.1321 || Learning rate: lr=0.0001.
===> Epoch[80](220/324): Loss: 1.2665 || Learning rate: lr=0.0001.
===> Epoch[80](230/324): Loss: 1.2310 || Learning rate: lr=0.0001.
===> Epoch[80](240/324): Loss: 0.8301 || Learning rate: lr=0.0001.
===> Epoch[80](250/324): Loss: 1.0683 || Learning rate: lr=0.0001.
===> Epoch[80](260/324): Loss: 0.6933 || Learning rate: lr=0.0001.
===> Epoch[80](270/324): Loss: 1.1102 || Learning rate: lr=0.0001.
===> Epoch[80](280/324): Loss: 0.7369 || Learning rate: lr=0.0001.
===> Epoch[80](290/324): Loss: 0.6152 || Learning rate: lr=0.0001.
===> Epoch[80](300/324): Loss: 1.0814 || Learning rate: lr=0.0001.
===> Epoch[80](310/324): Loss: 0.7357 || Learning rate: lr=0.0001.
===> Epoch[80](320/324): Loss: 0.7414 || Learning rate: lr=0.0001.
Checkpoint saved to weights/epoch_v2_80.pth
===> Epoch[81](10/324): Loss: 1.1442 || Learning rate: lr=0.0001.
===> Epoch[81](20/324): Loss: 0.9037 || Learning rate: lr=0.0001.
===> Epoch[81](30/324): Loss: 1.0504 || Learning rate: lr=0.0001.
===> Epoch[81](40/324): Loss: 0.6895 || Learning rate: lr=0.0001.
===> Epoch[81](50/324): Loss: 1.0369 || Learning rate: lr=0.0001.
===> Epoch[81](60/324): Loss: 0.9018 || Learning rate: lr=0.0001.
===> Epoch[81](70/324): Loss: 0.8574 || Learning rate: lr=0.0001.
===> Epoch[81](80/324): Loss: 0.5247 || Learning rate: lr=0.0001.
===> Epoch[81](90/324): Loss: 1.1565 || Learning rate: lr=0.0001.
===> Epoch[81](100/324): Loss: 0.6840 || Learning rate: lr=0.0001.
===> Epoch[81](110/324): Loss: 1.3941 || Learning rate: lr=0.0001.
===> Epoch[81](120/324): Loss: 1.1768 || Learning rate: lr=0.0001.
===> Epoch[81](130/324): Loss: 0.8667 || Learning rate: lr=0.0001.
===> Epoch[81](140/324): Loss: 1.2468 || Learning rate: lr=0.0001.
===> Epoch[81](150/324): Loss: 0.7863 || Learning rate: lr=0.0001.
===> Epoch[81](160/324): Loss: 1.0164 || Learning rate: lr=0.0001.
===> Epoch[81](170/324): Loss: 1.2655 || Learning rate: lr=0.0001.
===> Epoch[81](180/324): Loss: 0.8856 || Learning rate: lr=0.0001.
===> Epoch[81](190/324): Loss: 1.0367 || Learning rate: lr=0.0001.
===> Epoch[81](200/324): Loss: 1.2364 || Learning rate: lr=0.0001.
===> Epoch[81](210/324): Loss: 0.6076 || Learning rate: lr=0.0001.
===> Epoch[81](220/324): Loss: 1.1879 || Learning rate: lr=0.0001.
===> Epoch[81](230/324): Loss: 1.1215 || Learning rate: lr=0.0001.
===> Epoch[81](240/324): Loss: 0.6205 || Learning rate: lr=0.0001.
===> Epoch[81](250/324): Loss: 0.9319 || Learning rate: lr=0.0001.
===> Epoch[81](260/324): Loss: 0.5611 || Learning rate: lr=0.0001.
===> Epoch[81](270/324): Loss: 0.8402 || Learning rate: lr=0.0001.
===> Epoch[81](280/324): Loss: 0.6998 || Learning rate: lr=0.0001.
===> Epoch[81](290/324): Loss: 0.5681 || Learning rate: lr=0.0001.
===> Epoch[81](300/324): Loss: 0.8596 || Learning rate: lr=0.0001.
===> Epoch[81](310/324): Loss: 0.9895 || Learning rate: lr=0.0001.
===> Epoch[81](320/324): Loss: 0.6996 || Learning rate: lr=0.0001.
===> Epoch[82](10/324): Loss: 0.8630 || Learning rate: lr=0.0001.
===> Epoch[82](20/324): Loss: 0.8453 || Learning rate: lr=0.0001.
===> Epoch[82](30/324): Loss: 1.3100 || Learning rate: lr=0.0001.
===> Epoch[82](40/324): Loss: 0.7818 || Learning rate: lr=0.0001.
===> Epoch[82](50/324): Loss: 1.2776 || Learning rate: lr=0.0001.
===> Epoch[82](60/324): Loss: 0.8643 || Learning rate: lr=0.0001.
===> Epoch[82](70/324): Loss: 1.3410 || Learning rate: lr=0.0001.
===> Epoch[82](80/324): Loss: 0.6575 || Learning rate: lr=0.0001.
===> Epoch[82](90/324): Loss: 0.5610 || Learning rate: lr=0.0001.
===> Epoch[82](100/324): Loss: 1.2475 || Learning rate: lr=0.0001.
===> Epoch[82](110/324): Loss: 0.4782 || Learning rate: lr=0.0001.
===> Epoch[82](120/324): Loss: 0.6656 || Learning rate: lr=0.0001.
===> Epoch[82](130/324): Loss: 0.8820 || Learning rate: lr=0.0001.
===> Epoch[82](140/324): Loss: 0.7446 || Learning rate: lr=0.0001.
===> Epoch[82](150/324): Loss: 1.6753 || Learning rate: lr=0.0001.
===> Epoch[82](160/324): Loss: 0.7444 || Learning rate: lr=0.0001.
===> Epoch[82](170/324): Loss: 1.9224 || Learning rate: lr=0.0001.
===> Epoch[82](180/324): Loss: 0.8916 || Learning rate: lr=0.0001.
===> Epoch[82](190/324): Loss: 1.5060 || Learning rate: lr=0.0001.
===> Epoch[82](200/324): Loss: 1.1359 || Learning rate: lr=0.0001.
===> Epoch[82](210/324): Loss: 1.3197 || Learning rate: lr=0.0001.
===> Epoch[82](220/324): Loss: 1.0170 || Learning rate: lr=0.0001.
===> Epoch[82](230/324): Loss: 0.9659 || Learning rate: lr=0.0001.
===> Epoch[82](240/324): Loss: 0.8612 || Learning rate: lr=0.0001.
===> Epoch[82](250/324): Loss: 0.8668 || Learning rate: lr=0.0001.
===> Epoch[82](260/324): Loss: 1.3060 || Learning rate: lr=0.0001.
===> Epoch[82](270/324): Loss: 1.0799 || Learning rate: lr=0.0001.
===> Epoch[82](280/324): Loss: 0.8483 || Learning rate: lr=0.0001.
===> Epoch[82](290/324): Loss: 1.2647 || Learning rate: lr=0.0001.
===> Epoch[82](300/324): Loss: 0.8809 || Learning rate: lr=0.0001.
===> Epoch[82](310/324): Loss: 0.8823 || Learning rate: lr=0.0001.
===> Epoch[82](320/324): Loss: 0.9867 || Learning rate: lr=0.0001.
===> Epoch[83](10/324): Loss: 0.7397 || Learning rate: lr=0.0001.
===> Epoch[83](20/324): Loss: 0.8577 || Learning rate: lr=0.0001.
===> Epoch[83](30/324): Loss: 1.1095 || Learning rate: lr=0.0001.
===> Epoch[83](40/324): Loss: 1.3020 || Learning rate: lr=0.0001.
===> Epoch[83](50/324): Loss: 1.0458 || Learning rate: lr=0.0001.
===> Epoch[83](60/324): Loss: 0.9206 || Learning rate: lr=0.0001.
===> Epoch[83](70/324): Loss: 0.7459 || Learning rate: lr=0.0001.
===> Epoch[83](80/324): Loss: 1.1142 || Learning rate: lr=0.0001.
===> Epoch[83](90/324): Loss: 1.4813 || Learning rate: lr=0.0001.
===> Epoch[83](100/324): Loss: 0.5042 || Learning rate: lr=0.0001.
===> Epoch[83](110/324): Loss: 1.0586 || Learning rate: lr=0.0001.
===> Epoch[83](120/324): Loss: 0.8998 || Learning rate: lr=0.0001.
===> Epoch[83](130/324): Loss: 0.7571 || Learning rate: lr=0.0001.
===> Epoch[83](140/324): Loss: 1.0107 || Learning rate: lr=0.0001.
===> Epoch[83](150/324): Loss: 0.5997 || Learning rate: lr=0.0001.
===> Epoch[83](160/324): Loss: 1.0745 || Learning rate: lr=0.0001.
===> Epoch[83](170/324): Loss: 1.0935 || Learning rate: lr=0.0001.
===> Epoch[83](180/324): Loss: 0.8814 || Learning rate: lr=0.0001.
===> Epoch[83](190/324): Loss: 1.3385 || Learning rate: lr=0.0001.
===> Epoch[83](200/324): Loss: 1.2942 || Learning rate: lr=0.0001.
===> Epoch[83](210/324): Loss: 1.0345 || Learning rate: lr=0.0001.
===> Epoch[83](220/324): Loss: 1.2975 || Learning rate: lr=0.0001.
===> Epoch[83](230/324): Loss: 0.6725 || Learning rate: lr=0.0001.
===> Epoch[83](240/324): Loss: 1.2278 || Learning rate: lr=0.0001.
===> Epoch[83](250/324): Loss: 1.7810 || Learning rate: lr=0.0001.
===> Epoch[83](260/324): Loss: 0.8758 || Learning rate: lr=0.0001.
===> Epoch[83](270/324): Loss: 1.4213 || Learning rate: lr=0.0001.
===> Epoch[83](280/324): Loss: 1.1699 || Learning rate: lr=0.0001.
===> Epoch[83](290/324): Loss: 1.2302 || Learning rate: lr=0.0001.
===> Epoch[83](300/324): Loss: 1.1332 || Learning rate: lr=0.0001.
===> Epoch[83](310/324): Loss: 1.3430 || Learning rate: lr=0.0001.
===> Epoch[83](320/324): Loss: 0.9573 || Learning rate: lr=0.0001.
===> Epoch[84](10/324): Loss: 0.9070 || Learning rate: lr=0.0001.
===> Epoch[84](20/324): Loss: 1.1015 || Learning rate: lr=0.0001.
===> Epoch[84](30/324): Loss: 0.5738 || Learning rate: lr=0.0001.
===> Epoch[84](40/324): Loss: 0.7712 || Learning rate: lr=0.0001.
===> Epoch[84](50/324): Loss: 1.0315 || Learning rate: lr=0.0001.
===> Epoch[84](60/324): Loss: 1.2611 || Learning rate: lr=0.0001.
===> Epoch[84](70/324): Loss: 0.8529 || Learning rate: lr=0.0001.
===> Epoch[84](80/324): Loss: 0.9507 || Learning rate: lr=0.0001.
===> Epoch[84](90/324): Loss: 1.6112 || Learning rate: lr=0.0001.
===> Epoch[84](100/324): Loss: 0.8598 || Learning rate: lr=0.0001.
===> Epoch[84](110/324): Loss: 0.8847 || Learning rate: lr=0.0001.
===> Epoch[84](120/324): Loss: 0.6146 || Learning rate: lr=0.0001.
===> Epoch[84](130/324): Loss: 0.6106 || Learning rate: lr=0.0001.
===> Epoch[84](140/324): Loss: 0.7346 || Learning rate: lr=0.0001.
===> Epoch[84](150/324): Loss: 0.9850 || Learning rate: lr=0.0001.
===> Epoch[84](160/324): Loss: 0.8218 || Learning rate: lr=0.0001.
===> Epoch[84](170/324): Loss: 1.0327 || Learning rate: lr=0.0001.
===> Epoch[84](180/324): Loss: 1.3138 || Learning rate: lr=0.0001.
===> Epoch[84](190/324): Loss: 0.7004 || Learning rate: lr=0.0001.
===> Epoch[84](200/324): Loss: 1.0448 || Learning rate: lr=0.0001.
===> Epoch[84](210/324): Loss: 0.7700 || Learning rate: lr=0.0001.
===> Epoch[84](220/324): Loss: 0.9728 || Learning rate: lr=0.0001.
===> Epoch[84](230/324): Loss: 1.2067 || Learning rate: lr=0.0001.
===> Epoch[84](240/324): Loss: 1.1815 || Learning rate: lr=0.0001.
===> Epoch[84](250/324): Loss: 1.4461 || Learning rate: lr=0.0001.
===> Epoch[84](260/324): Loss: 1.1966 || Learning rate: lr=0.0001.
===> Epoch[84](270/324): Loss: 0.8685 || Learning rate: lr=0.0001.
===> Epoch[84](280/324): Loss: 0.8136 || Learning rate: lr=0.0001.
===> Epoch[84](290/324): Loss: 1.1251 || Learning rate: lr=0.0001.
===> Epoch[84](300/324): Loss: 1.0052 || Learning rate: lr=0.0001.
===> Epoch[84](310/324): Loss: 0.9352 || Learning rate: lr=0.0001.
===> Epoch[84](320/324): Loss: 0.8430 || Learning rate: lr=0.0001.
===> Epoch[85](10/324): Loss: 1.4093 || Learning rate: lr=0.0001.
===> Epoch[85](20/324): Loss: 0.9277 || Learning rate: lr=0.0001.
===> Epoch[85](30/324): Loss: 0.6778 || Learning rate: lr=0.0001.
===> Epoch[85](40/324): Loss: 1.2876 || Learning rate: lr=0.0001.
===> Epoch[85](50/324): Loss: 1.1350 || Learning rate: lr=0.0001.
===> Epoch[85](60/324): Loss: 0.7570 || Learning rate: lr=0.0001.
===> Epoch[85](70/324): Loss: 1.1081 || Learning rate: lr=0.0001.
===> Epoch[85](80/324): Loss: 0.6574 || Learning rate: lr=0.0001.
===> Epoch[85](90/324): Loss: 0.5964 || Learning rate: lr=0.0001.
===> Epoch[85](100/324): Loss: 0.6266 || Learning rate: lr=0.0001.
===> Epoch[85](110/324): Loss: 0.5640 || Learning rate: lr=0.0001.
===> Epoch[85](120/324): Loss: 0.8569 || Learning rate: lr=0.0001.
===> Epoch[85](130/324): Loss: 0.7441 || Learning rate: lr=0.0001.
===> Epoch[85](140/324): Loss: 0.5768 || Learning rate: lr=0.0001.
===> Epoch[85](150/324): Loss: 0.7325 || Learning rate: lr=0.0001.
===> Epoch[85](160/324): Loss: 1.0382 || Learning rate: lr=0.0001.
===> Epoch[85](170/324): Loss: 1.3974 || Learning rate: lr=0.0001.
===> Epoch[85](180/324): Loss: 0.8601 || Learning rate: lr=0.0001.
===> Epoch[85](190/324): Loss: 0.8928 || Learning rate: lr=0.0001.
===> Epoch[85](200/324): Loss: 0.7042 || Learning rate: lr=0.0001.
===> Epoch[85](210/324): Loss: 0.8299 || Learning rate: lr=0.0001.
===> Epoch[85](220/324): Loss: 0.9372 || Learning rate: lr=0.0001.
===> Epoch[85](230/324): Loss: 0.8043 || Learning rate: lr=0.0001.
===> Epoch[85](240/324): Loss: 0.8369 || Learning rate: lr=0.0001.
===> Epoch[85](250/324): Loss: 1.2654 || Learning rate: lr=0.0001.
===> Epoch[85](260/324): Loss: 1.6473 || Learning rate: lr=0.0001.
===> Epoch[85](270/324): Loss: 1.3123 || Learning rate: lr=0.0001.
===> Epoch[85](280/324): Loss: 2.4451 || Learning rate: lr=0.0001.
===> Epoch[85](290/324): Loss: 4.3398 || Learning rate: lr=0.0001.
===> Epoch[85](300/324): Loss: 2.0097 || Learning rate: lr=0.0001.
===> Epoch[85](310/324): Loss: 2.2357 || Learning rate: lr=0.0001.
===> Epoch[85](320/324): Loss: 0.9456 || Learning rate: lr=0.0001.
===> Epoch[86](10/324): Loss: 1.0518 || Learning rate: lr=0.0001.
===> Epoch[86](20/324): Loss: 1.0215 || Learning rate: lr=0.0001.
===> Epoch[86](30/324): Loss: 0.7194 || Learning rate: lr=0.0001.
===> Epoch[86](40/324): Loss: 1.1700 || Learning rate: lr=0.0001.
===> Epoch[86](50/324): Loss: 1.0346 || Learning rate: lr=0.0001.
===> Epoch[86](60/324): Loss: 0.9997 || Learning rate: lr=0.0001.
===> Epoch[86](70/324): Loss: 1.3483 || Learning rate: lr=0.0001.
===> Epoch[86](80/324): Loss: 0.8360 || Learning rate: lr=0.0001.
===> Epoch[86](90/324): Loss: 1.0771 || Learning rate: lr=0.0001.
===> Epoch[86](100/324): Loss: 0.8058 || Learning rate: lr=0.0001.
===> Epoch[86](110/324): Loss: 1.3258 || Learning rate: lr=0.0001.
===> Epoch[86](120/324): Loss: 1.0357 || Learning rate: lr=0.0001.
===> Epoch[86](130/324): Loss: 1.4410 || Learning rate: lr=0.0001.
===> Epoch[86](140/324): Loss: 0.7273 || Learning rate: lr=0.0001.
===> Epoch[86](150/324): Loss: 1.0429 || Learning rate: lr=0.0001.
===> Epoch[86](160/324): Loss: 1.0334 || Learning rate: lr=0.0001.
===> Epoch[86](170/324): Loss: 1.0408 || Learning rate: lr=0.0001.
===> Epoch[86](180/324): Loss: 0.7635 || Learning rate: lr=0.0001.
===> Epoch[86](190/324): Loss: 0.6908 || Learning rate: lr=0.0001.
===> Epoch[86](200/324): Loss: 0.9475 || Learning rate: lr=0.0001.
===> Epoch[86](210/324): Loss: 1.1602 || Learning rate: lr=0.0001.
===> Epoch[86](220/324): Loss: 0.9376 || Learning rate: lr=0.0001.
===> Epoch[86](230/324): Loss: 0.5609 || Learning rate: lr=0.0001.
===> Epoch[86](240/324): Loss: 1.1322 || Learning rate: lr=0.0001.
===> Epoch[86](250/324): Loss: 0.9458 || Learning rate: lr=0.0001.
===> Epoch[86](260/324): Loss: 0.9998 || Learning rate: lr=0.0001.
===> Epoch[86](270/324): Loss: 1.1114 || Learning rate: lr=0.0001.
===> Epoch[86](280/324): Loss: 0.9879 || Learning rate: lr=0.0001.
===> Epoch[86](290/324): Loss: 0.9023 || Learning rate: lr=0.0001.
===> Epoch[86](300/324): Loss: 0.8135 || Learning rate: lr=0.0001.
===> Epoch[86](310/324): Loss: 1.1973 || Learning rate: lr=0.0001.
===> Epoch[86](320/324): Loss: 0.8040 || Learning rate: lr=0.0001.
===> Epoch[87](10/324): Loss: 1.0438 || Learning rate: lr=0.0001.
===> Epoch[87](20/324): Loss: 0.6603 || Learning rate: lr=0.0001.
===> Epoch[87](30/324): Loss: 1.1722 || Learning rate: lr=0.0001.
===> Epoch[87](40/324): Loss: 0.6560 || Learning rate: lr=0.0001.
===> Epoch[87](50/324): Loss: 1.0613 || Learning rate: lr=0.0001.
===> Epoch[87](60/324): Loss: 0.6633 || Learning rate: lr=0.0001.
===> Epoch[87](70/324): Loss: 1.0624 || Learning rate: lr=0.0001.
===> Epoch[87](80/324): Loss: 1.3181 || Learning rate: lr=0.0001.
===> Epoch[87](90/324): Loss: 0.8273 || Learning rate: lr=0.0001.
===> Epoch[87](100/324): Loss: 0.9598 || Learning rate: lr=0.0001.
===> Epoch[87](110/324): Loss: 0.5768 || Learning rate: lr=0.0001.
===> Epoch[87](120/324): Loss: 1.3646 || Learning rate: lr=0.0001.
===> Epoch[87](130/324): Loss: 1.0028 || Learning rate: lr=0.0001.
===> Epoch[87](140/324): Loss: 0.8363 || Learning rate: lr=0.0001.
===> Epoch[87](150/324): Loss: 0.9699 || Learning rate: lr=0.0001.
===> Epoch[87](160/324): Loss: 1.1118 || Learning rate: lr=0.0001.
===> Epoch[87](170/324): Loss: 0.5985 || Learning rate: lr=0.0001.
===> Epoch[87](180/324): Loss: 0.8353 || Learning rate: lr=0.0001.
===> Epoch[87](190/324): Loss: 0.8075 || Learning rate: lr=0.0001.
===> Epoch[87](200/324): Loss: 0.9586 || Learning rate: lr=0.0001.
===> Epoch[87](210/324): Loss: 0.8087 || Learning rate: lr=0.0001.
===> Epoch[87](220/324): Loss: 1.1348 || Learning rate: lr=0.0001.
===> Epoch[87](230/324): Loss: 0.9815 || Learning rate: lr=0.0001.
===> Epoch[87](240/324): Loss: 0.7945 || Learning rate: lr=0.0001.
===> Epoch[87](250/324): Loss: 1.0179 || Learning rate: lr=0.0001.
===> Epoch[87](260/324): Loss: 1.3643 || Learning rate: lr=0.0001.
===> Epoch[87](270/324): Loss: 0.6975 || Learning rate: lr=0.0001.
===> Epoch[87](280/324): Loss: 0.7818 || Learning rate: lr=0.0001.
===> Epoch[87](290/324): Loss: 0.9322 || Learning rate: lr=0.0001.
===> Epoch[87](300/324): Loss: 0.7565 || Learning rate: lr=0.0001.
===> Epoch[87](310/324): Loss: 0.8354 || Learning rate: lr=0.0001.
===> Epoch[87](320/324): Loss: 1.0238 || Learning rate: lr=0.0001.
===> Epoch[88](10/324): Loss: 0.7222 || Learning rate: lr=0.0001.
===> Epoch[88](20/324): Loss: 0.7527 || Learning rate: lr=0.0001.
===> Epoch[88](30/324): Loss: 0.7354 || Learning rate: lr=0.0001.
===> Epoch[88](40/324): Loss: 0.8673 || Learning rate: lr=0.0001.
===> Epoch[88](50/324): Loss: 0.9685 || Learning rate: lr=0.0001.
===> Epoch[88](60/324): Loss: 0.6681 || Learning rate: lr=0.0001.
===> Epoch[88](70/324): Loss: 0.7263 || Learning rate: lr=0.0001.
===> Epoch[88](80/324): Loss: 0.8810 || Learning rate: lr=0.0001.
===> Epoch[88](90/324): Loss: 0.8107 || Learning rate: lr=0.0001.
===> Epoch[88](100/324): Loss: 0.8317 || Learning rate: lr=0.0001.
===> Epoch[88](110/324): Loss: 0.6712 || Learning rate: lr=0.0001.
===> Epoch[88](120/324): Loss: 0.7398 || Learning rate: lr=0.0001.
===> Epoch[88](130/324): Loss: 0.6314 || Learning rate: lr=0.0001.
===> Epoch[88](140/324): Loss: 0.7177 || Learning rate: lr=0.0001.
===> Epoch[88](150/324): Loss: 1.0309 || Learning rate: lr=0.0001.
===> Epoch[88](160/324): Loss: 0.7228 || Learning rate: lr=0.0001.
===> Epoch[88](170/324): Loss: 0.8150 || Learning rate: lr=0.0001.
===> Epoch[88](180/324): Loss: 1.3451 || Learning rate: lr=0.0001.
===> Epoch[88](190/324): Loss: 0.9756 || Learning rate: lr=0.0001.
===> Epoch[88](200/324): Loss: 0.9456 || Learning rate: lr=0.0001.
===> Epoch[88](210/324): Loss: 0.9203 || Learning rate: lr=0.0001.
===> Epoch[88](220/324): Loss: 0.7958 || Learning rate: lr=0.0001.
===> Epoch[88](230/324): Loss: 0.9802 || Learning rate: lr=0.0001.
===> Epoch[88](240/324): Loss: 1.2071 || Learning rate: lr=0.0001.
===> Epoch[88](250/324): Loss: 0.8101 || Learning rate: lr=0.0001.
===> Epoch[88](260/324): Loss: 1.0792 || Learning rate: lr=0.0001.
===> Epoch[88](270/324): Loss: 1.8063 || Learning rate: lr=0.0001.
===> Epoch[88](280/324): Loss: 1.1909 || Learning rate: lr=0.0001.
===> Epoch[88](290/324): Loss: 1.0660 || Learning rate: lr=0.0001.
===> Epoch[88](300/324): Loss: 0.9249 || Learning rate: lr=0.0001.
===> Epoch[88](310/324): Loss: 0.9424 || Learning rate: lr=0.0001.
===> Epoch[88](320/324): Loss: 0.7260 || Learning rate: lr=0.0001.
===> Epoch[89](10/324): Loss: 1.0996 || Learning rate: lr=0.0001.
===> Epoch[89](20/324): Loss: 0.8657 || Learning rate: lr=0.0001.
===> Epoch[89](30/324): Loss: 0.8315 || Learning rate: lr=0.0001.
===> Epoch[89](40/324): Loss: 1.0571 || Learning rate: lr=0.0001.
===> Epoch[89](50/324): Loss: 0.8236 || Learning rate: lr=0.0001.
===> Epoch[89](60/324): Loss: 0.7999 || Learning rate: lr=0.0001.
===> Epoch[89](70/324): Loss: 0.6996 || Learning rate: lr=0.0001.
===> Epoch[89](80/324): Loss: 0.9775 || Learning rate: lr=0.0001.
===> Epoch[89](90/324): Loss: 1.1270 || Learning rate: lr=0.0001.
===> Epoch[89](100/324): Loss: 0.7225 || Learning rate: lr=0.0001.
===> Epoch[89](110/324): Loss: 0.7228 || Learning rate: lr=0.0001.
===> Epoch[89](120/324): Loss: 1.2515 || Learning rate: lr=0.0001.
===> Epoch[89](130/324): Loss: 0.6420 || Learning rate: lr=0.0001.
===> Epoch[89](140/324): Loss: 0.8276 || Learning rate: lr=0.0001.
===> Epoch[89](150/324): Loss: 0.4029 || Learning rate: lr=0.0001.
===> Epoch[89](160/324): Loss: 1.1232 || Learning rate: lr=0.0001.
===> Epoch[89](170/324): Loss: 0.8985 || Learning rate: lr=0.0001.
===> Epoch[89](180/324): Loss: 0.7779 || Learning rate: lr=0.0001.
===> Epoch[89](190/324): Loss: 1.2650 || Learning rate: lr=0.0001.
===> Epoch[89](200/324): Loss: 0.8007 || Learning rate: lr=0.0001.
===> Epoch[89](210/324): Loss: 0.9017 || Learning rate: lr=0.0001.
===> Epoch[89](220/324): Loss: 0.9603 || Learning rate: lr=0.0001.
===> Epoch[89](230/324): Loss: 0.8090 || Learning rate: lr=0.0001.
===> Epoch[89](240/324): Loss: 0.6391 || Learning rate: lr=0.0001.
===> Epoch[89](250/324): Loss: 1.2848 || Learning rate: lr=0.0001.
===> Epoch[89](260/324): Loss: 0.8909 || Learning rate: lr=0.0001.
===> Epoch[89](270/324): Loss: 1.0846 || Learning rate: lr=0.0001.
===> Epoch[89](280/324): Loss: 0.9387 || Learning rate: lr=0.0001.
===> Epoch[89](290/324): Loss: 1.3755 || Learning rate: lr=0.0001.
===> Epoch[89](300/324): Loss: 0.9535 || Learning rate: lr=0.0001.
===> Epoch[89](310/324): Loss: 0.8431 || Learning rate: lr=0.0001.
===> Epoch[89](320/324): Loss: 1.0633 || Learning rate: lr=0.0001.
===> Epoch[90](10/324): Loss: 0.7260 || Learning rate: lr=0.0001.
===> Epoch[90](20/324): Loss: 0.8126 || Learning rate: lr=0.0001.
===> Epoch[90](30/324): Loss: 0.7251 || Learning rate: lr=0.0001.
===> Epoch[90](40/324): Loss: 0.6012 || Learning rate: lr=0.0001.
===> Epoch[90](50/324): Loss: 0.8614 || Learning rate: lr=0.0001.
===> Epoch[90](60/324): Loss: 0.5378 || Learning rate: lr=0.0001.
===> Epoch[90](70/324): Loss: 0.6300 || Learning rate: lr=0.0001.
===> Epoch[90](80/324): Loss: 0.8330 || Learning rate: lr=0.0001.
===> Epoch[90](90/324): Loss: 1.1161 || Learning rate: lr=0.0001.
===> Epoch[90](100/324): Loss: 0.6541 || Learning rate: lr=0.0001.
===> Epoch[90](110/324): Loss: 1.3085 || Learning rate: lr=0.0001.
===> Epoch[90](120/324): Loss: 0.7315 || Learning rate: lr=0.0001.
===> Epoch[90](130/324): Loss: 0.9531 || Learning rate: lr=0.0001.
===> Epoch[90](140/324): Loss: 1.5537 || Learning rate: lr=0.0001.
===> Epoch[90](150/324): Loss: 0.8901 || Learning rate: lr=0.0001.
===> Epoch[90](160/324): Loss: 0.9846 || Learning rate: lr=0.0001.
===> Epoch[90](170/324): Loss: 0.9378 || Learning rate: lr=0.0001.
===> Epoch[90](180/324): Loss: 1.2865 || Learning rate: lr=0.0001.
===> Epoch[90](190/324): Loss: 0.7007 || Learning rate: lr=0.0001.
===> Epoch[90](200/324): Loss: 1.1142 || Learning rate: lr=0.0001.
===> Epoch[90](210/324): Loss: 1.1402 || Learning rate: lr=0.0001.
===> Epoch[90](220/324): Loss: 0.6514 || Learning rate: lr=0.0001.
===> Epoch[90](230/324): Loss: 1.6460 || Learning rate: lr=0.0001.
===> Epoch[90](240/324): Loss: 1.2594 || Learning rate: lr=0.0001.
===> Epoch[90](250/324): Loss: 1.0168 || Learning rate: lr=0.0001.
===> Epoch[90](260/324): Loss: 1.1300 || Learning rate: lr=0.0001.
===> Epoch[90](270/324): Loss: 1.2340 || Learning rate: lr=0.0001.
===> Epoch[90](280/324): Loss: 0.7296 || Learning rate: lr=0.0001.
===> Epoch[90](290/324): Loss: 2.0479 || Learning rate: lr=0.0001.
===> Epoch[90](300/324): Loss: 1.8744 || Learning rate: lr=0.0001.
===> Epoch[90](310/324): Loss: 2.2006 || Learning rate: lr=0.0001.
===> Epoch[90](320/324): Loss: 1.4792 || Learning rate: lr=0.0001.
===> Epoch[91](10/324): Loss: 1.0568 || Learning rate: lr=0.0001.
===> Epoch[91](20/324): Loss: 0.9385 || Learning rate: lr=0.0001.
===> Epoch[91](30/324): Loss: 0.7948 || Learning rate: lr=0.0001.
===> Epoch[91](40/324): Loss: 1.0488 || Learning rate: lr=0.0001.
===> Epoch[91](50/324): Loss: 0.9750 || Learning rate: lr=0.0001.
===> Epoch[91](60/324): Loss: 1.2140 || Learning rate: lr=0.0001.
===> Epoch[91](70/324): Loss: 1.0937 || Learning rate: lr=0.0001.
===> Epoch[91](80/324): Loss: 1.7537 || Learning rate: lr=0.0001.
===> Epoch[91](90/324): Loss: 1.2358 || Learning rate: lr=0.0001.
===> Epoch[91](100/324): Loss: 1.0791 || Learning rate: lr=0.0001.
===> Epoch[91](110/324): Loss: 0.6643 || Learning rate: lr=0.0001.
===> Epoch[91](120/324): Loss: 0.9133 || Learning rate: lr=0.0001.
===> Epoch[91](130/324): Loss: 1.0221 || Learning rate: lr=0.0001.
===> Epoch[91](140/324): Loss: 0.9963 || Learning rate: lr=0.0001.
===> Epoch[91](150/324): Loss: 1.1497 || Learning rate: lr=0.0001.
===> Epoch[91](160/324): Loss: 1.4313 || Learning rate: lr=0.0001.
===> Epoch[91](170/324): Loss: 0.8854 || Learning rate: lr=0.0001.
===> Epoch[91](180/324): Loss: 1.1135 || Learning rate: lr=0.0001.
===> Epoch[91](190/324): Loss: 1.5570 || Learning rate: lr=0.0001.
===> Epoch[91](200/324): Loss: 0.7182 || Learning rate: lr=0.0001.
===> Epoch[91](210/324): Loss: 0.9536 || Learning rate: lr=0.0001.
===> Epoch[91](220/324): Loss: 0.7143 || Learning rate: lr=0.0001.
===> Epoch[91](230/324): Loss: 0.6232 || Learning rate: lr=0.0001.
===> Epoch[91](240/324): Loss: 1.3556 || Learning rate: lr=0.0001.
===> Epoch[91](250/324): Loss: 0.9133 || Learning rate: lr=0.0001.
===> Epoch[91](260/324): Loss: 1.3210 || Learning rate: lr=0.0001.
===> Epoch[91](270/324): Loss: 1.0759 || Learning rate: lr=0.0001.
===> Epoch[91](280/324): Loss: 0.6171 || Learning rate: lr=0.0001.
===> Epoch[91](290/324): Loss: 1.1251 || Learning rate: lr=0.0001.
===> Epoch[91](300/324): Loss: 0.7540 || Learning rate: lr=0.0001.
===> Epoch[91](310/324): Loss: 0.9820 || Learning rate: lr=0.0001.
===> Epoch[91](320/324): Loss: 1.3426 || Learning rate: lr=0.0001.
===> Epoch[92](10/324): Loss: 1.1740 || Learning rate: lr=0.0001.
===> Epoch[92](20/324): Loss: 0.6908 || Learning rate: lr=0.0001.
===> Epoch[92](30/324): Loss: 1.3762 || Learning rate: lr=0.0001.
===> Epoch[92](40/324): Loss: 1.4499 || Learning rate: lr=0.0001.
===> Epoch[92](50/324): Loss: 0.8548 || Learning rate: lr=0.0001.
===> Epoch[92](60/324): Loss: 0.8434 || Learning rate: lr=0.0001.
===> Epoch[92](70/324): Loss: 0.9720 || Learning rate: lr=0.0001.
===> Epoch[92](80/324): Loss: 0.7795 || Learning rate: lr=0.0001.
===> Epoch[92](90/324): Loss: 0.9410 || Learning rate: lr=0.0001.
===> Epoch[92](100/324): Loss: 0.9877 || Learning rate: lr=0.0001.
===> Epoch[92](110/324): Loss: 0.8457 || Learning rate: lr=0.0001.
===> Epoch[92](120/324): Loss: 0.8974 || Learning rate: lr=0.0001.
===> Epoch[92](130/324): Loss: 1.2622 || Learning rate: lr=0.0001.
===> Epoch[92](140/324): Loss: 0.7813 || Learning rate: lr=0.0001.
===> Epoch[92](150/324): Loss: 1.0620 || Learning rate: lr=0.0001.
===> Epoch[92](160/324): Loss: 0.7606 || Learning rate: lr=0.0001.
===> Epoch[92](170/324): Loss: 0.7493 || Learning rate: lr=0.0001.
===> Epoch[92](180/324): Loss: 0.6508 || Learning rate: lr=0.0001.
===> Epoch[92](190/324): Loss: 0.6698 || Learning rate: lr=0.0001.
===> Epoch[92](200/324): Loss: 0.9748 || Learning rate: lr=0.0001.
===> Epoch[92](210/324): Loss: 0.6231 || Learning rate: lr=0.0001.
===> Epoch[92](220/324): Loss: 0.5662 || Learning rate: lr=0.0001.
===> Epoch[92](230/324): Loss: 0.6959 || Learning rate: lr=0.0001.
===> Epoch[92](240/324): Loss: 0.7214 || Learning rate: lr=0.0001.
===> Epoch[92](250/324): Loss: 0.8419 || Learning rate: lr=0.0001.
===> Epoch[92](260/324): Loss: 1.1783 || Learning rate: lr=0.0001.
===> Epoch[92](270/324): Loss: 1.0636 || Learning rate: lr=0.0001.
===> Epoch[92](280/324): Loss: 1.3282 || Learning rate: lr=0.0001.
===> Epoch[92](290/324): Loss: 1.3627 || Learning rate: lr=0.0001.
===> Epoch[92](300/324): Loss: 3.0806 || Learning rate: lr=0.0001.
===> Epoch[92](310/324): Loss: 1.1153 || Learning rate: lr=0.0001.
===> Epoch[92](320/324): Loss: 1.4286 || Learning rate: lr=0.0001.
===> Epoch[93](10/324): Loss: 1.0141 || Learning rate: lr=0.0001.
===> Epoch[93](20/324): Loss: 1.2837 || Learning rate: lr=0.0001.
===> Epoch[93](30/324): Loss: 0.8364 || Learning rate: lr=0.0001.
===> Epoch[93](40/324): Loss: 1.2578 || Learning rate: lr=0.0001.
===> Epoch[93](50/324): Loss: 1.2753 || Learning rate: lr=0.0001.
===> Epoch[93](60/324): Loss: 0.8177 || Learning rate: lr=0.0001.
===> Epoch[93](70/324): Loss: 1.1826 || Learning rate: lr=0.0001.
===> Epoch[93](80/324): Loss: 0.9299 || Learning rate: lr=0.0001.
===> Epoch[93](90/324): Loss: 0.7853 || Learning rate: lr=0.0001.
===> Epoch[93](100/324): Loss: 0.8195 || Learning rate: lr=0.0001.
===> Epoch[93](110/324): Loss: 0.7512 || Learning rate: lr=0.0001.
===> Epoch[93](120/324): Loss: 0.9039 || Learning rate: lr=0.0001.
===> Epoch[93](130/324): Loss: 0.8346 || Learning rate: lr=0.0001.
===> Epoch[93](140/324): Loss: 0.6282 || Learning rate: lr=0.0001.
===> Epoch[93](150/324): Loss: 0.5700 || Learning rate: lr=0.0001.
===> Epoch[93](160/324): Loss: 0.7593 || Learning rate: lr=0.0001.
===> Epoch[93](170/324): Loss: 1.0738 || Learning rate: lr=0.0001.
===> Epoch[93](180/324): Loss: 0.6350 || Learning rate: lr=0.0001.
===> Epoch[93](190/324): Loss: 0.7089 || Learning rate: lr=0.0001.
===> Epoch[93](200/324): Loss: 1.7778 || Learning rate: lr=0.0001.
===> Epoch[93](210/324): Loss: 0.9447 || Learning rate: lr=0.0001.
===> Epoch[93](220/324): Loss: 1.2738 || Learning rate: lr=0.0001.
===> Epoch[93](230/324): Loss: 1.5818 || Learning rate: lr=0.0001.
===> Epoch[93](240/324): Loss: 1.2879 || Learning rate: lr=0.0001.
===> Epoch[93](250/324): Loss: 1.4624 || Learning rate: lr=0.0001.
===> Epoch[93](260/324): Loss: 0.7827 || Learning rate: lr=0.0001.
===> Epoch[93](270/324): Loss: 1.1088 || Learning rate: lr=0.0001.
===> Epoch[93](280/324): Loss: 1.0726 || Learning rate: lr=0.0001.
===> Epoch[93](290/324): Loss: 0.6795 || Learning rate: lr=0.0001.
===> Epoch[93](300/324): Loss: 0.8848 || Learning rate: lr=0.0001.
===> Epoch[93](310/324): Loss: 0.8190 || Learning rate: lr=0.0001.
===> Epoch[93](320/324): Loss: 0.9441 || Learning rate: lr=0.0001.
===> Epoch[94](10/324): Loss: 1.0980 || Learning rate: lr=0.0001.
===> Epoch[94](20/324): Loss: 0.5709 || Learning rate: lr=0.0001.
===> Epoch[94](30/324): Loss: 1.7081 || Learning rate: lr=0.0001.
===> Epoch[94](40/324): Loss: 1.2345 || Learning rate: lr=0.0001.
===> Epoch[94](50/324): Loss: 1.1564 || Learning rate: lr=0.0001.
===> Epoch[94](60/324): Loss: 0.8058 || Learning rate: lr=0.0001.
===> Epoch[94](70/324): Loss: 1.7446 || Learning rate: lr=0.0001.
===> Epoch[94](80/324): Loss: 1.1443 || Learning rate: lr=0.0001.
===> Epoch[94](90/324): Loss: 0.7946 || Learning rate: lr=0.0001.
===> Epoch[94](100/324): Loss: 1.0148 || Learning rate: lr=0.0001.
===> Epoch[94](110/324): Loss: 0.6951 || Learning rate: lr=0.0001.
===> Epoch[94](120/324): Loss: 0.8742 || Learning rate: lr=0.0001.
===> Epoch[94](130/324): Loss: 1.2514 || Learning rate: lr=0.0001.
===> Epoch[94](140/324): Loss: 0.8167 || Learning rate: lr=0.0001.
===> Epoch[94](150/324): Loss: 1.4319 || Learning rate: lr=0.0001.
===> Epoch[94](160/324): Loss: 0.9556 || Learning rate: lr=0.0001.
===> Epoch[94](170/324): Loss: 0.5948 || Learning rate: lr=0.0001.
===> Epoch[94](180/324): Loss: 0.8386 || Learning rate: lr=0.0001.
===> Epoch[94](190/324): Loss: 0.6451 || Learning rate: lr=0.0001.
===> Epoch[94](200/324): Loss: 0.6988 || Learning rate: lr=0.0001.
===> Epoch[94](210/324): Loss: 0.7334 || Learning rate: lr=0.0001.
===> Epoch[94](220/324): Loss: 0.7802 || Learning rate: lr=0.0001.
===> Epoch[94](230/324): Loss: 1.2894 || Learning rate: lr=0.0001.
===> Epoch[94](240/324): Loss: 1.2521 || Learning rate: lr=0.0001.
===> Epoch[94](250/324): Loss: 1.2005 || Learning rate: lr=0.0001.
===> Epoch[94](260/324): Loss: 1.5421 || Learning rate: lr=0.0001.
===> Epoch[94](270/324): Loss: 1.3892 || Learning rate: lr=0.0001.
===> Epoch[94](280/324): Loss: 1.0296 || Learning rate: lr=0.0001.
===> Epoch[94](290/324): Loss: 0.9118 || Learning rate: lr=0.0001.
===> Epoch[94](300/324): Loss: 1.0486 || Learning rate: lr=0.0001.
===> Epoch[94](310/324): Loss: 0.7690 || Learning rate: lr=0.0001.
===> Epoch[94](320/324): Loss: 1.0756 || Learning rate: lr=0.0001.
===> Epoch[95](10/324): Loss: 0.5960 || Learning rate: lr=0.0001.
===> Epoch[95](20/324): Loss: 0.6337 || Learning rate: lr=0.0001.
===> Epoch[95](30/324): Loss: 0.9609 || Learning rate: lr=0.0001.
===> Epoch[95](40/324): Loss: 1.5431 || Learning rate: lr=0.0001.
===> Epoch[95](50/324): Loss: 1.3497 || Learning rate: lr=0.0001.
===> Epoch[95](60/324): Loss: 0.8838 || Learning rate: lr=0.0001.
===> Epoch[95](70/324): Loss: 1.3098 || Learning rate: lr=0.0001.
===> Epoch[95](80/324): Loss: 1.1623 || Learning rate: lr=0.0001.
===> Epoch[95](90/324): Loss: 0.7130 || Learning rate: lr=0.0001.
===> Epoch[95](100/324): Loss: 0.5881 || Learning rate: lr=0.0001.
===> Epoch[95](110/324): Loss: 0.4982 || Learning rate: lr=0.0001.
===> Epoch[95](120/324): Loss: 0.5605 || Learning rate: lr=0.0001.
===> Epoch[95](130/324): Loss: 0.7492 || Learning rate: lr=0.0001.
===> Epoch[95](140/324): Loss: 0.9238 || Learning rate: lr=0.0001.
===> Epoch[95](150/324): Loss: 0.6627 || Learning rate: lr=0.0001.
===> Epoch[95](160/324): Loss: 0.6725 || Learning rate: lr=0.0001.
===> Epoch[95](170/324): Loss: 1.0208 || Learning rate: lr=0.0001.
===> Epoch[95](180/324): Loss: 0.8391 || Learning rate: lr=0.0001.
===> Epoch[95](190/324): Loss: 0.8363 || Learning rate: lr=0.0001.
===> Epoch[95](200/324): Loss: 0.6838 || Learning rate: lr=0.0001.
===> Epoch[95](210/324): Loss: 0.6880 || Learning rate: lr=0.0001.
===> Epoch[95](220/324): Loss: 1.1046 || Learning rate: lr=0.0001.
===> Epoch[95](230/324): Loss: 0.5460 || Learning rate: lr=0.0001.
===> Epoch[95](240/324): Loss: 0.8266 || Learning rate: lr=0.0001.
===> Epoch[95](250/324): Loss: 1.1459 || Learning rate: lr=0.0001.
===> Epoch[95](260/324): Loss: 0.6924 || Learning rate: lr=0.0001.
===> Epoch[95](270/324): Loss: 0.9655 || Learning rate: lr=0.0001.
===> Epoch[95](280/324): Loss: 0.5528 || Learning rate: lr=0.0001.
===> Epoch[95](290/324): Loss: 1.1846 || Learning rate: lr=0.0001.
===> Epoch[95](300/324): Loss: 1.2625 || Learning rate: lr=0.0001.
===> Epoch[95](310/324): Loss: 0.8579 || Learning rate: lr=0.0001.
===> Epoch[95](320/324): Loss: 0.9713 || Learning rate: lr=0.0001.
===> Epoch[96](10/324): Loss: 1.4555 || Learning rate: lr=0.0001.
===> Epoch[96](20/324): Loss: 0.9765 || Learning rate: lr=0.0001.
===> Epoch[96](30/324): Loss: 0.6069 || Learning rate: lr=0.0001.
===> Epoch[96](40/324): Loss: 1.1265 || Learning rate: lr=0.0001.
===> Epoch[96](50/324): Loss: 1.0499 || Learning rate: lr=0.0001.
===> Epoch[96](60/324): Loss: 0.8207 || Learning rate: lr=0.0001.
===> Epoch[96](70/324): Loss: 1.1702 || Learning rate: lr=0.0001.
===> Epoch[96](80/324): Loss: 0.8370 || Learning rate: lr=0.0001.
===> Epoch[96](90/324): Loss: 0.8990 || Learning rate: lr=0.0001.
===> Epoch[96](100/324): Loss: 0.9651 || Learning rate: lr=0.0001.
===> Epoch[96](110/324): Loss: 0.6288 || Learning rate: lr=0.0001.
===> Epoch[96](120/324): Loss: 0.6006 || Learning rate: lr=0.0001.
===> Epoch[96](130/324): Loss: 1.0398 || Learning rate: lr=0.0001.
===> Epoch[96](140/324): Loss: 0.6629 || Learning rate: lr=0.0001.
===> Epoch[96](150/324): Loss: 0.9457 || Learning rate: lr=0.0001.
===> Epoch[96](160/324): Loss: 1.0009 || Learning rate: lr=0.0001.
===> Epoch[96](170/324): Loss: 0.7857 || Learning rate: lr=0.0001.
===> Epoch[96](180/324): Loss: 0.7516 || Learning rate: lr=0.0001.
===> Epoch[96](190/324): Loss: 0.9416 || Learning rate: lr=0.0001.
===> Epoch[96](200/324): Loss: 0.6495 || Learning rate: lr=0.0001.
===> Epoch[96](210/324): Loss: 0.7410 || Learning rate: lr=0.0001.
===> Epoch[96](220/324): Loss: 1.1109 || Learning rate: lr=0.0001.
===> Epoch[96](230/324): Loss: 0.7588 || Learning rate: lr=0.0001.
===> Epoch[96](240/324): Loss: 0.7427 || Learning rate: lr=0.0001.
===> Epoch[96](250/324): Loss: 1.0637 || Learning rate: lr=0.0001.
===> Epoch[96](260/324): Loss: 1.1659 || Learning rate: lr=0.0001.
===> Epoch[96](270/324): Loss: 0.9629 || Learning rate: lr=0.0001.
===> Epoch[96](280/324): Loss: 0.7514 || Learning rate: lr=0.0001.
===> Epoch[96](290/324): Loss: 0.7874 || Learning rate: lr=0.0001.
===> Epoch[96](300/324): Loss: 1.4534 || Learning rate: lr=0.0001.
===> Epoch[96](310/324): Loss: 0.9075 || Learning rate: lr=0.0001.
===> Epoch[96](320/324): Loss: 0.9676 || Learning rate: lr=0.0001.
===> Epoch[97](10/324): Loss: 1.0786 || Learning rate: lr=0.0001.
===> Epoch[97](20/324): Loss: 1.0060 || Learning rate: lr=0.0001.
===> Epoch[97](30/324): Loss: 1.0972 || Learning rate: lr=0.0001.
===> Epoch[97](40/324): Loss: 0.7629 || Learning rate: lr=0.0001.
===> Epoch[97](50/324): Loss: 0.9089 || Learning rate: lr=0.0001.
===> Epoch[97](60/324): Loss: 0.7295 || Learning rate: lr=0.0001.
===> Epoch[97](70/324): Loss: 0.6051 || Learning rate: lr=0.0001.
===> Epoch[97](80/324): Loss: 1.0554 || Learning rate: lr=0.0001.
===> Epoch[97](90/324): Loss: 0.7916 || Learning rate: lr=0.0001.
===> Epoch[97](100/324): Loss: 1.1736 || Learning rate: lr=0.0001.
===> Epoch[97](110/324): Loss: 1.0605 || Learning rate: lr=0.0001.
===> Epoch[97](120/324): Loss: 1.5719 || Learning rate: lr=0.0001.
===> Epoch[97](130/324): Loss: 1.3056 || Learning rate: lr=0.0001.
===> Epoch[97](140/324): Loss: 0.8596 || Learning rate: lr=0.0001.
===> Epoch[97](150/324): Loss: 1.2616 || Learning rate: lr=0.0001.
===> Epoch[97](160/324): Loss: 1.0293 || Learning rate: lr=0.0001.
===> Epoch[97](170/324): Loss: 0.9938 || Learning rate: lr=0.0001.
===> Epoch[97](180/324): Loss: 0.8996 || Learning rate: lr=0.0001.
===> Epoch[97](190/324): Loss: 1.6332 || Learning rate: lr=0.0001.
===> Epoch[97](200/324): Loss: 0.8844 || Learning rate: lr=0.0001.
===> Epoch[97](210/324): Loss: 1.1021 || Learning rate: lr=0.0001.
===> Epoch[97](220/324): Loss: 0.8776 || Learning rate: lr=0.0001.
===> Epoch[97](230/324): Loss: 1.0309 || Learning rate: lr=0.0001.
===> Epoch[97](240/324): Loss: 0.5261 || Learning rate: lr=0.0001.
===> Epoch[97](250/324): Loss: 1.1555 || Learning rate: lr=0.0001.
===> Epoch[97](260/324): Loss: 0.8174 || Learning rate: lr=0.0001.
===> Epoch[97](270/324): Loss: 0.7242 || Learning rate: lr=0.0001.
===> Epoch[97](280/324): Loss: 0.7800 || Learning rate: lr=0.0001.
===> Epoch[97](290/324): Loss: 0.9088 || Learning rate: lr=0.0001.
===> Epoch[97](300/324): Loss: 0.8591 || Learning rate: lr=0.0001.
===> Epoch[97](310/324): Loss: 0.9908 || Learning rate: lr=0.0001.
===> Epoch[97](320/324): Loss: 0.9425 || Learning rate: lr=0.0001.
===> Epoch[98](10/324): Loss: 0.8494 || Learning rate: lr=0.0001.
===> Epoch[98](20/324): Loss: 0.5031 || Learning rate: lr=0.0001.
===> Epoch[98](30/324): Loss: 0.7990 || Learning rate: lr=0.0001.
===> Epoch[98](40/324): Loss: 1.1073 || Learning rate: lr=0.0001.
===> Epoch[98](50/324): Loss: 0.8946 || Learning rate: lr=0.0001.
===> Epoch[98](60/324): Loss: 0.6295 || Learning rate: lr=0.0001.
===> Epoch[98](70/324): Loss: 1.1399 || Learning rate: lr=0.0001.
===> Epoch[98](80/324): Loss: 0.6081 || Learning rate: lr=0.0001.
===> Epoch[98](90/324): Loss: 0.7658 || Learning rate: lr=0.0001.
===> Epoch[98](100/324): Loss: 0.7064 || Learning rate: lr=0.0001.
===> Epoch[98](110/324): Loss: 0.8901 || Learning rate: lr=0.0001.
===> Epoch[98](120/324): Loss: 0.8995 || Learning rate: lr=0.0001.
===> Epoch[98](130/324): Loss: 0.7943 || Learning rate: lr=0.0001.
===> Epoch[98](140/324): Loss: 1.0041 || Learning rate: lr=0.0001.
===> Epoch[98](150/324): Loss: 0.8628 || Learning rate: lr=0.0001.
===> Epoch[98](160/324): Loss: 1.2267 || Learning rate: lr=0.0001.
===> Epoch[98](170/324): Loss: 0.9117 || Learning rate: lr=0.0001.
===> Epoch[98](180/324): Loss: 0.5246 || Learning rate: lr=0.0001.
===> Epoch[98](190/324): Loss: 0.6133 || Learning rate: lr=0.0001.
===> Epoch[98](200/324): Loss: 0.9285 || Learning rate: lr=0.0001.
===> Epoch[98](210/324): Loss: 0.9287 || Learning rate: lr=0.0001.
===> Epoch[98](220/324): Loss: 0.6933 || Learning rate: lr=0.0001.
===> Epoch[98](230/324): Loss: 1.5416 || Learning rate: lr=0.0001.
===> Epoch[98](240/324): Loss: 1.0098 || Learning rate: lr=0.0001.
===> Epoch[98](250/324): Loss: 1.4014 || Learning rate: lr=0.0001.
===> Epoch[98](260/324): Loss: 1.2074 || Learning rate: lr=0.0001.
===> Epoch[98](270/324): Loss: 1.3703 || Learning rate: lr=0.0001.
===> Epoch[98](280/324): Loss: 1.3316 || Learning rate: lr=0.0001.
===> Epoch[98](290/324): Loss: 0.6384 || Learning rate: lr=0.0001.
===> Epoch[98](300/324): Loss: 0.9858 || Learning rate: lr=0.0001.
===> Epoch[98](310/324): Loss: 1.2582 || Learning rate: lr=0.0001.
===> Epoch[98](320/324): Loss: 1.2928 || Learning rate: lr=0.0001.
===> Epoch[99](10/324): Loss: 0.7404 || Learning rate: lr=0.0001.
===> Epoch[99](20/324): Loss: 1.0987 || Learning rate: lr=0.0001.
===> Epoch[99](30/324): Loss: 1.0122 || Learning rate: lr=0.0001.
===> Epoch[99](40/324): Loss: 1.2220 || Learning rate: lr=0.0001.
===> Epoch[99](50/324): Loss: 1.0360 || Learning rate: lr=0.0001.
===> Epoch[99](60/324): Loss: 0.8762 || Learning rate: lr=0.0001.
===> Epoch[99](70/324): Loss: 0.9085 || Learning rate: lr=0.0001.
===> Epoch[99](80/324): Loss: 1.0050 || Learning rate: lr=0.0001.
===> Epoch[99](90/324): Loss: 0.5325 || Learning rate: lr=0.0001.
===> Epoch[99](100/324): Loss: 0.7531 || Learning rate: lr=0.0001.
===> Epoch[99](110/324): Loss: 0.6041 || Learning rate: lr=0.0001.
===> Epoch[99](120/324): Loss: 0.7140 || Learning rate: lr=0.0001.
===> Epoch[99](130/324): Loss: 1.1172 || Learning rate: lr=0.0001.
===> Epoch[99](140/324): Loss: 0.7470 || Learning rate: lr=0.0001.
===> Epoch[99](150/324): Loss: 0.6842 || Learning rate: lr=0.0001.
===> Epoch[99](160/324): Loss: 0.9630 || Learning rate: lr=0.0001.
===> Epoch[99](170/324): Loss: 1.3053 || Learning rate: lr=0.0001.
===> Epoch[99](180/324): Loss: 0.9707 || Learning rate: lr=0.0001.
===> Epoch[99](190/324): Loss: 0.7439 || Learning rate: lr=0.0001.
===> Epoch[99](200/324): Loss: 1.0540 || Learning rate: lr=0.0001.
===> Epoch[99](210/324): Loss: 1.2845 || Learning rate: lr=0.0001.
===> Epoch[99](220/324): Loss: 0.7209 || Learning rate: lr=0.0001.
===> Epoch[99](230/324): Loss: 0.8978 || Learning rate: lr=0.0001.
===> Epoch[99](240/324): Loss: 0.8933 || Learning rate: lr=0.0001.
===> Epoch[99](250/324): Loss: 1.0404 || Learning rate: lr=0.0001.
===> Epoch[99](260/324): Loss: 1.0283 || Learning rate: lr=0.0001.
===> Epoch[99](270/324): Loss: 0.9037 || Learning rate: lr=0.0001.
===> Epoch[99](280/324): Loss: 0.8968 || Learning rate: lr=0.0001.
===> Epoch[99](290/324): Loss: 0.9130 || Learning rate: lr=0.0001.
===> Epoch[99](300/324): Loss: 0.5805 || Learning rate: lr=0.0001.
===> Epoch[99](310/324): Loss: 0.8805 || Learning rate: lr=0.0001.
===> Epoch[99](320/324): Loss: 0.8694 || Learning rate: lr=0.0001.
===> Epoch[100](10/324): Loss: 0.6615 || Learning rate: lr=0.0001.
===> Epoch[100](20/324): Loss: 0.6564 || Learning rate: lr=0.0001.
===> Epoch[100](30/324): Loss: 0.8231 || Learning rate: lr=0.0001.
===> Epoch[100](40/324): Loss: 0.8469 || Learning rate: lr=0.0001.
===> Epoch[100](50/324): Loss: 1.5394 || Learning rate: lr=0.0001.
===> Epoch[100](60/324): Loss: 0.7371 || Learning rate: lr=0.0001.
===> Epoch[100](70/324): Loss: 1.4643 || Learning rate: lr=0.0001.
===> Epoch[100](80/324): Loss: 1.0107 || Learning rate: lr=0.0001.
===> Epoch[100](90/324): Loss: 1.3093 || Learning rate: lr=0.0001.
===> Epoch[100](100/324): Loss: 1.1770 || Learning rate: lr=0.0001.
===> Epoch[100](110/324): Loss: 1.1434 || Learning rate: lr=0.0001.
===> Epoch[100](120/324): Loss: 1.4803 || Learning rate: lr=0.0001.
===> Epoch[100](130/324): Loss: 1.1029 || Learning rate: lr=0.0001.
===> Epoch[100](140/324): Loss: 1.3768 || Learning rate: lr=0.0001.
===> Epoch[100](150/324): Loss: 0.8036 || Learning rate: lr=0.0001.
===> Epoch[100](160/324): Loss: 1.2498 || Learning rate: lr=0.0001.
===> Epoch[100](170/324): Loss: 1.0170 || Learning rate: lr=0.0001.
===> Epoch[100](180/324): Loss: 7.2568 || Learning rate: lr=0.0001.
===> Epoch[100](190/324): Loss: 9.0258 || Learning rate: lr=0.0001.
===> Epoch[100](200/324): Loss: 2.3619 || Learning rate: lr=0.0001.
===> Epoch[100](210/324): Loss: 1.9701 || Learning rate: lr=0.0001.
===> Epoch[100](220/324): Loss: 1.4336 || Learning rate: lr=0.0001.
===> Epoch[100](230/324): Loss: 1.0689 || Learning rate: lr=0.0001.
===> Epoch[100](240/324): Loss: 1.3704 || Learning rate: lr=0.0001.
===> Epoch[100](250/324): Loss: 1.7413 || Learning rate: lr=0.0001.
===> Epoch[100](260/324): Loss: 1.0780 || Learning rate: lr=0.0001.
===> Epoch[100](270/324): Loss: 0.5783 || Learning rate: lr=0.0001.
===> Epoch[100](280/324): Loss: 1.1835 || Learning rate: lr=0.0001.
===> Epoch[100](290/324): Loss: 1.3504 || Learning rate: lr=0.0001.
===> Epoch[100](300/324): Loss: 1.1827 || Learning rate: lr=0.0001.
===> Epoch[100](310/324): Loss: 1.7672 || Learning rate: lr=0.0001.
===> Epoch[100](320/324): Loss: 1.2901 || Learning rate: lr=0.0001.
Checkpoint saved to weights/epoch_v2_100.pth
===> Epoch[101](10/324): Loss: 0.9212 || Learning rate: lr=5e-05.
===> Epoch[101](20/324): Loss: 0.9699 || Learning rate: lr=5e-05.
===> Epoch[101](30/324): Loss: 0.7203 || Learning rate: lr=5e-05.
===> Epoch[101](40/324): Loss: 0.8167 || Learning rate: lr=5e-05.
===> Epoch[101](50/324): Loss: 0.5763 || Learning rate: lr=5e-05.
===> Epoch[101](60/324): Loss: 0.9345 || Learning rate: lr=5e-05.
===> Epoch[101](70/324): Loss: 1.2717 || Learning rate: lr=5e-05.
===> Epoch[101](80/324): Loss: 0.6847 || Learning rate: lr=5e-05.
===> Epoch[101](90/324): Loss: 0.9798 || Learning rate: lr=5e-05.
===> Epoch[101](100/324): Loss: 0.7956 || Learning rate: lr=5e-05.
===> Epoch[101](110/324): Loss: 1.2877 || Learning rate: lr=5e-05.
===> Epoch[101](120/324): Loss: 1.0043 || Learning rate: lr=5e-05.
===> Epoch[101](130/324): Loss: 0.6028 || Learning rate: lr=5e-05.
===> Epoch[101](140/324): Loss: 0.9587 || Learning rate: lr=5e-05.
===> Epoch[101](150/324): Loss: 0.6547 || Learning rate: lr=5e-05.
===> Epoch[101](160/324): Loss: 0.9028 || Learning rate: lr=5e-05.
===> Epoch[101](170/324): Loss: 0.6846 || Learning rate: lr=5e-05.
===> Epoch[101](180/324): Loss: 0.5376 || Learning rate: lr=5e-05.
===> Epoch[101](190/324): Loss: 1.1598 || Learning rate: lr=5e-05.
===> Epoch[101](200/324): Loss: 0.7750 || Learning rate: lr=5e-05.
===> Epoch[101](210/324): Loss: 0.7821 || Learning rate: lr=5e-05.
===> Epoch[101](220/324): Loss: 1.1656 || Learning rate: lr=5e-05.
===> Epoch[101](230/324): Loss: 0.7172 || Learning rate: lr=5e-05.
===> Epoch[101](240/324): Loss: 0.8173 || Learning rate: lr=5e-05.
===> Epoch[101](250/324): Loss: 0.6567 || Learning rate: lr=5e-05.
===> Epoch[101](260/324): Loss: 0.9634 || Learning rate: lr=5e-05.
===> Epoch[101](270/324): Loss: 0.8420 || Learning rate: lr=5e-05.
===> Epoch[101](280/324): Loss: 0.6681 || Learning rate: lr=5e-05.
===> Epoch[101](290/324): Loss: 1.0789 || Learning rate: lr=5e-05.
===> Epoch[101](300/324): Loss: 0.6696 || Learning rate: lr=5e-05.
===> Epoch[101](310/324): Loss: 0.8317 || Learning rate: lr=5e-05.
===> Epoch[101](320/324): Loss: 0.7043 || Learning rate: lr=5e-05.
===> Epoch[102](10/324): Loss: 0.4944 || Learning rate: lr=5e-05.
===> Epoch[102](20/324): Loss: 0.6570 || Learning rate: lr=5e-05.
===> Epoch[102](30/324): Loss: 0.8784 || Learning rate: lr=5e-05.
===> Epoch[102](40/324): Loss: 1.0269 || Learning rate: lr=5e-05.
===> Epoch[102](50/324): Loss: 0.9366 || Learning rate: lr=5e-05.
===> Epoch[102](60/324): Loss: 0.6050 || Learning rate: lr=5e-05.
===> Epoch[102](70/324): Loss: 1.5992 || Learning rate: lr=5e-05.
===> Epoch[102](80/324): Loss: 0.7724 || Learning rate: lr=5e-05.
===> Epoch[102](90/324): Loss: 0.5657 || Learning rate: lr=5e-05.
===> Epoch[102](100/324): Loss: 1.1238 || Learning rate: lr=5e-05.
===> Epoch[102](110/324): Loss: 0.5732 || Learning rate: lr=5e-05.
===> Epoch[102](120/324): Loss: 1.0462 || Learning rate: lr=5e-05.
===> Epoch[102](130/324): Loss: 0.7725 || Learning rate: lr=5e-05.
===> Epoch[102](140/324): Loss: 0.9434 || Learning rate: lr=5e-05.
===> Epoch[102](150/324): Loss: 1.0727 || Learning rate: lr=5e-05.
===> Epoch[102](160/324): Loss: 0.7252 || Learning rate: lr=5e-05.
===> Epoch[102](170/324): Loss: 0.8802 || Learning rate: lr=5e-05.
===> Epoch[102](180/324): Loss: 1.0807 || Learning rate: lr=5e-05.
===> Epoch[102](190/324): Loss: 0.7889 || Learning rate: lr=5e-05.
===> Epoch[102](200/324): Loss: 0.5441 || Learning rate: lr=5e-05.
===> Epoch[102](210/324): Loss: 0.6726 || Learning rate: lr=5e-05.
===> Epoch[102](220/324): Loss: 1.0033 || Learning rate: lr=5e-05.
===> Epoch[102](230/324): Loss: 0.7859 || Learning rate: lr=5e-05.
===> Epoch[102](240/324): Loss: 0.7848 || Learning rate: lr=5e-05.
===> Epoch[102](250/324): Loss: 0.8724 || Learning rate: lr=5e-05.
===> Epoch[102](260/324): Loss: 0.5469 || Learning rate: lr=5e-05.
===> Epoch[102](270/324): Loss: 0.6136 || Learning rate: lr=5e-05.
===> Epoch[102](280/324): Loss: 0.9346 || Learning rate: lr=5e-05.
===> Epoch[102](290/324): Loss: 0.9290 || Learning rate: lr=5e-05.
===> Epoch[102](300/324): Loss: 1.1330 || Learning rate: lr=5e-05.
===> Epoch[102](310/324): Loss: 1.2093 || Learning rate: lr=5e-05.
===> Epoch[102](320/324): Loss: 0.6937 || Learning rate: lr=5e-05.
===> Epoch[103](10/324): Loss: 0.6919 || Learning rate: lr=5e-05.
===> Epoch[103](20/324): Loss: 1.0326 || Learning rate: lr=5e-05.
===> Epoch[103](30/324): Loss: 1.0265 || Learning rate: lr=5e-05.
===> Epoch[103](40/324): Loss: 0.6441 || Learning rate: lr=5e-05.
===> Epoch[103](50/324): Loss: 0.8079 || Learning rate: lr=5e-05.
===> Epoch[103](60/324): Loss: 0.6801 || Learning rate: lr=5e-05.
===> Epoch[103](70/324): Loss: 0.6366 || Learning rate: lr=5e-05.
===> Epoch[103](80/324): Loss: 0.5515 || Learning rate: lr=5e-05.
===> Epoch[103](90/324): Loss: 1.1472 || Learning rate: lr=5e-05.
===> Epoch[103](100/324): Loss: 0.9676 || Learning rate: lr=5e-05.
===> Epoch[103](110/324): Loss: 0.6933 || Learning rate: lr=5e-05.
===> Epoch[103](120/324): Loss: 0.7984 || Learning rate: lr=5e-05.
===> Epoch[103](130/324): Loss: 0.7633 || Learning rate: lr=5e-05.
===> Epoch[103](140/324): Loss: 1.2472 || Learning rate: lr=5e-05.
===> Epoch[103](150/324): Loss: 0.6394 || Learning rate: lr=5e-05.
===> Epoch[103](160/324): Loss: 1.0846 || Learning rate: lr=5e-05.
===> Epoch[103](170/324): Loss: 0.7371 || Learning rate: lr=5e-05.
===> Epoch[103](180/324): Loss: 1.1750 || Learning rate: lr=5e-05.
===> Epoch[103](190/324): Loss: 0.7631 || Learning rate: lr=5e-05.
===> Epoch[103](200/324): Loss: 0.4481 || Learning rate: lr=5e-05.
===> Epoch[103](210/324): Loss: 0.8500 || Learning rate: lr=5e-05.
===> Epoch[103](220/324): Loss: 0.7128 || Learning rate: lr=5e-05.
===> Epoch[103](230/324): Loss: 0.9266 || Learning rate: lr=5e-05.
===> Epoch[103](240/324): Loss: 0.5170 || Learning rate: lr=5e-05.
===> Epoch[103](250/324): Loss: 0.6732 || Learning rate: lr=5e-05.
===> Epoch[103](260/324): Loss: 0.5915 || Learning rate: lr=5e-05.
===> Epoch[103](270/324): Loss: 0.9644 || Learning rate: lr=5e-05.
===> Epoch[103](280/324): Loss: 1.0968 || Learning rate: lr=5e-05.
===> Epoch[103](290/324): Loss: 0.6869 || Learning rate: lr=5e-05.
===> Epoch[103](300/324): Loss: 0.9965 || Learning rate: lr=5e-05.
===> Epoch[103](310/324): Loss: 0.8839 || Learning rate: lr=5e-05.
===> Epoch[103](320/324): Loss: 0.7818 || Learning rate: lr=5e-05.
===> Epoch[104](10/324): Loss: 0.6570 || Learning rate: lr=5e-05.
===> Epoch[104](20/324): Loss: 1.0682 || Learning rate: lr=5e-05.
===> Epoch[104](30/324): Loss: 0.9069 || Learning rate: lr=5e-05.
===> Epoch[104](40/324): Loss: 0.8138 || Learning rate: lr=5e-05.
===> Epoch[104](50/324): Loss: 0.9184 || Learning rate: lr=5e-05.
===> Epoch[104](60/324): Loss: 0.7362 || Learning rate: lr=5e-05.
===> Epoch[104](70/324): Loss: 0.6500 || Learning rate: lr=5e-05.
===> Epoch[104](80/324): Loss: 0.8784 || Learning rate: lr=5e-05.
===> Epoch[104](90/324): Loss: 0.7443 || Learning rate: lr=5e-05.
===> Epoch[104](100/324): Loss: 0.8629 || Learning rate: lr=5e-05.
===> Epoch[104](110/324): Loss: 1.0866 || Learning rate: lr=5e-05.
===> Epoch[104](120/324): Loss: 0.9442 || Learning rate: lr=5e-05.
===> Epoch[104](130/324): Loss: 0.5546 || Learning rate: lr=5e-05.
===> Epoch[104](140/324): Loss: 0.8485 || Learning rate: lr=5e-05.
===> Epoch[104](150/324): Loss: 0.7978 || Learning rate: lr=5e-05.
===> Epoch[104](160/324): Loss: 0.7184 || Learning rate: lr=5e-05.
===> Epoch[104](170/324): Loss: 0.7063 || Learning rate: lr=5e-05.
===> Epoch[104](180/324): Loss: 0.7565 || Learning rate: lr=5e-05.
===> Epoch[104](190/324): Loss: 0.7468 || Learning rate: lr=5e-05.
===> Epoch[104](200/324): Loss: 1.0592 || Learning rate: lr=5e-05.
===> Epoch[104](210/324): Loss: 0.8585 || Learning rate: lr=5e-05.
===> Epoch[104](220/324): Loss: 0.7274 || Learning rate: lr=5e-05.
===> Epoch[104](230/324): Loss: 0.9169 || Learning rate: lr=5e-05.
===> Epoch[104](240/324): Loss: 0.5689 || Learning rate: lr=5e-05.
===> Epoch[104](250/324): Loss: 0.8595 || Learning rate: lr=5e-05.
===> Epoch[104](260/324): Loss: 0.9599 || Learning rate: lr=5e-05.
===> Epoch[104](270/324): Loss: 1.1848 || Learning rate: lr=5e-05.
===> Epoch[104](280/324): Loss: 0.5835 || Learning rate: lr=5e-05.
===> Epoch[104](290/324): Loss: 0.8230 || Learning rate: lr=5e-05.
===> Epoch[104](300/324): Loss: 0.7396 || Learning rate: lr=5e-05.
===> Epoch[104](310/324): Loss: 0.7397 || Learning rate: lr=5e-05.
===> Epoch[104](320/324): Loss: 1.1447 || Learning rate: lr=5e-05.
===> Epoch[105](10/324): Loss: 0.8271 || Learning rate: lr=5e-05.
===> Epoch[105](20/324): Loss: 0.7404 || Learning rate: lr=5e-05.
===> Epoch[105](30/324): Loss: 1.1346 || Learning rate: lr=5e-05.
===> Epoch[105](40/324): Loss: 0.9446 || Learning rate: lr=5e-05.
===> Epoch[105](50/324): Loss: 1.0732 || Learning rate: lr=5e-05.
===> Epoch[105](60/324): Loss: 1.1228 || Learning rate: lr=5e-05.
===> Epoch[105](70/324): Loss: 1.0185 || Learning rate: lr=5e-05.
===> Epoch[105](80/324): Loss: 0.6657 || Learning rate: lr=5e-05.
===> Epoch[105](90/324): Loss: 0.6840 || Learning rate: lr=5e-05.
===> Epoch[105](100/324): Loss: 0.8605 || Learning rate: lr=5e-05.
===> Epoch[105](110/324): Loss: 0.7154 || Learning rate: lr=5e-05.
===> Epoch[105](120/324): Loss: 0.9100 || Learning rate: lr=5e-05.
===> Epoch[105](130/324): Loss: 1.0705 || Learning rate: lr=5e-05.
===> Epoch[105](140/324): Loss: 0.9024 || Learning rate: lr=5e-05.
===> Epoch[105](150/324): Loss: 0.8191 || Learning rate: lr=5e-05.
===> Epoch[105](160/324): Loss: 0.8184 || Learning rate: lr=5e-05.
===> Epoch[105](170/324): Loss: 0.7533 || Learning rate: lr=5e-05.
===> Epoch[105](180/324): Loss: 0.6079 || Learning rate: lr=5e-05.
===> Epoch[105](190/324): Loss: 1.1884 || Learning rate: lr=5e-05.
===> Epoch[105](200/324): Loss: 0.7391 || Learning rate: lr=5e-05.
===> Epoch[105](210/324): Loss: 0.6807 || Learning rate: lr=5e-05.
===> Epoch[105](220/324): Loss: 0.5725 || Learning rate: lr=5e-05.
===> Epoch[105](230/324): Loss: 0.6025 || Learning rate: lr=5e-05.
===> Epoch[105](240/324): Loss: 1.1644 || Learning rate: lr=5e-05.
===> Epoch[105](250/324): Loss: 0.7510 || Learning rate: lr=5e-05.
===> Epoch[105](260/324): Loss: 0.8350 || Learning rate: lr=5e-05.
===> Epoch[105](270/324): Loss: 0.7848 || Learning rate: lr=5e-05.
===> Epoch[105](280/324): Loss: 1.0573 || Learning rate: lr=5e-05.
===> Epoch[105](290/324): Loss: 1.2079 || Learning rate: lr=5e-05.
===> Epoch[105](300/324): Loss: 0.9780 || Learning rate: lr=5e-05.
===> Epoch[105](310/324): Loss: 0.6914 || Learning rate: lr=5e-05.
===> Epoch[105](320/324): Loss: 1.1324 || Learning rate: lr=5e-05.
===> Epoch[106](10/324): Loss: 0.6606 || Learning rate: lr=5e-05.
===> Epoch[106](20/324): Loss: 0.7143 || Learning rate: lr=5e-05.
===> Epoch[106](30/324): Loss: 0.9095 || Learning rate: lr=5e-05.
===> Epoch[106](40/324): Loss: 0.5487 || Learning rate: lr=5e-05.
===> Epoch[106](50/324): Loss: 0.8434 || Learning rate: lr=5e-05.
===> Epoch[106](60/324): Loss: 0.7826 || Learning rate: lr=5e-05.
===> Epoch[106](70/324): Loss: 0.7168 || Learning rate: lr=5e-05.
===> Epoch[106](80/324): Loss: 0.9871 || Learning rate: lr=5e-05.
===> Epoch[106](90/324): Loss: 0.9742 || Learning rate: lr=5e-05.
===> Epoch[106](100/324): Loss: 0.8049 || Learning rate: lr=5e-05.
===> Epoch[106](110/324): Loss: 1.0221 || Learning rate: lr=5e-05.
===> Epoch[106](120/324): Loss: 0.7930 || Learning rate: lr=5e-05.
===> Epoch[106](130/324): Loss: 1.1180 || Learning rate: lr=5e-05.
===> Epoch[106](140/324): Loss: 0.8584 || Learning rate: lr=5e-05.
===> Epoch[106](150/324): Loss: 0.7360 || Learning rate: lr=5e-05.
===> Epoch[106](160/324): Loss: 0.5143 || Learning rate: lr=5e-05.
===> Epoch[106](170/324): Loss: 1.1269 || Learning rate: lr=5e-05.
===> Epoch[106](180/324): Loss: 0.7445 || Learning rate: lr=5e-05.
===> Epoch[106](190/324): Loss: 0.7942 || Learning rate: lr=5e-05.
===> Epoch[106](200/324): Loss: 0.8048 || Learning rate: lr=5e-05.
===> Epoch[106](210/324): Loss: 0.6811 || Learning rate: lr=5e-05.
===> Epoch[106](220/324): Loss: 0.7177 || Learning rate: lr=5e-05.
===> Epoch[106](230/324): Loss: 0.9239 || Learning rate: lr=5e-05.
===> Epoch[106](240/324): Loss: 1.1825 || Learning rate: lr=5e-05.
===> Epoch[106](250/324): Loss: 0.8218 || Learning rate: lr=5e-05.
===> Epoch[106](260/324): Loss: 0.7170 || Learning rate: lr=5e-05.
===> Epoch[106](270/324): Loss: 0.7079 || Learning rate: lr=5e-05.
===> Epoch[106](280/324): Loss: 0.7771 || Learning rate: lr=5e-05.
===> Epoch[106](290/324): Loss: 0.7256 || Learning rate: lr=5e-05.
===> Epoch[106](300/324): Loss: 0.5418 || Learning rate: lr=5e-05.
===> Epoch[106](310/324): Loss: 0.6858 || Learning rate: lr=5e-05.
===> Epoch[106](320/324): Loss: 0.7695 || Learning rate: lr=5e-05.
===> Epoch[107](10/324): Loss: 0.7153 || Learning rate: lr=5e-05.
===> Epoch[107](20/324): Loss: 0.5805 || Learning rate: lr=5e-05.
===> Epoch[107](30/324): Loss: 0.5412 || Learning rate: lr=5e-05.
===> Epoch[107](40/324): Loss: 0.7671 || Learning rate: lr=5e-05.
===> Epoch[107](50/324): Loss: 0.6225 || Learning rate: lr=5e-05.
===> Epoch[107](60/324): Loss: 0.7686 || Learning rate: lr=5e-05.
===> Epoch[107](70/324): Loss: 0.8641 || Learning rate: lr=5e-05.
===> Epoch[107](80/324): Loss: 1.0370 || Learning rate: lr=5e-05.
===> Epoch[107](90/324): Loss: 0.7629 || Learning rate: lr=5e-05.
===> Epoch[107](100/324): Loss: 1.0585 || Learning rate: lr=5e-05.
===> Epoch[107](110/324): Loss: 0.7040 || Learning rate: lr=5e-05.
===> Epoch[107](120/324): Loss: 0.6019 || Learning rate: lr=5e-05.
===> Epoch[107](130/324): Loss: 0.6715 || Learning rate: lr=5e-05.
===> Epoch[107](140/324): Loss: 1.1715 || Learning rate: lr=5e-05.
===> Epoch[107](150/324): Loss: 0.4863 || Learning rate: lr=5e-05.
===> Epoch[107](160/324): Loss: 0.9714 || Learning rate: lr=5e-05.
===> Epoch[107](170/324): Loss: 0.6592 || Learning rate: lr=5e-05.
===> Epoch[107](180/324): Loss: 0.6391 || Learning rate: lr=5e-05.
===> Epoch[107](190/324): Loss: 0.8047 || Learning rate: lr=5e-05.
===> Epoch[107](200/324): Loss: 0.8789 || Learning rate: lr=5e-05.
===> Epoch[107](210/324): Loss: 0.9021 || Learning rate: lr=5e-05.
===> Epoch[107](220/324): Loss: 0.7944 || Learning rate: lr=5e-05.
===> Epoch[107](230/324): Loss: 0.7385 || Learning rate: lr=5e-05.
===> Epoch[107](240/324): Loss: 1.0758 || Learning rate: lr=5e-05.
===> Epoch[107](250/324): Loss: 0.7726 || Learning rate: lr=5e-05.
===> Epoch[107](260/324): Loss: 1.1954 || Learning rate: lr=5e-05.
===> Epoch[107](270/324): Loss: 0.7956 || Learning rate: lr=5e-05.
===> Epoch[107](280/324): Loss: 1.1636 || Learning rate: lr=5e-05.
===> Epoch[107](290/324): Loss: 0.7835 || Learning rate: lr=5e-05.
===> Epoch[107](300/324): Loss: 1.1246 || Learning rate: lr=5e-05.
===> Epoch[107](310/324): Loss: 0.9933 || Learning rate: lr=5e-05.
===> Epoch[107](320/324): Loss: 0.9648 || Learning rate: lr=5e-05.
===> Epoch[108](10/324): Loss: 0.7343 || Learning rate: lr=5e-05.
===> Epoch[108](20/324): Loss: 1.0592 || Learning rate: lr=5e-05.
===> Epoch[108](30/324): Loss: 0.7576 || Learning rate: lr=5e-05.
===> Epoch[108](40/324): Loss: 0.8766 || Learning rate: lr=5e-05.
===> Epoch[108](50/324): Loss: 1.1161 || Learning rate: lr=5e-05.
===> Epoch[108](60/324): Loss: 0.8431 || Learning rate: lr=5e-05.
===> Epoch[108](70/324): Loss: 0.5047 || Learning rate: lr=5e-05.
===> Epoch[108](80/324): Loss: 0.9281 || Learning rate: lr=5e-05.
===> Epoch[108](90/324): Loss: 0.8226 || Learning rate: lr=5e-05.
===> Epoch[108](100/324): Loss: 0.8668 || Learning rate: lr=5e-05.
===> Epoch[108](110/324): Loss: 0.7576 || Learning rate: lr=5e-05.
===> Epoch[108](120/324): Loss: 0.6771 || Learning rate: lr=5e-05.
===> Epoch[108](130/324): Loss: 0.7226 || Learning rate: lr=5e-05.
===> Epoch[108](140/324): Loss: 0.8535 || Learning rate: lr=5e-05.
===> Epoch[108](150/324): Loss: 0.6799 || Learning rate: lr=5e-05.
===> Epoch[108](160/324): Loss: 1.2255 || Learning rate: lr=5e-05.
===> Epoch[108](170/324): Loss: 0.9360 || Learning rate: lr=5e-05.
===> Epoch[108](180/324): Loss: 0.5928 || Learning rate: lr=5e-05.
===> Epoch[108](190/324): Loss: 0.5869 || Learning rate: lr=5e-05.
===> Epoch[108](200/324): Loss: 1.1797 || Learning rate: lr=5e-05.
===> Epoch[108](210/324): Loss: 0.7248 || Learning rate: lr=5e-05.
===> Epoch[108](220/324): Loss: 0.7014 || Learning rate: lr=5e-05.
===> Epoch[108](230/324): Loss: 0.7263 || Learning rate: lr=5e-05.
===> Epoch[108](240/324): Loss: 0.8983 || Learning rate: lr=5e-05.
===> Epoch[108](250/324): Loss: 0.9654 || Learning rate: lr=5e-05.
===> Epoch[108](260/324): Loss: 0.6522 || Learning rate: lr=5e-05.
===> Epoch[108](270/324): Loss: 0.5732 || Learning rate: lr=5e-05.
===> Epoch[108](280/324): Loss: 0.7721 || Learning rate: lr=5e-05.
===> Epoch[108](290/324): Loss: 0.6197 || Learning rate: lr=5e-05.
===> Epoch[108](300/324): Loss: 0.6340 || Learning rate: lr=5e-05.
===> Epoch[108](310/324): Loss: 0.6638 || Learning rate: lr=5e-05.
===> Epoch[108](320/324): Loss: 0.7734 || Learning rate: lr=5e-05.
===> Epoch[109](10/324): Loss: 0.8567 || Learning rate: lr=5e-05.
===> Epoch[109](20/324): Loss: 0.6953 || Learning rate: lr=5e-05.
===> Epoch[109](30/324): Loss: 1.1736 || Learning rate: lr=5e-05.
===> Epoch[109](40/324): Loss: 0.8709 || Learning rate: lr=5e-05.
===> Epoch[109](50/324): Loss: 0.8553 || Learning rate: lr=5e-05.
===> Epoch[109](60/324): Loss: 0.9519 || Learning rate: lr=5e-05.
===> Epoch[109](70/324): Loss: 0.8377 || Learning rate: lr=5e-05.
===> Epoch[109](80/324): Loss: 0.6757 || Learning rate: lr=5e-05.
===> Epoch[109](90/324): Loss: 0.9419 || Learning rate: lr=5e-05.
===> Epoch[109](100/324): Loss: 0.8190 || Learning rate: lr=5e-05.
===> Epoch[109](110/324): Loss: 0.8303 || Learning rate: lr=5e-05.
===> Epoch[109](120/324): Loss: 0.7009 || Learning rate: lr=5e-05.
===> Epoch[109](130/324): Loss: 0.8255 || Learning rate: lr=5e-05.
===> Epoch[109](140/324): Loss: 0.6799 || Learning rate: lr=5e-05.
===> Epoch[109](150/324): Loss: 0.8356 || Learning rate: lr=5e-05.
===> Epoch[109](160/324): Loss: 0.7638 || Learning rate: lr=5e-05.
===> Epoch[109](170/324): Loss: 0.7984 || Learning rate: lr=5e-05.
===> Epoch[109](180/324): Loss: 0.6554 || Learning rate: lr=5e-05.
===> Epoch[109](190/324): Loss: 0.6758 || Learning rate: lr=5e-05.
===> Epoch[109](200/324): Loss: 0.6515 || Learning rate: lr=5e-05.
===> Epoch[109](210/324): Loss: 0.6976 || Learning rate: lr=5e-05.
===> Epoch[109](220/324): Loss: 1.1458 || Learning rate: lr=5e-05.
===> Epoch[109](230/324): Loss: 0.6854 || Learning rate: lr=5e-05.
===> Epoch[109](240/324): Loss: 0.6266 || Learning rate: lr=5e-05.
===> Epoch[109](250/324): Loss: 0.7543 || Learning rate: lr=5e-05.
===> Epoch[109](260/324): Loss: 0.9072 || Learning rate: lr=5e-05.
===> Epoch[109](270/324): Loss: 0.7386 || Learning rate: lr=5e-05.
===> Epoch[109](280/324): Loss: 0.8907 || Learning rate: lr=5e-05.
===> Epoch[109](290/324): Loss: 0.7499 || Learning rate: lr=5e-05.
===> Epoch[109](300/324): Loss: 0.5793 || Learning rate: lr=5e-05.
===> Epoch[109](310/324): Loss: 0.8417 || Learning rate: lr=5e-05.
===> Epoch[109](320/324): Loss: 0.5117 || Learning rate: lr=5e-05.
===> Epoch[110](10/324): Loss: 0.7998 || Learning rate: lr=5e-05.
===> Epoch[110](20/324): Loss: 0.7009 || Learning rate: lr=5e-05.
===> Epoch[110](30/324): Loss: 0.7392 || Learning rate: lr=5e-05.
===> Epoch[110](40/324): Loss: 0.8499 || Learning rate: lr=5e-05.
===> Epoch[110](50/324): Loss: 0.6123 || Learning rate: lr=5e-05.
===> Epoch[110](60/324): Loss: 0.7980 || Learning rate: lr=5e-05.
===> Epoch[110](70/324): Loss: 0.6745 || Learning rate: lr=5e-05.
===> Epoch[110](80/324): Loss: 0.7310 || Learning rate: lr=5e-05.
===> Epoch[110](90/324): Loss: 0.7622 || Learning rate: lr=5e-05.
===> Epoch[110](100/324): Loss: 1.0227 || Learning rate: lr=5e-05.
===> Epoch[110](110/324): Loss: 1.1261 || Learning rate: lr=5e-05.
===> Epoch[110](120/324): Loss: 0.6759 || Learning rate: lr=5e-05.
===> Epoch[110](130/324): Loss: 1.0518 || Learning rate: lr=5e-05.
===> Epoch[110](140/324): Loss: 1.4742 || Learning rate: lr=5e-05.
===> Epoch[110](150/324): Loss: 0.7272 || Learning rate: lr=5e-05.
===> Epoch[110](160/324): Loss: 1.1212 || Learning rate: lr=5e-05.
===> Epoch[110](170/324): Loss: 1.0130 || Learning rate: lr=5e-05.
===> Epoch[110](180/324): Loss: 1.0475 || Learning rate: lr=5e-05.
===> Epoch[110](190/324): Loss: 1.0662 || Learning rate: lr=5e-05.
===> Epoch[110](200/324): Loss: 0.7111 || Learning rate: lr=5e-05.
===> Epoch[110](210/324): Loss: 0.5714 || Learning rate: lr=5e-05.
===> Epoch[110](220/324): Loss: 0.7228 || Learning rate: lr=5e-05.
===> Epoch[110](230/324): Loss: 1.0383 || Learning rate: lr=5e-05.
===> Epoch[110](240/324): Loss: 1.0436 || Learning rate: lr=5e-05.
===> Epoch[110](250/324): Loss: 0.6477 || Learning rate: lr=5e-05.
===> Epoch[110](260/324): Loss: 0.8511 || Learning rate: lr=5e-05.
===> Epoch[110](270/324): Loss: 0.9722 || Learning rate: lr=5e-05.
===> Epoch[110](280/324): Loss: 0.8291 || Learning rate: lr=5e-05.
===> Epoch[110](290/324): Loss: 0.8138 || Learning rate: lr=5e-05.
===> Epoch[110](300/324): Loss: 0.8891 || Learning rate: lr=5e-05.
===> Epoch[110](310/324): Loss: 0.4887 || Learning rate: lr=5e-05.
===> Epoch[110](320/324): Loss: 0.7223 || Learning rate: lr=5e-05.
===> Epoch[111](10/324): Loss: 0.6362 || Learning rate: lr=5e-05.
===> Epoch[111](20/324): Loss: 0.6903 || Learning rate: lr=5e-05.
===> Epoch[111](30/324): Loss: 0.7584 || Learning rate: lr=5e-05.
===> Epoch[111](40/324): Loss: 0.5947 || Learning rate: lr=5e-05.
===> Epoch[111](50/324): Loss: 0.7213 || Learning rate: lr=5e-05.
===> Epoch[111](60/324): Loss: 0.5410 || Learning rate: lr=5e-05.
===> Epoch[111](70/324): Loss: 0.9321 || Learning rate: lr=5e-05.
===> Epoch[111](80/324): Loss: 0.7587 || Learning rate: lr=5e-05.
===> Epoch[111](90/324): Loss: 0.7965 || Learning rate: lr=5e-05.
===> Epoch[111](100/324): Loss: 0.6146 || Learning rate: lr=5e-05.
===> Epoch[111](110/324): Loss: 0.9354 || Learning rate: lr=5e-05.
===> Epoch[111](120/324): Loss: 1.1005 || Learning rate: lr=5e-05.
===> Epoch[111](130/324): Loss: 0.9542 || Learning rate: lr=5e-05.
===> Epoch[111](140/324): Loss: 0.7059 || Learning rate: lr=5e-05.
===> Epoch[111](150/324): Loss: 0.5327 || Learning rate: lr=5e-05.
===> Epoch[111](160/324): Loss: 0.8682 || Learning rate: lr=5e-05.
===> Epoch[111](170/324): Loss: 0.6832 || Learning rate: lr=5e-05.
===> Epoch[111](180/324): Loss: 0.8190 || Learning rate: lr=5e-05.
===> Epoch[111](190/324): Loss: 0.4906 || Learning rate: lr=5e-05.
===> Epoch[111](200/324): Loss: 0.9189 || Learning rate: lr=5e-05.
===> Epoch[111](210/324): Loss: 0.8184 || Learning rate: lr=5e-05.
===> Epoch[111](220/324): Loss: 0.5667 || Learning rate: lr=5e-05.
===> Epoch[111](230/324): Loss: 0.9099 || Learning rate: lr=5e-05.
===> Epoch[111](240/324): Loss: 0.8941 || Learning rate: lr=5e-05.
===> Epoch[111](250/324): Loss: 0.6442 || Learning rate: lr=5e-05.
===> Epoch[111](260/324): Loss: 0.8016 || Learning rate: lr=5e-05.
===> Epoch[111](270/324): Loss: 0.8481 || Learning rate: lr=5e-05.
===> Epoch[111](280/324): Loss: 1.0411 || Learning rate: lr=5e-05.
===> Epoch[111](290/324): Loss: 0.9706 || Learning rate: lr=5e-05.
===> Epoch[111](300/324): Loss: 0.5721 || Learning rate: lr=5e-05.
===> Epoch[111](310/324): Loss: 0.9215 || Learning rate: lr=5e-05.
===> Epoch[111](320/324): Loss: 0.9475 || Learning rate: lr=5e-05.
===> Epoch[112](10/324): Loss: 0.5732 || Learning rate: lr=5e-05.
===> Epoch[112](20/324): Loss: 1.0310 || Learning rate: lr=5e-05.
===> Epoch[112](30/324): Loss: 0.7842 || Learning rate: lr=5e-05.
===> Epoch[112](40/324): Loss: 0.7250 || Learning rate: lr=5e-05.
===> Epoch[112](50/324): Loss: 0.6147 || Learning rate: lr=5e-05.
===> Epoch[112](60/324): Loss: 0.7187 || Learning rate: lr=5e-05.
===> Epoch[112](70/324): Loss: 0.5849 || Learning rate: lr=5e-05.
===> Epoch[112](80/324): Loss: 0.9934 || Learning rate: lr=5e-05.
===> Epoch[112](90/324): Loss: 0.7783 || Learning rate: lr=5e-05.
===> Epoch[112](100/324): Loss: 0.6337 || Learning rate: lr=5e-05.
===> Epoch[112](110/324): Loss: 1.1148 || Learning rate: lr=5e-05.
===> Epoch[112](120/324): Loss: 0.8088 || Learning rate: lr=5e-05.
===> Epoch[112](130/324): Loss: 0.9607 || Learning rate: lr=5e-05.
===> Epoch[112](140/324): Loss: 0.7611 || Learning rate: lr=5e-05.
===> Epoch[112](150/324): Loss: 0.9786 || Learning rate: lr=5e-05.
===> Epoch[112](160/324): Loss: 0.8888 || Learning rate: lr=5e-05.
===> Epoch[112](170/324): Loss: 0.9159 || Learning rate: lr=5e-05.
===> Epoch[112](180/324): Loss: 0.6208 || Learning rate: lr=5e-05.
===> Epoch[112](190/324): Loss: 0.9794 || Learning rate: lr=5e-05.
===> Epoch[112](200/324): Loss: 0.7385 || Learning rate: lr=5e-05.
===> Epoch[112](210/324): Loss: 0.7593 || Learning rate: lr=5e-05.
===> Epoch[112](220/324): Loss: 0.7201 || Learning rate: lr=5e-05.
===> Epoch[112](230/324): Loss: 1.2018 || Learning rate: lr=5e-05.
===> Epoch[112](240/324): Loss: 0.8647 || Learning rate: lr=5e-05.
===> Epoch[112](250/324): Loss: 0.7403 || Learning rate: lr=5e-05.
===> Epoch[112](260/324): Loss: 0.7570 || Learning rate: lr=5e-05.
===> Epoch[112](270/324): Loss: 0.7418 || Learning rate: lr=5e-05.
===> Epoch[112](280/324): Loss: 0.7245 || Learning rate: lr=5e-05.
===> Epoch[112](290/324): Loss: 0.4665 || Learning rate: lr=5e-05.
===> Epoch[112](300/324): Loss: 1.0300 || Learning rate: lr=5e-05.
===> Epoch[112](310/324): Loss: 1.0577 || Learning rate: lr=5e-05.
===> Epoch[112](320/324): Loss: 0.6477 || Learning rate: lr=5e-05.
===> Epoch[113](10/324): Loss: 0.9436 || Learning rate: lr=5e-05.
===> Epoch[113](20/324): Loss: 0.4442 || Learning rate: lr=5e-05.
===> Epoch[113](30/324): Loss: 0.6987 || Learning rate: lr=5e-05.
===> Epoch[113](40/324): Loss: 0.8092 || Learning rate: lr=5e-05.
===> Epoch[113](50/324): Loss: 0.6016 || Learning rate: lr=5e-05.
===> Epoch[113](60/324): Loss: 0.7904 || Learning rate: lr=5e-05.
===> Epoch[113](70/324): Loss: 0.8161 || Learning rate: lr=5e-05.
===> Epoch[113](80/324): Loss: 1.1629 || Learning rate: lr=5e-05.
===> Epoch[113](90/324): Loss: 0.9212 || Learning rate: lr=5e-05.
===> Epoch[113](100/324): Loss: 0.6115 || Learning rate: lr=5e-05.
===> Epoch[113](110/324): Loss: 0.6941 || Learning rate: lr=5e-05.
===> Epoch[113](120/324): Loss: 0.7983 || Learning rate: lr=5e-05.
===> Epoch[113](130/324): Loss: 0.7798 || Learning rate: lr=5e-05.
===> Epoch[113](140/324): Loss: 0.8381 || Learning rate: lr=5e-05.
===> Epoch[113](150/324): Loss: 0.8890 || Learning rate: lr=5e-05.
===> Epoch[113](160/324): Loss: 1.1430 || Learning rate: lr=5e-05.
===> Epoch[113](170/324): Loss: 0.5890 || Learning rate: lr=5e-05.
===> Epoch[113](180/324): Loss: 0.4371 || Learning rate: lr=5e-05.
===> Epoch[113](190/324): Loss: 0.9628 || Learning rate: lr=5e-05.
===> Epoch[113](200/324): Loss: 0.8340 || Learning rate: lr=5e-05.
===> Epoch[113](210/324): Loss: 0.7349 || Learning rate: lr=5e-05.
===> Epoch[113](220/324): Loss: 0.8430 || Learning rate: lr=5e-05.
===> Epoch[113](230/324): Loss: 0.6497 || Learning rate: lr=5e-05.
===> Epoch[113](240/324): Loss: 0.7406 || Learning rate: lr=5e-05.
===> Epoch[113](250/324): Loss: 0.8819 || Learning rate: lr=5e-05.
===> Epoch[113](260/324): Loss: 0.7511 || Learning rate: lr=5e-05.
===> Epoch[113](270/324): Loss: 0.8155 || Learning rate: lr=5e-05.
===> Epoch[113](280/324): Loss: 0.7440 || Learning rate: lr=5e-05.
===> Epoch[113](290/324): Loss: 0.7479 || Learning rate: lr=5e-05.
===> Epoch[113](300/324): Loss: 0.9699 || Learning rate: lr=5e-05.
===> Epoch[113](310/324): Loss: 0.7653 || Learning rate: lr=5e-05.
===> Epoch[113](320/324): Loss: 0.9902 || Learning rate: lr=5e-05.
===> Epoch[114](10/324): Loss: 0.7040 || Learning rate: lr=5e-05.
===> Epoch[114](20/324): Loss: 0.6677 || Learning rate: lr=5e-05.
===> Epoch[114](30/324): Loss: 1.1021 || Learning rate: lr=5e-05.
===> Epoch[114](40/324): Loss: 0.7701 || Learning rate: lr=5e-05.
===> Epoch[114](50/324): Loss: 0.4798 || Learning rate: lr=5e-05.
===> Epoch[114](60/324): Loss: 0.6814 || Learning rate: lr=5e-05.
===> Epoch[114](70/324): Loss: 1.0303 || Learning rate: lr=5e-05.
===> Epoch[114](80/324): Loss: 0.8163 || Learning rate: lr=5e-05.
===> Epoch[114](90/324): Loss: 0.6524 || Learning rate: lr=5e-05.
===> Epoch[114](100/324): Loss: 0.9755 || Learning rate: lr=5e-05.
===> Epoch[114](110/324): Loss: 1.0046 || Learning rate: lr=5e-05.
===> Epoch[114](120/324): Loss: 0.8452 || Learning rate: lr=5e-05.
===> Epoch[114](130/324): Loss: 0.5130 || Learning rate: lr=5e-05.
===> Epoch[114](140/324): Loss: 0.7272 || Learning rate: lr=5e-05.
===> Epoch[114](150/324): Loss: 1.0129 || Learning rate: lr=5e-05.
===> Epoch[114](160/324): Loss: 0.6909 || Learning rate: lr=5e-05.
===> Epoch[114](170/324): Loss: 0.6313 || Learning rate: lr=5e-05.
===> Epoch[114](180/324): Loss: 1.2363 || Learning rate: lr=5e-05.
===> Epoch[114](190/324): Loss: 0.7494 || Learning rate: lr=5e-05.
===> Epoch[114](200/324): Loss: 0.7188 || Learning rate: lr=5e-05.
===> Epoch[114](210/324): Loss: 0.6608 || Learning rate: lr=5e-05.
===> Epoch[114](220/324): Loss: 0.9675 || Learning rate: lr=5e-05.
===> Epoch[114](230/324): Loss: 1.0875 || Learning rate: lr=5e-05.
===> Epoch[114](240/324): Loss: 0.8297 || Learning rate: lr=5e-05.
===> Epoch[114](250/324): Loss: 0.9619 || Learning rate: lr=5e-05.
===> Epoch[114](260/324): Loss: 1.3500 || Learning rate: lr=5e-05.
===> Epoch[114](270/324): Loss: 1.2348 || Learning rate: lr=5e-05.
===> Epoch[114](280/324): Loss: 0.6926 || Learning rate: lr=5e-05.
===> Epoch[114](290/324): Loss: 0.9209 || Learning rate: lr=5e-05.
===> Epoch[114](300/324): Loss: 1.1582 || Learning rate: lr=5e-05.
===> Epoch[114](310/324): Loss: 0.9204 || Learning rate: lr=5e-05.
===> Epoch[114](320/324): Loss: 0.8528 || Learning rate: lr=5e-05.
===> Epoch[115](10/324): Loss: 0.7657 || Learning rate: lr=5e-05.
===> Epoch[115](20/324): Loss: 0.7770 || Learning rate: lr=5e-05.
===> Epoch[115](30/324): Loss: 0.9458 || Learning rate: lr=5e-05.
===> Epoch[115](40/324): Loss: 0.8460 || Learning rate: lr=5e-05.
===> Epoch[115](50/324): Loss: 0.5240 || Learning rate: lr=5e-05.
===> Epoch[115](60/324): Loss: 0.6867 || Learning rate: lr=5e-05.
===> Epoch[115](70/324): Loss: 0.9582 || Learning rate: lr=5e-05.
===> Epoch[115](80/324): Loss: 0.9075 || Learning rate: lr=5e-05.
===> Epoch[115](90/324): Loss: 0.8239 || Learning rate: lr=5e-05.
===> Epoch[115](100/324): Loss: 0.9927 || Learning rate: lr=5e-05.
===> Epoch[115](110/324): Loss: 0.5989 || Learning rate: lr=5e-05.
===> Epoch[115](120/324): Loss: 1.1229 || Learning rate: lr=5e-05.
===> Epoch[115](130/324): Loss: 0.7093 || Learning rate: lr=5e-05.
===> Epoch[115](140/324): Loss: 0.6116 || Learning rate: lr=5e-05.
===> Epoch[115](150/324): Loss: 0.8816 || Learning rate: lr=5e-05.
===> Epoch[115](160/324): Loss: 0.8828 || Learning rate: lr=5e-05.
===> Epoch[115](170/324): Loss: 0.6096 || Learning rate: lr=5e-05.
===> Epoch[115](180/324): Loss: 1.0045 || Learning rate: lr=5e-05.
===> Epoch[115](190/324): Loss: 0.6045 || Learning rate: lr=5e-05.
===> Epoch[115](200/324): Loss: 0.6293 || Learning rate: lr=5e-05.
===> Epoch[115](210/324): Loss: 0.7275 || Learning rate: lr=5e-05.
===> Epoch[115](220/324): Loss: 0.7186 || Learning rate: lr=5e-05.
===> Epoch[115](230/324): Loss: 0.5766 || Learning rate: lr=5e-05.
===> Epoch[115](240/324): Loss: 0.7265 || Learning rate: lr=5e-05.
===> Epoch[115](250/324): Loss: 0.5499 || Learning rate: lr=5e-05.
===> Epoch[115](260/324): Loss: 0.9005 || Learning rate: lr=5e-05.
===> Epoch[115](270/324): Loss: 1.3084 || Learning rate: lr=5e-05.
===> Epoch[115](280/324): Loss: 0.7317 || Learning rate: lr=5e-05.
===> Epoch[115](290/324): Loss: 1.0423 || Learning rate: lr=5e-05.
===> Epoch[115](300/324): Loss: 0.9658 || Learning rate: lr=5e-05.
===> Epoch[115](310/324): Loss: 0.6111 || Learning rate: lr=5e-05.
===> Epoch[115](320/324): Loss: 0.7677 || Learning rate: lr=5e-05.
===> Epoch[116](10/324): Loss: 0.5792 || Learning rate: lr=5e-05.
===> Epoch[116](20/324): Loss: 0.7180 || Learning rate: lr=5e-05.
===> Epoch[116](30/324): Loss: 0.7690 || Learning rate: lr=5e-05.
===> Epoch[116](40/324): Loss: 0.9879 || Learning rate: lr=5e-05.
===> Epoch[116](50/324): Loss: 1.1297 || Learning rate: lr=5e-05.
===> Epoch[116](60/324): Loss: 0.6283 || Learning rate: lr=5e-05.
===> Epoch[116](70/324): Loss: 0.5502 || Learning rate: lr=5e-05.
===> Epoch[116](80/324): Loss: 0.9168 || Learning rate: lr=5e-05.
===> Epoch[116](90/324): Loss: 0.8090 || Learning rate: lr=5e-05.
===> Epoch[116](100/324): Loss: 0.8090 || Learning rate: lr=5e-05.
===> Epoch[116](110/324): Loss: 1.4838 || Learning rate: lr=5e-05.
===> Epoch[116](120/324): Loss: 1.0716 || Learning rate: lr=5e-05.
===> Epoch[116](130/324): Loss: 1.2224 || Learning rate: lr=5e-05.
===> Epoch[116](140/324): Loss: 1.7865 || Learning rate: lr=5e-05.
===> Epoch[116](150/324): Loss: 1.3741 || Learning rate: lr=5e-05.
===> Epoch[116](160/324): Loss: 1.2257 || Learning rate: lr=5e-05.
===> Epoch[116](170/324): Loss: 0.7462 || Learning rate: lr=5e-05.
===> Epoch[116](180/324): Loss: 0.6364 || Learning rate: lr=5e-05.
===> Epoch[116](190/324): Loss: 0.7317 || Learning rate: lr=5e-05.
===> Epoch[116](200/324): Loss: 0.8073 || Learning rate: lr=5e-05.
===> Epoch[116](210/324): Loss: 1.2043 || Learning rate: lr=5e-05.
===> Epoch[116](220/324): Loss: 0.7866 || Learning rate: lr=5e-05.
===> Epoch[116](230/324): Loss: 0.8894 || Learning rate: lr=5e-05.
===> Epoch[116](240/324): Loss: 0.8028 || Learning rate: lr=5e-05.
===> Epoch[116](250/324): Loss: 0.6857 || Learning rate: lr=5e-05.
===> Epoch[116](260/324): Loss: 0.9850 || Learning rate: lr=5e-05.
===> Epoch[116](270/324): Loss: 0.7654 || Learning rate: lr=5e-05.
===> Epoch[116](280/324): Loss: 0.7758 || Learning rate: lr=5e-05.
===> Epoch[116](290/324): Loss: 0.8296 || Learning rate: lr=5e-05.
===> Epoch[116](300/324): Loss: 1.0427 || Learning rate: lr=5e-05.
===> Epoch[116](310/324): Loss: 0.8795 || Learning rate: lr=5e-05.
===> Epoch[116](320/324): Loss: 0.6003 || Learning rate: lr=5e-05.
===> Epoch[117](10/324): Loss: 1.1119 || Learning rate: lr=5e-05.
===> Epoch[117](20/324): Loss: 0.9613 || Learning rate: lr=5e-05.
===> Epoch[117](30/324): Loss: 0.6253 || Learning rate: lr=5e-05.
===> Epoch[117](40/324): Loss: 0.6468 || Learning rate: lr=5e-05.
===> Epoch[117](50/324): Loss: 0.5732 || Learning rate: lr=5e-05.
===> Epoch[117](60/324): Loss: 0.8023 || Learning rate: lr=5e-05.
===> Epoch[117](70/324): Loss: 0.9180 || Learning rate: lr=5e-05.
===> Epoch[117](80/324): Loss: 1.0282 || Learning rate: lr=5e-05.
===> Epoch[117](90/324): Loss: 1.1680 || Learning rate: lr=5e-05.
===> Epoch[117](100/324): Loss: 0.7148 || Learning rate: lr=5e-05.
===> Epoch[117](110/324): Loss: 0.9664 || Learning rate: lr=5e-05.
===> Epoch[117](120/324): Loss: 0.7468 || Learning rate: lr=5e-05.
===> Epoch[117](130/324): Loss: 0.8222 || Learning rate: lr=5e-05.
===> Epoch[117](140/324): Loss: 0.6643 || Learning rate: lr=5e-05.
===> Epoch[117](150/324): Loss: 1.0714 || Learning rate: lr=5e-05.
===> Epoch[117](160/324): Loss: 0.8160 || Learning rate: lr=5e-05.
===> Epoch[117](170/324): Loss: 0.8771 || Learning rate: lr=5e-05.
===> Epoch[117](180/324): Loss: 1.0463 || Learning rate: lr=5e-05.
===> Epoch[117](190/324): Loss: 0.7041 || Learning rate: lr=5e-05.
===> Epoch[117](200/324): Loss: 0.6927 || Learning rate: lr=5e-05.
===> Epoch[117](210/324): Loss: 0.7157 || Learning rate: lr=5e-05.
===> Epoch[117](220/324): Loss: 1.0632 || Learning rate: lr=5e-05.
===> Epoch[117](230/324): Loss: 0.8195 || Learning rate: lr=5e-05.
===> Epoch[117](240/324): Loss: 0.9320 || Learning rate: lr=5e-05.
===> Epoch[117](250/324): Loss: 0.9805 || Learning rate: lr=5e-05.
===> Epoch[117](260/324): Loss: 0.8409 || Learning rate: lr=5e-05.
===> Epoch[117](270/324): Loss: 0.8887 || Learning rate: lr=5e-05.
===> Epoch[117](280/324): Loss: 0.5750 || Learning rate: lr=5e-05.
===> Epoch[117](290/324): Loss: 0.7126 || Learning rate: lr=5e-05.
===> Epoch[117](300/324): Loss: 0.9715 || Learning rate: lr=5e-05.
===> Epoch[117](310/324): Loss: 0.5482 || Learning rate: lr=5e-05.
===> Epoch[117](320/324): Loss: 0.7137 || Learning rate: lr=5e-05.
===> Epoch[118](10/324): Loss: 0.6400 || Learning rate: lr=5e-05.
===> Epoch[118](20/324): Loss: 0.8803 || Learning rate: lr=5e-05.
===> Epoch[118](30/324): Loss: 0.9076 || Learning rate: lr=5e-05.
===> Epoch[118](40/324): Loss: 0.9780 || Learning rate: lr=5e-05.
===> Epoch[118](50/324): Loss: 0.8016 || Learning rate: lr=5e-05.
===> Epoch[118](60/324): Loss: 0.5682 || Learning rate: lr=5e-05.
===> Epoch[118](70/324): Loss: 0.7311 || Learning rate: lr=5e-05.
===> Epoch[118](80/324): Loss: 0.7477 || Learning rate: lr=5e-05.
===> Epoch[118](90/324): Loss: 0.8430 || Learning rate: lr=5e-05.
===> Epoch[118](100/324): Loss: 1.1916 || Learning rate: lr=5e-05.
===> Epoch[118](110/324): Loss: 0.6817 || Learning rate: lr=5e-05.
===> Epoch[118](120/324): Loss: 1.0510 || Learning rate: lr=5e-05.
===> Epoch[118](130/324): Loss: 0.7698 || Learning rate: lr=5e-05.
===> Epoch[118](140/324): Loss: 0.7718 || Learning rate: lr=5e-05.
===> Epoch[118](150/324): Loss: 0.5430 || Learning rate: lr=5e-05.
===> Epoch[118](160/324): Loss: 0.6040 || Learning rate: lr=5e-05.
===> Epoch[118](170/324): Loss: 0.7671 || Learning rate: lr=5e-05.
===> Epoch[118](180/324): Loss: 0.6094 || Learning rate: lr=5e-05.
===> Epoch[118](190/324): Loss: 0.7165 || Learning rate: lr=5e-05.
===> Epoch[118](200/324): Loss: 0.8422 || Learning rate: lr=5e-05.
===> Epoch[118](210/324): Loss: 0.6754 || Learning rate: lr=5e-05.
===> Epoch[118](220/324): Loss: 0.9314 || Learning rate: lr=5e-05.
===> Epoch[118](230/324): Loss: 0.8484 || Learning rate: lr=5e-05.
===> Epoch[118](240/324): Loss: 0.8749 || Learning rate: lr=5e-05.
===> Epoch[118](250/324): Loss: 0.9943 || Learning rate: lr=5e-05.
===> Epoch[118](260/324): Loss: 0.7758 || Learning rate: lr=5e-05.
===> Epoch[118](270/324): Loss: 0.6104 || Learning rate: lr=5e-05.
===> Epoch[118](280/324): Loss: 0.8029 || Learning rate: lr=5e-05.
===> Epoch[118](290/324): Loss: 0.7616 || Learning rate: lr=5e-05.
===> Epoch[118](300/324): Loss: 0.5813 || Learning rate: lr=5e-05.
===> Epoch[118](310/324): Loss: 0.8532 || Learning rate: lr=5e-05.
===> Epoch[118](320/324): Loss: 0.5541 || Learning rate: lr=5e-05.
===> Epoch[119](10/324): Loss: 0.6736 || Learning rate: lr=5e-05.
===> Epoch[119](20/324): Loss: 0.5383 || Learning rate: lr=5e-05.
===> Epoch[119](30/324): Loss: 0.5518 || Learning rate: lr=5e-05.
===> Epoch[119](40/324): Loss: 0.6690 || Learning rate: lr=5e-05.
===> Epoch[119](50/324): Loss: 1.2277 || Learning rate: lr=5e-05.
===> Epoch[119](60/324): Loss: 0.8166 || Learning rate: lr=5e-05.
===> Epoch[119](70/324): Loss: 0.8220 || Learning rate: lr=5e-05.
===> Epoch[119](80/324): Loss: 1.0557 || Learning rate: lr=5e-05.
===> Epoch[119](90/324): Loss: 0.5310 || Learning rate: lr=5e-05.
===> Epoch[119](100/324): Loss: 0.9661 || Learning rate: lr=5e-05.
===> Epoch[119](110/324): Loss: 1.0507 || Learning rate: lr=5e-05.
===> Epoch[119](120/324): Loss: 0.7345 || Learning rate: lr=5e-05.
===> Epoch[119](130/324): Loss: 0.7761 || Learning rate: lr=5e-05.
===> Epoch[119](140/324): Loss: 0.8940 || Learning rate: lr=5e-05.
===> Epoch[119](150/324): Loss: 1.1625 || Learning rate: lr=5e-05.
===> Epoch[119](160/324): Loss: 0.5482 || Learning rate: lr=5e-05.
===> Epoch[119](170/324): Loss: 0.5063 || Learning rate: lr=5e-05.
===> Epoch[119](180/324): Loss: 0.8714 || Learning rate: lr=5e-05.
===> Epoch[119](190/324): Loss: 0.7737 || Learning rate: lr=5e-05.
===> Epoch[119](200/324): Loss: 0.7731 || Learning rate: lr=5e-05.
===> Epoch[119](210/324): Loss: 0.8604 || Learning rate: lr=5e-05.
===> Epoch[119](220/324): Loss: 0.7470 || Learning rate: lr=5e-05.
===> Epoch[119](230/324): Loss: 0.8999 || Learning rate: lr=5e-05.
===> Epoch[119](240/324): Loss: 0.5810 || Learning rate: lr=5e-05.
===> Epoch[119](250/324): Loss: 0.7304 || Learning rate: lr=5e-05.
===> Epoch[119](260/324): Loss: 0.8471 || Learning rate: lr=5e-05.
===> Epoch[119](270/324): Loss: 0.7436 || Learning rate: lr=5e-05.
===> Epoch[119](280/324): Loss: 0.8566 || Learning rate: lr=5e-05.
===> Epoch[119](290/324): Loss: 1.1619 || Learning rate: lr=5e-05.
===> Epoch[119](300/324): Loss: 0.9005 || Learning rate: lr=5e-05.
===> Epoch[119](310/324): Loss: 0.5661 || Learning rate: lr=5e-05.
===> Epoch[119](320/324): Loss: 1.0183 || Learning rate: lr=5e-05.
===> Epoch[120](10/324): Loss: 0.8513 || Learning rate: lr=5e-05.
===> Epoch[120](20/324): Loss: 0.7062 || Learning rate: lr=5e-05.
===> Epoch[120](30/324): Loss: 0.7918 || Learning rate: lr=5e-05.
===> Epoch[120](40/324): Loss: 0.6111 || Learning rate: lr=5e-05.
===> Epoch[120](50/324): Loss: 1.0093 || Learning rate: lr=5e-05.
===> Epoch[120](60/324): Loss: 1.1196 || Learning rate: lr=5e-05.
===> Epoch[120](70/324): Loss: 1.0539 || Learning rate: lr=5e-05.
===> Epoch[120](80/324): Loss: 0.7658 || Learning rate: lr=5e-05.
===> Epoch[120](90/324): Loss: 1.1799 || Learning rate: lr=5e-05.
===> Epoch[120](100/324): Loss: 0.7088 || Learning rate: lr=5e-05.
===> Epoch[120](110/324): Loss: 1.0559 || Learning rate: lr=5e-05.
===> Epoch[120](120/324): Loss: 0.7572 || Learning rate: lr=5e-05.
===> Epoch[120](130/324): Loss: 0.7634 || Learning rate: lr=5e-05.
===> Epoch[120](140/324): Loss: 0.4636 || Learning rate: lr=5e-05.
===> Epoch[120](150/324): Loss: 1.0503 || Learning rate: lr=5e-05.
===> Epoch[120](160/324): Loss: 1.0635 || Learning rate: lr=5e-05.
===> Epoch[120](170/324): Loss: 0.4227 || Learning rate: lr=5e-05.
===> Epoch[120](180/324): Loss: 0.6412 || Learning rate: lr=5e-05.
===> Epoch[120](190/324): Loss: 0.4635 || Learning rate: lr=5e-05.
===> Epoch[120](200/324): Loss: 0.7106 || Learning rate: lr=5e-05.
===> Epoch[120](210/324): Loss: 0.6844 || Learning rate: lr=5e-05.
===> Epoch[120](220/324): Loss: 0.8954 || Learning rate: lr=5e-05.
===> Epoch[120](230/324): Loss: 0.8051 || Learning rate: lr=5e-05.
===> Epoch[120](240/324): Loss: 0.5416 || Learning rate: lr=5e-05.
===> Epoch[120](250/324): Loss: 0.6415 || Learning rate: lr=5e-05.
===> Epoch[120](260/324): Loss: 0.7221 || Learning rate: lr=5e-05.
===> Epoch[120](270/324): Loss: 0.9334 || Learning rate: lr=5e-05.
===> Epoch[120](280/324): Loss: 0.7407 || Learning rate: lr=5e-05.
===> Epoch[120](290/324): Loss: 1.0428 || Learning rate: lr=5e-05.
===> Epoch[120](300/324): Loss: 0.6435 || Learning rate: lr=5e-05.
===> Epoch[120](310/324): Loss: 0.6635 || Learning rate: lr=5e-05.
===> Epoch[120](320/324): Loss: 0.7548 || Learning rate: lr=5e-05.
Checkpoint saved to weights/epoch_v2_120.pth
===> Epoch[121](10/324): Loss: 0.7057 || Learning rate: lr=5e-05.
===> Epoch[121](20/324): Loss: 0.5772 || Learning rate: lr=5e-05.
===> Epoch[121](30/324): Loss: 1.0213 || Learning rate: lr=5e-05.
===> Epoch[121](40/324): Loss: 1.0751 || Learning rate: lr=5e-05.
===> Epoch[121](50/324): Loss: 0.8976 || Learning rate: lr=5e-05.
===> Epoch[121](60/324): Loss: 0.7787 || Learning rate: lr=5e-05.
===> Epoch[121](70/324): Loss: 0.6485 || Learning rate: lr=5e-05.
===> Epoch[121](80/324): Loss: 0.7552 || Learning rate: lr=5e-05.
===> Epoch[121](90/324): Loss: 0.8031 || Learning rate: lr=5e-05.
===> Epoch[121](100/324): Loss: 0.8367 || Learning rate: lr=5e-05.
===> Epoch[121](110/324): Loss: 0.5894 || Learning rate: lr=5e-05.
===> Epoch[121](120/324): Loss: 0.9151 || Learning rate: lr=5e-05.
===> Epoch[121](130/324): Loss: 0.7125 || Learning rate: lr=5e-05.
===> Epoch[121](140/324): Loss: 0.4864 || Learning rate: lr=5e-05.
===> Epoch[121](150/324): Loss: 0.5761 || Learning rate: lr=5e-05.
===> Epoch[121](160/324): Loss: 0.7717 || Learning rate: lr=5e-05.
===> Epoch[121](170/324): Loss: 0.7884 || Learning rate: lr=5e-05.
===> Epoch[121](180/324): Loss: 1.0053 || Learning rate: lr=5e-05.
===> Epoch[121](190/324): Loss: 0.6746 || Learning rate: lr=5e-05.
===> Epoch[121](200/324): Loss: 0.6491 || Learning rate: lr=5e-05.
===> Epoch[121](210/324): Loss: 0.8863 || Learning rate: lr=5e-05.
===> Epoch[121](220/324): Loss: 0.5183 || Learning rate: lr=5e-05.
===> Epoch[121](230/324): Loss: 1.1496 || Learning rate: lr=5e-05.
===> Epoch[121](240/324): Loss: 0.7726 || Learning rate: lr=5e-05.
===> Epoch[121](250/324): Loss: 0.8405 || Learning rate: lr=5e-05.
===> Epoch[121](260/324): Loss: 0.8450 || Learning rate: lr=5e-05.
===> Epoch[121](270/324): Loss: 0.8516 || Learning rate: lr=5e-05.
===> Epoch[121](280/324): Loss: 0.5258 || Learning rate: lr=5e-05.
===> Epoch[121](290/324): Loss: 0.5337 || Learning rate: lr=5e-05.
===> Epoch[121](300/324): Loss: 1.0992 || Learning rate: lr=5e-05.
===> Epoch[121](310/324): Loss: 0.8344 || Learning rate: lr=5e-05.
===> Epoch[121](320/324): Loss: 0.5500 || Learning rate: lr=5e-05.
===> Epoch[122](10/324): Loss: 0.6118 || Learning rate: lr=5e-05.
===> Epoch[122](20/324): Loss: 0.7466 || Learning rate: lr=5e-05.
===> Epoch[122](30/324): Loss: 0.8038 || Learning rate: lr=5e-05.
===> Epoch[122](40/324): Loss: 1.0107 || Learning rate: lr=5e-05.
===> Epoch[122](50/324): Loss: 0.7600 || Learning rate: lr=5e-05.
===> Epoch[122](60/324): Loss: 0.6829 || Learning rate: lr=5e-05.
===> Epoch[122](70/324): Loss: 0.8154 || Learning rate: lr=5e-05.
===> Epoch[122](80/324): Loss: 0.4824 || Learning rate: lr=5e-05.
===> Epoch[122](90/324): Loss: 0.6682 || Learning rate: lr=5e-05.
===> Epoch[122](100/324): Loss: 0.6706 || Learning rate: lr=5e-05.
===> Epoch[122](110/324): Loss: 0.6583 || Learning rate: lr=5e-05.
===> Epoch[122](120/324): Loss: 0.7805 || Learning rate: lr=5e-05.
===> Epoch[122](130/324): Loss: 0.8262 || Learning rate: lr=5e-05.
===> Epoch[122](140/324): Loss: 0.6152 || Learning rate: lr=5e-05.
===> Epoch[122](150/324): Loss: 0.8031 || Learning rate: lr=5e-05.
===> Epoch[122](160/324): Loss: 1.0304 || Learning rate: lr=5e-05.
===> Epoch[122](170/324): Loss: 0.6382 || Learning rate: lr=5e-05.
===> Epoch[122](180/324): Loss: 0.9303 || Learning rate: lr=5e-05.
===> Epoch[122](190/324): Loss: 0.7662 || Learning rate: lr=5e-05.
===> Epoch[122](200/324): Loss: 0.7553 || Learning rate: lr=5e-05.
===> Epoch[122](210/324): Loss: 0.6977 || Learning rate: lr=5e-05.
===> Epoch[122](220/324): Loss: 0.7083 || Learning rate: lr=5e-05.
===> Epoch[122](230/324): Loss: 1.4666 || Learning rate: lr=5e-05.
===> Epoch[122](240/324): Loss: 1.1071 || Learning rate: lr=5e-05.
===> Epoch[122](250/324): Loss: 1.4221 || Learning rate: lr=5e-05.
===> Epoch[122](260/324): Loss: 1.1600 || Learning rate: lr=5e-05.
===> Epoch[122](270/324): Loss: 0.6997 || Learning rate: lr=5e-05.
===> Epoch[122](280/324): Loss: 0.6319 || Learning rate: lr=5e-05.
===> Epoch[122](290/324): Loss: 0.7288 || Learning rate: lr=5e-05.
===> Epoch[122](300/324): Loss: 0.6102 || Learning rate: lr=5e-05.
===> Epoch[122](310/324): Loss: 0.9371 || Learning rate: lr=5e-05.
===> Epoch[122](320/324): Loss: 0.7967 || Learning rate: lr=5e-05.
===> Epoch[123](10/324): Loss: 0.9997 || Learning rate: lr=5e-05.
===> Epoch[123](20/324): Loss: 0.7287 || Learning rate: lr=5e-05.
===> Epoch[123](30/324): Loss: 0.8642 || Learning rate: lr=5e-05.
===> Epoch[123](40/324): Loss: 0.8438 || Learning rate: lr=5e-05.
===> Epoch[123](50/324): Loss: 0.8216 || Learning rate: lr=5e-05.
===> Epoch[123](60/324): Loss: 0.9214 || Learning rate: lr=5e-05.
===> Epoch[123](70/324): Loss: 0.9506 || Learning rate: lr=5e-05.
===> Epoch[123](80/324): Loss: 0.7031 || Learning rate: lr=5e-05.
===> Epoch[123](90/324): Loss: 0.9321 || Learning rate: lr=5e-05.
===> Epoch[123](100/324): Loss: 0.5604 || Learning rate: lr=5e-05.
===> Epoch[123](110/324): Loss: 1.1231 || Learning rate: lr=5e-05.
===> Epoch[123](120/324): Loss: 0.6877 || Learning rate: lr=5e-05.
===> Epoch[123](130/324): Loss: 0.8172 || Learning rate: lr=5e-05.
===> Epoch[123](140/324): Loss: 0.8664 || Learning rate: lr=5e-05.
===> Epoch[123](150/324): Loss: 1.0513 || Learning rate: lr=5e-05.
===> Epoch[123](160/324): Loss: 0.7494 || Learning rate: lr=5e-05.
===> Epoch[123](170/324): Loss: 0.7909 || Learning rate: lr=5e-05.
===> Epoch[123](180/324): Loss: 0.7241 || Learning rate: lr=5e-05.
===> Epoch[123](190/324): Loss: 0.5113 || Learning rate: lr=5e-05.
===> Epoch[123](200/324): Loss: 0.9867 || Learning rate: lr=5e-05.
===> Epoch[123](210/324): Loss: 0.5396 || Learning rate: lr=5e-05.
===> Epoch[123](220/324): Loss: 0.8472 || Learning rate: lr=5e-05.
===> Epoch[123](230/324): Loss: 0.7961 || Learning rate: lr=5e-05.
===> Epoch[123](240/324): Loss: 0.8184 || Learning rate: lr=5e-05.
===> Epoch[123](250/324): Loss: 0.6225 || Learning rate: lr=5e-05.
===> Epoch[123](260/324): Loss: 1.0720 || Learning rate: lr=5e-05.
===> Epoch[123](270/324): Loss: 0.8495 || Learning rate: lr=5e-05.
===> Epoch[123](280/324): Loss: 0.6920 || Learning rate: lr=5e-05.
===> Epoch[123](290/324): Loss: 0.7749 || Learning rate: lr=5e-05.
===> Epoch[123](300/324): Loss: 0.7555 || Learning rate: lr=5e-05.
===> Epoch[123](310/324): Loss: 0.6940 || Learning rate: lr=5e-05.
===> Epoch[123](320/324): Loss: 0.9619 || Learning rate: lr=5e-05.
===> Epoch[124](10/324): Loss: 1.0840 || Learning rate: lr=5e-05.
===> Epoch[124](20/324): Loss: 0.5146 || Learning rate: lr=5e-05.
===> Epoch[124](30/324): Loss: 1.0566 || Learning rate: lr=5e-05.
===> Epoch[124](40/324): Loss: 0.8215 || Learning rate: lr=5e-05.
===> Epoch[124](50/324): Loss: 0.7497 || Learning rate: lr=5e-05.
===> Epoch[124](60/324): Loss: 1.0880 || Learning rate: lr=5e-05.
===> Epoch[124](70/324): Loss: 1.2466 || Learning rate: lr=5e-05.
===> Epoch[124](80/324): Loss: 0.6829 || Learning rate: lr=5e-05.
===> Epoch[124](90/324): Loss: 1.1470 || Learning rate: lr=5e-05.
===> Epoch[124](100/324): Loss: 0.9721 || Learning rate: lr=5e-05.
===> Epoch[124](110/324): Loss: 0.7455 || Learning rate: lr=5e-05.
===> Epoch[124](120/324): Loss: 0.9848 || Learning rate: lr=5e-05.
===> Epoch[124](130/324): Loss: 0.5456 || Learning rate: lr=5e-05.
===> Epoch[124](140/324): Loss: 0.6444 || Learning rate: lr=5e-05.
===> Epoch[124](150/324): Loss: 0.7280 || Learning rate: lr=5e-05.
===> Epoch[124](160/324): Loss: 0.4913 || Learning rate: lr=5e-05.
===> Epoch[124](170/324): Loss: 0.7528 || Learning rate: lr=5e-05.
===> Epoch[124](180/324): Loss: 1.1823 || Learning rate: lr=5e-05.
===> Epoch[124](190/324): Loss: 0.9626 || Learning rate: lr=5e-05.
===> Epoch[124](200/324): Loss: 0.8429 || Learning rate: lr=5e-05.
===> Epoch[124](210/324): Loss: 0.5640 || Learning rate: lr=5e-05.
===> Epoch[124](220/324): Loss: 0.6027 || Learning rate: lr=5e-05.
===> Epoch[124](230/324): Loss: 0.4443 || Learning rate: lr=5e-05.
===> Epoch[124](240/324): Loss: 1.3932 || Learning rate: lr=5e-05.
===> Epoch[124](250/324): Loss: 0.7180 || Learning rate: lr=5e-05.
===> Epoch[124](260/324): Loss: 1.0768 || Learning rate: lr=5e-05.
===> Epoch[124](270/324): Loss: 0.7979 || Learning rate: lr=5e-05.
===> Epoch[124](280/324): Loss: 0.8867 || Learning rate: lr=5e-05.
===> Epoch[124](290/324): Loss: 0.6475 || Learning rate: lr=5e-05.
===> Epoch[124](300/324): Loss: 0.8143 || Learning rate: lr=5e-05.
===> Epoch[124](310/324): Loss: 1.0173 || Learning rate: lr=5e-05.
===> Epoch[124](320/324): Loss: 0.9619 || Learning rate: lr=5e-05.
===> Epoch[125](10/324): Loss: 0.8279 || Learning rate: lr=5e-05.
===> Epoch[125](20/324): Loss: 0.9725 || Learning rate: lr=5e-05.
===> Epoch[125](30/324): Loss: 0.5661 || Learning rate: lr=5e-05.
===> Epoch[125](40/324): Loss: 0.7324 || Learning rate: lr=5e-05.
===> Epoch[125](50/324): Loss: 0.8496 || Learning rate: lr=5e-05.
===> Epoch[125](60/324): Loss: 1.1270 || Learning rate: lr=5e-05.
===> Epoch[125](70/324): Loss: 0.6942 || Learning rate: lr=5e-05.
===> Epoch[125](80/324): Loss: 1.4718 || Learning rate: lr=5e-05.
===> Epoch[125](90/324): Loss: 1.1134 || Learning rate: lr=5e-05.
===> Epoch[125](100/324): Loss: 1.1238 || Learning rate: lr=5e-05.
===> Epoch[125](110/324): Loss: 0.8313 || Learning rate: lr=5e-05.
===> Epoch[125](120/324): Loss: 1.2472 || Learning rate: lr=5e-05.
===> Epoch[125](130/324): Loss: 0.9493 || Learning rate: lr=5e-05.
===> Epoch[125](140/324): Loss: 0.7202 || Learning rate: lr=5e-05.
===> Epoch[125](150/324): Loss: 0.6597 || Learning rate: lr=5e-05.
===> Epoch[125](160/324): Loss: 1.1805 || Learning rate: lr=5e-05.
===> Epoch[125](170/324): Loss: 0.9147 || Learning rate: lr=5e-05.
===> Epoch[125](180/324): Loss: 0.7401 || Learning rate: lr=5e-05.
===> Epoch[125](190/324): Loss: 0.6343 || Learning rate: lr=5e-05.
===> Epoch[125](200/324): Loss: 0.7868 || Learning rate: lr=5e-05.
===> Epoch[125](210/324): Loss: 0.6524 || Learning rate: lr=5e-05.
===> Epoch[125](220/324): Loss: 0.7186 || Learning rate: lr=5e-05.
===> Epoch[125](230/324): Loss: 0.5050 || Learning rate: lr=5e-05.
===> Epoch[125](240/324): Loss: 0.4909 || Learning rate: lr=5e-05.
===> Epoch[125](250/324): Loss: 0.8547 || Learning rate: lr=5e-05.
===> Epoch[125](260/324): Loss: 0.6516 || Learning rate: lr=5e-05.
===> Epoch[125](270/324): Loss: 0.5717 || Learning rate: lr=5e-05.
===> Epoch[125](280/324): Loss: 0.7928 || Learning rate: lr=5e-05.
===> Epoch[125](290/324): Loss: 0.7184 || Learning rate: lr=5e-05.
===> Epoch[125](300/324): Loss: 0.5635 || Learning rate: lr=5e-05.
===> Epoch[125](310/324): Loss: 0.5711 || Learning rate: lr=5e-05.
===> Epoch[125](320/324): Loss: 0.5899 || Learning rate: lr=5e-05.
===> Epoch[126](10/324): Loss: 0.7592 || Learning rate: lr=5e-05.
===> Epoch[126](20/324): Loss: 0.5009 || Learning rate: lr=5e-05.
===> Epoch[126](30/324): Loss: 0.7198 || Learning rate: lr=5e-05.
===> Epoch[126](40/324): Loss: 1.0499 || Learning rate: lr=5e-05.
===> Epoch[126](50/324): Loss: 0.6672 || Learning rate: lr=5e-05.
===> Epoch[126](60/324): Loss: 0.7979 || Learning rate: lr=5e-05.
===> Epoch[126](70/324): Loss: 0.4976 || Learning rate: lr=5e-05.
===> Epoch[126](80/324): Loss: 0.6442 || Learning rate: lr=5e-05.
===> Epoch[126](90/324): Loss: 0.7350 || Learning rate: lr=5e-05.
===> Epoch[126](100/324): Loss: 0.9676 || Learning rate: lr=5e-05.
===> Epoch[126](110/324): Loss: 0.7560 || Learning rate: lr=5e-05.
===> Epoch[126](120/324): Loss: 0.7562 || Learning rate: lr=5e-05.
===> Epoch[126](130/324): Loss: 0.5709 || Learning rate: lr=5e-05.
===> Epoch[126](140/324): Loss: 0.6759 || Learning rate: lr=5e-05.
===> Epoch[126](150/324): Loss: 0.9985 || Learning rate: lr=5e-05.
===> Epoch[126](160/324): Loss: 0.6269 || Learning rate: lr=5e-05.
===> Epoch[126](170/324): Loss: 0.6852 || Learning rate: lr=5e-05.
===> Epoch[126](180/324): Loss: 0.5649 || Learning rate: lr=5e-05.
===> Epoch[126](190/324): Loss: 0.6065 || Learning rate: lr=5e-05.
===> Epoch[126](200/324): Loss: 0.9059 || Learning rate: lr=5e-05.
===> Epoch[126](210/324): Loss: 0.6694 || Learning rate: lr=5e-05.
===> Epoch[126](220/324): Loss: 0.8987 || Learning rate: lr=5e-05.
===> Epoch[126](230/324): Loss: 0.6280 || Learning rate: lr=5e-05.
===> Epoch[126](240/324): Loss: 0.7644 || Learning rate: lr=5e-05.
===> Epoch[126](250/324): Loss: 0.7098 || Learning rate: lr=5e-05.
===> Epoch[126](260/324): Loss: 0.7772 || Learning rate: lr=5e-05.
===> Epoch[126](270/324): Loss: 0.7048 || Learning rate: lr=5e-05.
===> Epoch[126](280/324): Loss: 0.6867 || Learning rate: lr=5e-05.
===> Epoch[126](290/324): Loss: 0.7713 || Learning rate: lr=5e-05.
===> Epoch[126](300/324): Loss: 1.1407 || Learning rate: lr=5e-05.
===> Epoch[126](310/324): Loss: 0.9634 || Learning rate: lr=5e-05.
===> Epoch[126](320/324): Loss: 0.8811 || Learning rate: lr=5e-05.
===> Epoch[127](10/324): Loss: 0.4936 || Learning rate: lr=5e-05.
===> Epoch[127](20/324): Loss: 0.7549 || Learning rate: lr=5e-05.
===> Epoch[127](30/324): Loss: 1.1081 || Learning rate: lr=5e-05.
===> Epoch[127](40/324): Loss: 0.8281 || Learning rate: lr=5e-05.
===> Epoch[127](50/324): Loss: 0.7922 || Learning rate: lr=5e-05.
===> Epoch[127](60/324): Loss: 0.5704 || Learning rate: lr=5e-05.
===> Epoch[127](70/324): Loss: 0.9488 || Learning rate: lr=5e-05.
===> Epoch[127](80/324): Loss: 0.6270 || Learning rate: lr=5e-05.
===> Epoch[127](90/324): Loss: 0.8781 || Learning rate: lr=5e-05.
===> Epoch[127](100/324): Loss: 0.8201 || Learning rate: lr=5e-05.
===> Epoch[127](110/324): Loss: 0.7765 || Learning rate: lr=5e-05.
===> Epoch[127](120/324): Loss: 0.6635 || Learning rate: lr=5e-05.
===> Epoch[127](130/324): Loss: 0.7492 || Learning rate: lr=5e-05.
===> Epoch[127](140/324): Loss: 0.6117 || Learning rate: lr=5e-05.
===> Epoch[127](150/324): Loss: 0.7377 || Learning rate: lr=5e-05.
===> Epoch[127](160/324): Loss: 0.5974 || Learning rate: lr=5e-05.
===> Epoch[127](170/324): Loss: 0.7341 || Learning rate: lr=5e-05.
===> Epoch[127](180/324): Loss: 1.1993 || Learning rate: lr=5e-05.
===> Epoch[127](190/324): Loss: 0.7512 || Learning rate: lr=5e-05.
===> Epoch[127](200/324): Loss: 0.7273 || Learning rate: lr=5e-05.
===> Epoch[127](210/324): Loss: 0.6152 || Learning rate: lr=5e-05.
===> Epoch[127](220/324): Loss: 0.6033 || Learning rate: lr=5e-05.
===> Epoch[127](230/324): Loss: 1.2268 || Learning rate: lr=5e-05.
===> Epoch[127](240/324): Loss: 0.8342 || Learning rate: lr=5e-05.
===> Epoch[127](250/324): Loss: 0.9209 || Learning rate: lr=5e-05.
===> Epoch[127](260/324): Loss: 1.2594 || Learning rate: lr=5e-05.
===> Epoch[127](270/324): Loss: 0.8618 || Learning rate: lr=5e-05.
===> Epoch[127](280/324): Loss: 0.7627 || Learning rate: lr=5e-05.
===> Epoch[127](290/324): Loss: 0.8561 || Learning rate: lr=5e-05.
===> Epoch[127](300/324): Loss: 1.1339 || Learning rate: lr=5e-05.
===> Epoch[127](310/324): Loss: 1.0940 || Learning rate: lr=5e-05.
===> Epoch[127](320/324): Loss: 0.9292 || Learning rate: lr=5e-05.
===> Epoch[128](10/324): Loss: 0.9445 || Learning rate: lr=5e-05.
===> Epoch[128](20/324): Loss: 0.7384 || Learning rate: lr=5e-05.
===> Epoch[128](30/324): Loss: 0.5139 || Learning rate: lr=5e-05.
===> Epoch[128](40/324): Loss: 0.9190 || Learning rate: lr=5e-05.
===> Epoch[128](50/324): Loss: 0.5797 || Learning rate: lr=5e-05.
===> Epoch[128](60/324): Loss: 0.6436 || Learning rate: lr=5e-05.
===> Epoch[128](70/324): Loss: 0.6744 || Learning rate: lr=5e-05.
===> Epoch[128](80/324): Loss: 0.6756 || Learning rate: lr=5e-05.
===> Epoch[128](90/324): Loss: 1.0356 || Learning rate: lr=5e-05.
===> Epoch[128](100/324): Loss: 0.8397 || Learning rate: lr=5e-05.
===> Epoch[128](110/324): Loss: 0.9922 || Learning rate: lr=5e-05.
===> Epoch[128](120/324): Loss: 0.6428 || Learning rate: lr=5e-05.
===> Epoch[128](130/324): Loss: 0.8843 || Learning rate: lr=5e-05.
===> Epoch[128](140/324): Loss: 0.8435 || Learning rate: lr=5e-05.
===> Epoch[128](150/324): Loss: 0.5142 || Learning rate: lr=5e-05.
===> Epoch[128](160/324): Loss: 1.0923 || Learning rate: lr=5e-05.
===> Epoch[128](170/324): Loss: 0.6412 || Learning rate: lr=5e-05.
===> Epoch[128](180/324): Loss: 0.5804 || Learning rate: lr=5e-05.
===> Epoch[128](190/324): Loss: 0.9851 || Learning rate: lr=5e-05.
===> Epoch[128](200/324): Loss: 0.7896 || Learning rate: lr=5e-05.
===> Epoch[128](210/324): Loss: 1.1678 || Learning rate: lr=5e-05.
===> Epoch[128](220/324): Loss: 0.6335 || Learning rate: lr=5e-05.
===> Epoch[128](230/324): Loss: 0.8005 || Learning rate: lr=5e-05.
===> Epoch[128](240/324): Loss: 1.0442 || Learning rate: lr=5e-05.
===> Epoch[128](250/324): Loss: 0.8357 || Learning rate: lr=5e-05.
===> Epoch[128](260/324): Loss: 0.8733 || Learning rate: lr=5e-05.
===> Epoch[128](270/324): Loss: 1.0642 || Learning rate: lr=5e-05.
===> Epoch[128](280/324): Loss: 0.5822 || Learning rate: lr=5e-05.
===> Epoch[128](290/324): Loss: 0.8129 || Learning rate: lr=5e-05.
===> Epoch[128](300/324): Loss: 1.2744 || Learning rate: lr=5e-05.
===> Epoch[128](310/324): Loss: 0.5952 || Learning rate: lr=5e-05.
===> Epoch[128](320/324): Loss: 0.7376 || Learning rate: lr=5e-05.
===> Epoch[129](10/324): Loss: 0.6867 || Learning rate: lr=5e-05.
===> Epoch[129](20/324): Loss: 0.8265 || Learning rate: lr=5e-05.
===> Epoch[129](30/324): Loss: 0.7318 || Learning rate: lr=5e-05.
===> Epoch[129](40/324): Loss: 0.7993 || Learning rate: lr=5e-05.
===> Epoch[129](50/324): Loss: 0.7688 || Learning rate: lr=5e-05.
===> Epoch[129](60/324): Loss: 0.8201 || Learning rate: lr=5e-05.
===> Epoch[129](70/324): Loss: 1.0779 || Learning rate: lr=5e-05.
===> Epoch[129](80/324): Loss: 0.7292 || Learning rate: lr=5e-05.
===> Epoch[129](90/324): Loss: 0.6297 || Learning rate: lr=5e-05.
===> Epoch[129](100/324): Loss: 0.6303 || Learning rate: lr=5e-05.
===> Epoch[129](110/324): Loss: 0.7488 || Learning rate: lr=5e-05.
===> Epoch[129](120/324): Loss: 0.6506 || Learning rate: lr=5e-05.
===> Epoch[129](130/324): Loss: 0.5966 || Learning rate: lr=5e-05.
===> Epoch[129](140/324): Loss: 1.0143 || Learning rate: lr=5e-05.
===> Epoch[129](150/324): Loss: 0.4886 || Learning rate: lr=5e-05.
===> Epoch[129](160/324): Loss: 0.8584 || Learning rate: lr=5e-05.
===> Epoch[129](170/324): Loss: 1.0382 || Learning rate: lr=5e-05.
===> Epoch[129](180/324): Loss: 0.6775 || Learning rate: lr=5e-05.
===> Epoch[129](190/324): Loss: 0.9778 || Learning rate: lr=5e-05.
===> Epoch[129](200/324): Loss: 0.8623 || Learning rate: lr=5e-05.
===> Epoch[129](210/324): Loss: 1.0090 || Learning rate: lr=5e-05.
===> Epoch[129](220/324): Loss: 0.6913 || Learning rate: lr=5e-05.
===> Epoch[129](230/324): Loss: 0.7554 || Learning rate: lr=5e-05.
===> Epoch[129](240/324): Loss: 0.5794 || Learning rate: lr=5e-05.
===> Epoch[129](250/324): Loss: 0.9748 || Learning rate: lr=5e-05.
===> Epoch[129](260/324): Loss: 1.0350 || Learning rate: lr=5e-05.
===> Epoch[129](270/324): Loss: 0.8670 || Learning rate: lr=5e-05.
===> Epoch[129](280/324): Loss: 0.5951 || Learning rate: lr=5e-05.
===> Epoch[129](290/324): Loss: 1.1293 || Learning rate: lr=5e-05.
===> Epoch[129](300/324): Loss: 0.9475 || Learning rate: lr=5e-05.
===> Epoch[129](310/324): Loss: 0.7555 || Learning rate: lr=5e-05.
===> Epoch[129](320/324): Loss: 0.6015 || Learning rate: lr=5e-05.
===> Epoch[130](10/324): Loss: 0.7263 || Learning rate: lr=5e-05.
===> Epoch[130](20/324): Loss: 0.8425 || Learning rate: lr=5e-05.
===> Epoch[130](30/324): Loss: 0.8993 || Learning rate: lr=5e-05.
===> Epoch[130](40/324): Loss: 0.5099 || Learning rate: lr=5e-05.
===> Epoch[130](50/324): Loss: 0.9636 || Learning rate: lr=5e-05.
===> Epoch[130](60/324): Loss: 0.6533 || Learning rate: lr=5e-05.
===> Epoch[130](70/324): Loss: 1.0976 || Learning rate: lr=5e-05.
===> Epoch[130](80/324): Loss: 0.8505 || Learning rate: lr=5e-05.
===> Epoch[130](90/324): Loss: 0.6470 || Learning rate: lr=5e-05.
===> Epoch[130](100/324): Loss: 0.5250 || Learning rate: lr=5e-05.
===> Epoch[130](110/324): Loss: 0.6451 || Learning rate: lr=5e-05.
===> Epoch[130](120/324): Loss: 0.9550 || Learning rate: lr=5e-05.
===> Epoch[130](130/324): Loss: 0.7520 || Learning rate: lr=5e-05.
===> Epoch[130](140/324): Loss: 1.3039 || Learning rate: lr=5e-05.
===> Epoch[130](150/324): Loss: 0.5852 || Learning rate: lr=5e-05.
===> Epoch[130](160/324): Loss: 1.1464 || Learning rate: lr=5e-05.
===> Epoch[130](170/324): Loss: 0.7426 || Learning rate: lr=5e-05.
===> Epoch[130](180/324): Loss: 0.7937 || Learning rate: lr=5e-05.
===> Epoch[130](190/324): Loss: 0.6345 || Learning rate: lr=5e-05.
===> Epoch[130](200/324): Loss: 0.9486 || Learning rate: lr=5e-05.
===> Epoch[130](210/324): Loss: 0.7117 || Learning rate: lr=5e-05.
===> Epoch[130](220/324): Loss: 0.7116 || Learning rate: lr=5e-05.
===> Epoch[130](230/324): Loss: 0.7314 || Learning rate: lr=5e-05.
===> Epoch[130](240/324): Loss: 0.5920 || Learning rate: lr=5e-05.
===> Epoch[130](250/324): Loss: 0.6846 || Learning rate: lr=5e-05.
===> Epoch[130](260/324): Loss: 0.8675 || Learning rate: lr=5e-05.
===> Epoch[130](270/324): Loss: 0.8300 || Learning rate: lr=5e-05.
===> Epoch[130](280/324): Loss: 1.0332 || Learning rate: lr=5e-05.
===> Epoch[130](290/324): Loss: 0.6488 || Learning rate: lr=5e-05.
===> Epoch[130](300/324): Loss: 0.9516 || Learning rate: lr=5e-05.
===> Epoch[130](310/324): Loss: 0.8699 || Learning rate: lr=5e-05.
===> Epoch[130](320/324): Loss: 0.5952 || Learning rate: lr=5e-05.
===> Epoch[131](10/324): Loss: 0.6723 || Learning rate: lr=5e-05.
===> Epoch[131](20/324): Loss: 0.7206 || Learning rate: lr=5e-05.
===> Epoch[131](30/324): Loss: 0.7358 || Learning rate: lr=5e-05.
===> Epoch[131](40/324): Loss: 0.5500 || Learning rate: lr=5e-05.
===> Epoch[131](50/324): Loss: 0.5900 || Learning rate: lr=5e-05.
===> Epoch[131](60/324): Loss: 0.7131 || Learning rate: lr=5e-05.
===> Epoch[131](70/324): Loss: 1.0739 || Learning rate: lr=5e-05.
===> Epoch[131](80/324): Loss: 0.6049 || Learning rate: lr=5e-05.
===> Epoch[131](90/324): Loss: 1.0029 || Learning rate: lr=5e-05.
===> Epoch[131](100/324): Loss: 1.1442 || Learning rate: lr=5e-05.
===> Epoch[131](110/324): Loss: 0.6400 || Learning rate: lr=5e-05.
===> Epoch[131](120/324): Loss: 0.8840 || Learning rate: lr=5e-05.
===> Epoch[131](130/324): Loss: 0.7188 || Learning rate: lr=5e-05.
===> Epoch[131](140/324): Loss: 0.6463 || Learning rate: lr=5e-05.
===> Epoch[131](150/324): Loss: 0.9977 || Learning rate: lr=5e-05.
===> Epoch[131](160/324): Loss: 0.6546 || Learning rate: lr=5e-05.
===> Epoch[131](170/324): Loss: 0.5720 || Learning rate: lr=5e-05.
===> Epoch[131](180/324): Loss: 0.6475 || Learning rate: lr=5e-05.
===> Epoch[131](190/324): Loss: 0.7770 || Learning rate: lr=5e-05.
===> Epoch[131](200/324): Loss: 0.8060 || Learning rate: lr=5e-05.
===> Epoch[131](210/324): Loss: 0.7639 || Learning rate: lr=5e-05.
===> Epoch[131](220/324): Loss: 0.8055 || Learning rate: lr=5e-05.
===> Epoch[131](230/324): Loss: 0.7316 || Learning rate: lr=5e-05.
===> Epoch[131](240/324): Loss: 1.2183 || Learning rate: lr=5e-05.
===> Epoch[131](250/324): Loss: 0.5159 || Learning rate: lr=5e-05.
===> Epoch[131](260/324): Loss: 0.5720 || Learning rate: lr=5e-05.
===> Epoch[131](270/324): Loss: 0.7597 || Learning rate: lr=5e-05.
===> Epoch[131](280/324): Loss: 0.5601 || Learning rate: lr=5e-05.
===> Epoch[131](290/324): Loss: 1.0658 || Learning rate: lr=5e-05.
===> Epoch[131](300/324): Loss: 0.6969 || Learning rate: lr=5e-05.
===> Epoch[131](310/324): Loss: 0.4619 || Learning rate: lr=5e-05.
===> Epoch[131](320/324): Loss: 0.7992 || Learning rate: lr=5e-05.
===> Epoch[132](10/324): Loss: 0.6834 || Learning rate: lr=5e-05.
===> Epoch[132](20/324): Loss: 0.8984 || Learning rate: lr=5e-05.
===> Epoch[132](30/324): Loss: 0.7268 || Learning rate: lr=5e-05.
===> Epoch[132](40/324): Loss: 0.7922 || Learning rate: lr=5e-05.
===> Epoch[132](50/324): Loss: 0.9763 || Learning rate: lr=5e-05.
===> Epoch[132](60/324): Loss: 1.1187 || Learning rate: lr=5e-05.
===> Epoch[132](70/324): Loss: 0.9038 || Learning rate: lr=5e-05.
===> Epoch[132](80/324): Loss: 0.7608 || Learning rate: lr=5e-05.
===> Epoch[132](90/324): Loss: 0.9485 || Learning rate: lr=5e-05.
===> Epoch[132](100/324): Loss: 0.4915 || Learning rate: lr=5e-05.
===> Epoch[132](110/324): Loss: 0.6392 || Learning rate: lr=5e-05.
===> Epoch[132](120/324): Loss: 0.6138 || Learning rate: lr=5e-05.
===> Epoch[132](130/324): Loss: 0.6711 || Learning rate: lr=5e-05.
===> Epoch[132](140/324): Loss: 0.5935 || Learning rate: lr=5e-05.
===> Epoch[132](150/324): Loss: 0.6341 || Learning rate: lr=5e-05.
===> Epoch[132](160/324): Loss: 0.8710 || Learning rate: lr=5e-05.
===> Epoch[132](170/324): Loss: 0.6158 || Learning rate: lr=5e-05.
===> Epoch[132](180/324): Loss: 0.6381 || Learning rate: lr=5e-05.
===> Epoch[132](190/324): Loss: 1.1529 || Learning rate: lr=5e-05.
===> Epoch[132](200/324): Loss: 0.4198 || Learning rate: lr=5e-05.
===> Epoch[132](210/324): Loss: 0.7495 || Learning rate: lr=5e-05.
===> Epoch[132](220/324): Loss: 0.9660 || Learning rate: lr=5e-05.
===> Epoch[132](230/324): Loss: 0.6703 || Learning rate: lr=5e-05.
===> Epoch[132](240/324): Loss: 0.7147 || Learning rate: lr=5e-05.
===> Epoch[132](250/324): Loss: 0.6419 || Learning rate: lr=5e-05.
===> Epoch[132](260/324): Loss: 0.5421 || Learning rate: lr=5e-05.
===> Epoch[132](270/324): Loss: 0.9830 || Learning rate: lr=5e-05.
===> Epoch[132](280/324): Loss: 0.8231 || Learning rate: lr=5e-05.
===> Epoch[132](290/324): Loss: 0.7111 || Learning rate: lr=5e-05.
===> Epoch[132](300/324): Loss: 0.7756 || Learning rate: lr=5e-05.
===> Epoch[132](310/324): Loss: 0.6823 || Learning rate: lr=5e-05.
===> Epoch[132](320/324): Loss: 0.9782 || Learning rate: lr=5e-05.
===> Epoch[133](10/324): Loss: 1.1349 || Learning rate: lr=5e-05.
===> Epoch[133](20/324): Loss: 0.8074 || Learning rate: lr=5e-05.
===> Epoch[133](30/324): Loss: 0.8382 || Learning rate: lr=5e-05.
===> Epoch[133](40/324): Loss: 0.5591 || Learning rate: lr=5e-05.
===> Epoch[133](50/324): Loss: 0.7378 || Learning rate: lr=5e-05.
===> Epoch[133](60/324): Loss: 0.5863 || Learning rate: lr=5e-05.
===> Epoch[133](70/324): Loss: 0.8414 || Learning rate: lr=5e-05.
===> Epoch[133](80/324): Loss: 0.7155 || Learning rate: lr=5e-05.
===> Epoch[133](90/324): Loss: 0.6646 || Learning rate: lr=5e-05.
===> Epoch[133](100/324): Loss: 0.7642 || Learning rate: lr=5e-05.
===> Epoch[133](110/324): Loss: 0.6572 || Learning rate: lr=5e-05.
===> Epoch[133](120/324): Loss: 0.6459 || Learning rate: lr=5e-05.
===> Epoch[133](130/324): Loss: 0.4997 || Learning rate: lr=5e-05.
===> Epoch[133](140/324): Loss: 0.8647 || Learning rate: lr=5e-05.
===> Epoch[133](150/324): Loss: 0.6593 || Learning rate: lr=5e-05.
===> Epoch[133](160/324): Loss: 0.8249 || Learning rate: lr=5e-05.
===> Epoch[133](170/324): Loss: 0.7636 || Learning rate: lr=5e-05.
===> Epoch[133](180/324): Loss: 0.7320 || Learning rate: lr=5e-05.
===> Epoch[133](190/324): Loss: 0.9571 || Learning rate: lr=5e-05.
===> Epoch[133](200/324): Loss: 1.0354 || Learning rate: lr=5e-05.
===> Epoch[133](210/324): Loss: 0.8203 || Learning rate: lr=5e-05.
===> Epoch[133](220/324): Loss: 0.8394 || Learning rate: lr=5e-05.
===> Epoch[133](230/324): Loss: 0.5089 || Learning rate: lr=5e-05.
===> Epoch[133](240/324): Loss: 0.7493 || Learning rate: lr=5e-05.
===> Epoch[133](250/324): Loss: 0.6097 || Learning rate: lr=5e-05.
===> Epoch[133](260/324): Loss: 0.7728 || Learning rate: lr=5e-05.
===> Epoch[133](270/324): Loss: 0.6730 || Learning rate: lr=5e-05.
===> Epoch[133](280/324): Loss: 1.0183 || Learning rate: lr=5e-05.
===> Epoch[133](290/324): Loss: 0.6093 || Learning rate: lr=5e-05.
===> Epoch[133](300/324): Loss: 0.6671 || Learning rate: lr=5e-05.
===> Epoch[133](310/324): Loss: 0.6556 || Learning rate: lr=5e-05.
===> Epoch[133](320/324): Loss: 0.7057 || Learning rate: lr=5e-05.
===> Epoch[134](10/324): Loss: 0.9967 || Learning rate: lr=5e-05.
===> Epoch[134](20/324): Loss: 0.6765 || Learning rate: lr=5e-05.
===> Epoch[134](30/324): Loss: 0.9114 || Learning rate: lr=5e-05.
===> Epoch[134](40/324): Loss: 0.6616 || Learning rate: lr=5e-05.
===> Epoch[134](50/324): Loss: 0.6534 || Learning rate: lr=5e-05.
===> Epoch[134](60/324): Loss: 1.0098 || Learning rate: lr=5e-05.
===> Epoch[134](70/324): Loss: 0.6582 || Learning rate: lr=5e-05.
===> Epoch[134](80/324): Loss: 0.8166 || Learning rate: lr=5e-05.
===> Epoch[134](90/324): Loss: 0.8160 || Learning rate: lr=5e-05.
===> Epoch[134](100/324): Loss: 1.1420 || Learning rate: lr=5e-05.
===> Epoch[134](110/324): Loss: 0.6896 || Learning rate: lr=5e-05.
===> Epoch[134](120/324): Loss: 0.4195 || Learning rate: lr=5e-05.
===> Epoch[134](130/324): Loss: 0.8560 || Learning rate: lr=5e-05.
===> Epoch[134](140/324): Loss: 0.5287 || Learning rate: lr=5e-05.
===> Epoch[134](150/324): Loss: 1.0196 || Learning rate: lr=5e-05.
===> Epoch[134](160/324): Loss: 0.7652 || Learning rate: lr=5e-05.
===> Epoch[134](170/324): Loss: 0.5372 || Learning rate: lr=5e-05.
===> Epoch[134](180/324): Loss: 1.1846 || Learning rate: lr=5e-05.
===> Epoch[134](190/324): Loss: 0.7171 || Learning rate: lr=5e-05.
===> Epoch[134](200/324): Loss: 0.7502 || Learning rate: lr=5e-05.
===> Epoch[134](210/324): Loss: 0.4836 || Learning rate: lr=5e-05.
===> Epoch[134](220/324): Loss: 0.5512 || Learning rate: lr=5e-05.
===> Epoch[134](230/324): Loss: 0.7118 || Learning rate: lr=5e-05.
===> Epoch[134](240/324): Loss: 0.7623 || Learning rate: lr=5e-05.
===> Epoch[134](250/324): Loss: 0.5974 || Learning rate: lr=5e-05.
===> Epoch[134](260/324): Loss: 0.8735 || Learning rate: lr=5e-05.
===> Epoch[134](270/324): Loss: 0.7958 || Learning rate: lr=5e-05.
===> Epoch[134](280/324): Loss: 0.9119 || Learning rate: lr=5e-05.
===> Epoch[134](290/324): Loss: 1.0354 || Learning rate: lr=5e-05.
===> Epoch[134](300/324): Loss: 0.7337 || Learning rate: lr=5e-05.
===> Epoch[134](310/324): Loss: 0.6717 || Learning rate: lr=5e-05.
===> Epoch[134](320/324): Loss: 0.7376 || Learning rate: lr=5e-05.
===> Epoch[135](10/324): Loss: 1.0517 || Learning rate: lr=5e-05.
===> Epoch[135](20/324): Loss: 0.7412 || Learning rate: lr=5e-05.
===> Epoch[135](30/324): Loss: 0.9821 || Learning rate: lr=5e-05.
===> Epoch[135](40/324): Loss: 0.8491 || Learning rate: lr=5e-05.
===> Epoch[135](50/324): Loss: 0.8355 || Learning rate: lr=5e-05.
===> Epoch[135](60/324): Loss: 1.1358 || Learning rate: lr=5e-05.
===> Epoch[135](70/324): Loss: 0.8172 || Learning rate: lr=5e-05.
===> Epoch[135](80/324): Loss: 0.8103 || Learning rate: lr=5e-05.
===> Epoch[135](90/324): Loss: 0.9101 || Learning rate: lr=5e-05.
===> Epoch[135](100/324): Loss: 1.3365 || Learning rate: lr=5e-05.
===> Epoch[135](110/324): Loss: 0.8778 || Learning rate: lr=5e-05.
===> Epoch[135](120/324): Loss: 0.9233 || Learning rate: lr=5e-05.
===> Epoch[135](130/324): Loss: 1.0523 || Learning rate: lr=5e-05.
===> Epoch[135](140/324): Loss: 0.6622 || Learning rate: lr=5e-05.
===> Epoch[135](150/324): Loss: 0.8177 || Learning rate: lr=5e-05.
===> Epoch[135](160/324): Loss: 1.0976 || Learning rate: lr=5e-05.
===> Epoch[135](170/324): Loss: 0.9575 || Learning rate: lr=5e-05.
===> Epoch[135](180/324): Loss: 0.7764 || Learning rate: lr=5e-05.
===> Epoch[135](190/324): Loss: 1.0965 || Learning rate: lr=5e-05.
===> Epoch[135](200/324): Loss: 0.9481 || Learning rate: lr=5e-05.
===> Epoch[135](210/324): Loss: 1.1033 || Learning rate: lr=5e-05.
===> Epoch[135](220/324): Loss: 0.6400 || Learning rate: lr=5e-05.
===> Epoch[135](230/324): Loss: 0.5647 || Learning rate: lr=5e-05.
===> Epoch[135](240/324): Loss: 0.9893 || Learning rate: lr=5e-05.
===> Epoch[135](250/324): Loss: 0.6703 || Learning rate: lr=5e-05.
===> Epoch[135](260/324): Loss: 0.4878 || Learning rate: lr=5e-05.
===> Epoch[135](270/324): Loss: 0.7596 || Learning rate: lr=5e-05.
===> Epoch[135](280/324): Loss: 0.7616 || Learning rate: lr=5e-05.
===> Epoch[135](290/324): Loss: 0.8216 || Learning rate: lr=5e-05.
===> Epoch[135](300/324): Loss: 0.8761 || Learning rate: lr=5e-05.
===> Epoch[135](310/324): Loss: 1.1744 || Learning rate: lr=5e-05.
===> Epoch[135](320/324): Loss: 0.6994 || Learning rate: lr=5e-05.
===> Epoch[136](10/324): Loss: 0.9224 || Learning rate: lr=5e-05.
===> Epoch[136](20/324): Loss: 0.7511 || Learning rate: lr=5e-05.
===> Epoch[136](30/324): Loss: 0.8013 || Learning rate: lr=5e-05.
===> Epoch[136](40/324): Loss: 0.6434 || Learning rate: lr=5e-05.
===> Epoch[136](50/324): Loss: 0.6445 || Learning rate: lr=5e-05.
===> Epoch[136](60/324): Loss: 0.9209 || Learning rate: lr=5e-05.
===> Epoch[136](70/324): Loss: 1.1985 || Learning rate: lr=5e-05.
===> Epoch[136](80/324): Loss: 0.4921 || Learning rate: lr=5e-05.
===> Epoch[136](90/324): Loss: 0.6380 || Learning rate: lr=5e-05.
===> Epoch[136](100/324): Loss: 0.7189 || Learning rate: lr=5e-05.
===> Epoch[136](110/324): Loss: 0.8289 || Learning rate: lr=5e-05.
===> Epoch[136](120/324): Loss: 0.8841 || Learning rate: lr=5e-05.
===> Epoch[136](130/324): Loss: 0.4383 || Learning rate: lr=5e-05.
===> Epoch[136](140/324): Loss: 0.8255 || Learning rate: lr=5e-05.
===> Epoch[136](150/324): Loss: 0.9805 || Learning rate: lr=5e-05.
===> Epoch[136](160/324): Loss: 0.8307 || Learning rate: lr=5e-05.
===> Epoch[136](170/324): Loss: 0.4980 || Learning rate: lr=5e-05.
===> Epoch[136](180/324): Loss: 0.6964 || Learning rate: lr=5e-05.
===> Epoch[136](190/324): Loss: 0.7903 || Learning rate: lr=5e-05.
===> Epoch[136](200/324): Loss: 1.0755 || Learning rate: lr=5e-05.
===> Epoch[136](210/324): Loss: 0.7190 || Learning rate: lr=5e-05.
===> Epoch[136](220/324): Loss: 0.9638 || Learning rate: lr=5e-05.
===> Epoch[136](230/324): Loss: 0.8125 || Learning rate: lr=5e-05.
===> Epoch[136](240/324): Loss: 0.7064 || Learning rate: lr=5e-05.
===> Epoch[136](250/324): Loss: 0.5659 || Learning rate: lr=5e-05.
===> Epoch[136](260/324): Loss: 0.5009 || Learning rate: lr=5e-05.
===> Epoch[136](270/324): Loss: 1.0202 || Learning rate: lr=5e-05.
===> Epoch[136](280/324): Loss: 0.5933 || Learning rate: lr=5e-05.
===> Epoch[136](290/324): Loss: 0.8008 || Learning rate: lr=5e-05.
===> Epoch[136](300/324): Loss: 0.6420 || Learning rate: lr=5e-05.
===> Epoch[136](310/324): Loss: 1.0155 || Learning rate: lr=5e-05.
===> Epoch[136](320/324): Loss: 0.9966 || Learning rate: lr=5e-05.
===> Epoch[137](10/324): Loss: 1.5985 || Learning rate: lr=5e-05.
===> Epoch[137](20/324): Loss: 0.6662 || Learning rate: lr=5e-05.
===> Epoch[137](30/324): Loss: 0.8244 || Learning rate: lr=5e-05.
===> Epoch[137](40/324): Loss: 0.5893 || Learning rate: lr=5e-05.
===> Epoch[137](50/324): Loss: 0.6234 || Learning rate: lr=5e-05.
===> Epoch[137](60/324): Loss: 1.1401 || Learning rate: lr=5e-05.
===> Epoch[137](70/324): Loss: 0.6232 || Learning rate: lr=5e-05.
===> Epoch[137](80/324): Loss: 0.7231 || Learning rate: lr=5e-05.
===> Epoch[137](90/324): Loss: 0.5910 || Learning rate: lr=5e-05.
===> Epoch[137](100/324): Loss: 0.8062 || Learning rate: lr=5e-05.
===> Epoch[137](110/324): Loss: 0.9726 || Learning rate: lr=5e-05.
===> Epoch[137](120/324): Loss: 0.6545 || Learning rate: lr=5e-05.
===> Epoch[137](130/324): Loss: 0.7389 || Learning rate: lr=5e-05.
===> Epoch[137](140/324): Loss: 0.8365 || Learning rate: lr=5e-05.
===> Epoch[137](150/324): Loss: 0.6925 || Learning rate: lr=5e-05.
===> Epoch[137](160/324): Loss: 0.9774 || Learning rate: lr=5e-05.
===> Epoch[137](170/324): Loss: 0.7860 || Learning rate: lr=5e-05.
===> Epoch[137](180/324): Loss: 0.6363 || Learning rate: lr=5e-05.
===> Epoch[137](190/324): Loss: 0.6481 || Learning rate: lr=5e-05.
===> Epoch[137](200/324): Loss: 0.6374 || Learning rate: lr=5e-05.
===> Epoch[137](210/324): Loss: 0.6023 || Learning rate: lr=5e-05.
===> Epoch[137](220/324): Loss: 0.7219 || Learning rate: lr=5e-05.
===> Epoch[137](230/324): Loss: 0.8838 || Learning rate: lr=5e-05.
===> Epoch[137](240/324): Loss: 0.6169 || Learning rate: lr=5e-05.
===> Epoch[137](250/324): Loss: 0.7421 || Learning rate: lr=5e-05.
===> Epoch[137](260/324): Loss: 1.1186 || Learning rate: lr=5e-05.
===> Epoch[137](270/324): Loss: 0.8554 || Learning rate: lr=5e-05.
===> Epoch[137](280/324): Loss: 0.5893 || Learning rate: lr=5e-05.
===> Epoch[137](290/324): Loss: 0.8341 || Learning rate: lr=5e-05.
===> Epoch[137](300/324): Loss: 0.7706 || Learning rate: lr=5e-05.
===> Epoch[137](310/324): Loss: 0.7337 || Learning rate: lr=5e-05.
===> Epoch[137](320/324): Loss: 0.4963 || Learning rate: lr=5e-05.
===> Epoch[138](10/324): Loss: 0.9632 || Learning rate: lr=5e-05.
===> Epoch[138](20/324): Loss: 0.7494 || Learning rate: lr=5e-05.
===> Epoch[138](30/324): Loss: 0.7590 || Learning rate: lr=5e-05.
===> Epoch[138](40/324): Loss: 0.9024 || Learning rate: lr=5e-05.
===> Epoch[138](50/324): Loss: 0.5047 || Learning rate: lr=5e-05.
===> Epoch[138](60/324): Loss: 0.4667 || Learning rate: lr=5e-05.
===> Epoch[138](70/324): Loss: 0.9410 || Learning rate: lr=5e-05.
===> Epoch[138](80/324): Loss: 0.5169 || Learning rate: lr=5e-05.
===> Epoch[138](90/324): Loss: 0.7955 || Learning rate: lr=5e-05.
===> Epoch[138](100/324): Loss: 0.6894 || Learning rate: lr=5e-05.
===> Epoch[138](110/324): Loss: 0.3890 || Learning rate: lr=5e-05.
===> Epoch[138](120/324): Loss: 1.0469 || Learning rate: lr=5e-05.
===> Epoch[138](130/324): Loss: 1.0847 || Learning rate: lr=5e-05.
===> Epoch[138](140/324): Loss: 0.8894 || Learning rate: lr=5e-05.
===> Epoch[138](150/324): Loss: 0.5120 || Learning rate: lr=5e-05.
===> Epoch[138](160/324): Loss: 0.8278 || Learning rate: lr=5e-05.
===> Epoch[138](170/324): Loss: 0.8183 || Learning rate: lr=5e-05.
===> Epoch[138](180/324): Loss: 0.7564 || Learning rate: lr=5e-05.
===> Epoch[138](190/324): Loss: 0.8180 || Learning rate: lr=5e-05.
===> Epoch[138](200/324): Loss: 0.6853 || Learning rate: lr=5e-05.
===> Epoch[138](210/324): Loss: 0.4586 || Learning rate: lr=5e-05.
===> Epoch[138](220/324): Loss: 0.7284 || Learning rate: lr=5e-05.
===> Epoch[138](230/324): Loss: 0.7730 || Learning rate: lr=5e-05.
===> Epoch[138](240/324): Loss: 0.6575 || Learning rate: lr=5e-05.
===> Epoch[138](250/324): Loss: 0.8474 || Learning rate: lr=5e-05.
===> Epoch[138](260/324): Loss: 0.5536 || Learning rate: lr=5e-05.
===> Epoch[138](270/324): Loss: 0.6302 || Learning rate: lr=5e-05.
===> Epoch[138](280/324): Loss: 0.7566 || Learning rate: lr=5e-05.
===> Epoch[138](290/324): Loss: 0.6133 || Learning rate: lr=5e-05.
===> Epoch[138](300/324): Loss: 1.0941 || Learning rate: lr=5e-05.
===> Epoch[138](310/324): Loss: 0.6584 || Learning rate: lr=5e-05.
===> Epoch[138](320/324): Loss: 0.7945 || Learning rate: lr=5e-05.
===> Epoch[139](10/324): Loss: 0.8003 || Learning rate: lr=5e-05.
===> Epoch[139](20/324): Loss: 0.8367 || Learning rate: lr=5e-05.
===> Epoch[139](30/324): Loss: 0.7384 || Learning rate: lr=5e-05.
===> Epoch[139](40/324): Loss: 0.9623 || Learning rate: lr=5e-05.
===> Epoch[139](50/324): Loss: 0.4300 || Learning rate: lr=5e-05.
===> Epoch[139](60/324): Loss: 0.8691 || Learning rate: lr=5e-05.
===> Epoch[139](70/324): Loss: 0.9919 || Learning rate: lr=5e-05.
===> Epoch[139](80/324): Loss: 0.6109 || Learning rate: lr=5e-05.
===> Epoch[139](90/324): Loss: 0.8327 || Learning rate: lr=5e-05.
===> Epoch[139](100/324): Loss: 0.6604 || Learning rate: lr=5e-05.
===> Epoch[139](110/324): Loss: 0.8854 || Learning rate: lr=5e-05.
===> Epoch[139](120/324): Loss: 0.7164 || Learning rate: lr=5e-05.
===> Epoch[139](130/324): Loss: 1.4027 || Learning rate: lr=5e-05.
===> Epoch[139](140/324): Loss: 1.1212 || Learning rate: lr=5e-05.
===> Epoch[139](150/324): Loss: 0.9607 || Learning rate: lr=5e-05.
===> Epoch[139](160/324): Loss: 0.3468 || Learning rate: lr=5e-05.
===> Epoch[139](170/324): Loss: 1.1317 || Learning rate: lr=5e-05.
===> Epoch[139](180/324): Loss: 0.8280 || Learning rate: lr=5e-05.
===> Epoch[139](190/324): Loss: 0.7275 || Learning rate: lr=5e-05.
===> Epoch[139](200/324): Loss: 0.7378 || Learning rate: lr=5e-05.
===> Epoch[139](210/324): Loss: 0.5728 || Learning rate: lr=5e-05.
===> Epoch[139](220/324): Loss: 0.8331 || Learning rate: lr=5e-05.
===> Epoch[139](230/324): Loss: 0.7581 || Learning rate: lr=5e-05.
===> Epoch[139](240/324): Loss: 0.8478 || Learning rate: lr=5e-05.
===> Epoch[139](250/324): Loss: 0.7042 || Learning rate: lr=5e-05.
===> Epoch[139](260/324): Loss: 0.5291 || Learning rate: lr=5e-05.
===> Epoch[139](270/324): Loss: 0.9676 || Learning rate: lr=5e-05.
===> Epoch[139](280/324): Loss: 0.5428 || Learning rate: lr=5e-05.
===> Epoch[139](290/324): Loss: 0.8607 || Learning rate: lr=5e-05.
===> Epoch[139](300/324): Loss: 0.7230 || Learning rate: lr=5e-05.
===> Epoch[139](310/324): Loss: 0.5197 || Learning rate: lr=5e-05.
===> Epoch[139](320/324): Loss: 0.8538 || Learning rate: lr=5e-05.
===> Epoch[140](10/324): Loss: 0.8993 || Learning rate: lr=5e-05.
===> Epoch[140](20/324): Loss: 0.4927 || Learning rate: lr=5e-05.
===> Epoch[140](30/324): Loss: 0.6474 || Learning rate: lr=5e-05.
===> Epoch[140](40/324): Loss: 0.5027 || Learning rate: lr=5e-05.
===> Epoch[140](50/324): Loss: 0.7227 || Learning rate: lr=5e-05.
===> Epoch[140](60/324): Loss: 0.9976 || Learning rate: lr=5e-05.
===> Epoch[140](70/324): Loss: 0.7400 || Learning rate: lr=5e-05.
===> Epoch[140](80/324): Loss: 0.9254 || Learning rate: lr=5e-05.
===> Epoch[140](90/324): Loss: 0.9088 || Learning rate: lr=5e-05.
===> Epoch[140](100/324): Loss: 1.0204 || Learning rate: lr=5e-05.
===> Epoch[140](110/324): Loss: 0.7956 || Learning rate: lr=5e-05.
===> Epoch[140](120/324): Loss: 0.7089 || Learning rate: lr=5e-05.
===> Epoch[140](130/324): Loss: 0.6053 || Learning rate: lr=5e-05.
===> Epoch[140](140/324): Loss: 0.8502 || Learning rate: lr=5e-05.
===> Epoch[140](150/324): Loss: 0.7241 || Learning rate: lr=5e-05.
===> Epoch[140](160/324): Loss: 0.9370 || Learning rate: lr=5e-05.
===> Epoch[140](170/324): Loss: 0.5422 || Learning rate: lr=5e-05.
===> Epoch[140](180/324): Loss: 0.8343 || Learning rate: lr=5e-05.
===> Epoch[140](190/324): Loss: 0.6473 || Learning rate: lr=5e-05.
===> Epoch[140](200/324): Loss: 0.8459 || Learning rate: lr=5e-05.
===> Epoch[140](210/324): Loss: 0.8864 || Learning rate: lr=5e-05.
===> Epoch[140](220/324): Loss: 0.6170 || Learning rate: lr=5e-05.
===> Epoch[140](230/324): Loss: 0.8609 || Learning rate: lr=5e-05.
===> Epoch[140](240/324): Loss: 0.7330 || Learning rate: lr=5e-05.
===> Epoch[140](250/324): Loss: 0.7589 || Learning rate: lr=5e-05.
===> Epoch[140](260/324): Loss: 0.5138 || Learning rate: lr=5e-05.
===> Epoch[140](270/324): Loss: 1.0012 || Learning rate: lr=5e-05.
===> Epoch[140](280/324): Loss: 0.7576 || Learning rate: lr=5e-05.
===> Epoch[140](290/324): Loss: 0.6862 || Learning rate: lr=5e-05.
===> Epoch[140](300/324): Loss: 0.7679 || Learning rate: lr=5e-05.
===> Epoch[140](310/324): Loss: 0.7644 || Learning rate: lr=5e-05.
===> Epoch[140](320/324): Loss: 0.6910 || Learning rate: lr=5e-05.
Checkpoint saved to weights/epoch_v2_140.pth
===> Epoch[141](10/324): Loss: 0.7143 || Learning rate: lr=5e-05.
===> Epoch[141](20/324): Loss: 1.0033 || Learning rate: lr=5e-05.
===> Epoch[141](30/324): Loss: 0.7815 || Learning rate: lr=5e-05.
===> Epoch[141](40/324): Loss: 0.8328 || Learning rate: lr=5e-05.
===> Epoch[141](50/324): Loss: 0.5144 || Learning rate: lr=5e-05.
===> Epoch[141](60/324): Loss: 0.6434 || Learning rate: lr=5e-05.
===> Epoch[141](70/324): Loss: 0.6474 || Learning rate: lr=5e-05.
===> Epoch[141](80/324): Loss: 0.8024 || Learning rate: lr=5e-05.
===> Epoch[141](90/324): Loss: 0.6330 || Learning rate: lr=5e-05.
===> Epoch[141](100/324): Loss: 0.8789 || Learning rate: lr=5e-05.
===> Epoch[141](110/324): Loss: 0.5645 || Learning rate: lr=5e-05.
===> Epoch[141](120/324): Loss: 0.9451 || Learning rate: lr=5e-05.
===> Epoch[141](130/324): Loss: 0.8584 || Learning rate: lr=5e-05.
===> Epoch[141](140/324): Loss: 0.7364 || Learning rate: lr=5e-05.
===> Epoch[141](150/324): Loss: 0.6911 || Learning rate: lr=5e-05.
===> Epoch[141](160/324): Loss: 0.8222 || Learning rate: lr=5e-05.
===> Epoch[141](170/324): Loss: 0.5615 || Learning rate: lr=5e-05.
===> Epoch[141](180/324): Loss: 0.7337 || Learning rate: lr=5e-05.
===> Epoch[141](190/324): Loss: 1.2065 || Learning rate: lr=5e-05.
===> Epoch[141](200/324): Loss: 0.7650 || Learning rate: lr=5e-05.
===> Epoch[141](210/324): Loss: 0.7893 || Learning rate: lr=5e-05.
===> Epoch[141](220/324): Loss: 0.7508 || Learning rate: lr=5e-05.
===> Epoch[141](230/324): Loss: 0.7090 || Learning rate: lr=5e-05.
===> Epoch[141](240/324): Loss: 0.7813 || Learning rate: lr=5e-05.
===> Epoch[141](250/324): Loss: 0.9373 || Learning rate: lr=5e-05.
===> Epoch[141](260/324): Loss: 0.7008 || Learning rate: lr=5e-05.
===> Epoch[141](270/324): Loss: 0.6871 || Learning rate: lr=5e-05.
===> Epoch[141](280/324): Loss: 0.7401 || Learning rate: lr=5e-05.
===> Epoch[141](290/324): Loss: 0.6782 || Learning rate: lr=5e-05.
===> Epoch[141](300/324): Loss: 0.9191 || Learning rate: lr=5e-05.
===> Epoch[141](310/324): Loss: 0.9079 || Learning rate: lr=5e-05.
===> Epoch[141](320/324): Loss: 0.8602 || Learning rate: lr=5e-05.
===> Epoch[142](10/324): Loss: 1.1994 || Learning rate: lr=5e-05.
===> Epoch[142](20/324): Loss: 0.6904 || Learning rate: lr=5e-05.
===> Epoch[142](30/324): Loss: 0.7720 || Learning rate: lr=5e-05.
===> Epoch[142](40/324): Loss: 0.5703 || Learning rate: lr=5e-05.
===> Epoch[142](50/324): Loss: 0.6317 || Learning rate: lr=5e-05.
===> Epoch[142](60/324): Loss: 0.6726 || Learning rate: lr=5e-05.
===> Epoch[142](70/324): Loss: 0.9657 || Learning rate: lr=5e-05.
===> Epoch[142](80/324): Loss: 0.5321 || Learning rate: lr=5e-05.
===> Epoch[142](90/324): Loss: 0.8402 || Learning rate: lr=5e-05.
===> Epoch[142](100/324): Loss: 0.9349 || Learning rate: lr=5e-05.
===> Epoch[142](110/324): Loss: 0.8866 || Learning rate: lr=5e-05.
===> Epoch[142](120/324): Loss: 0.9075 || Learning rate: lr=5e-05.
===> Epoch[142](130/324): Loss: 1.0255 || Learning rate: lr=5e-05.
===> Epoch[142](140/324): Loss: 0.6972 || Learning rate: lr=5e-05.
===> Epoch[142](150/324): Loss: 0.8780 || Learning rate: lr=5e-05.
===> Epoch[142](160/324): Loss: 0.6256 || Learning rate: lr=5e-05.
===> Epoch[142](170/324): Loss: 0.7556 || Learning rate: lr=5e-05.
===> Epoch[142](180/324): Loss: 0.4664 || Learning rate: lr=5e-05.
===> Epoch[142](190/324): Loss: 0.6988 || Learning rate: lr=5e-05.
===> Epoch[142](200/324): Loss: 0.9802 || Learning rate: lr=5e-05.
===> Epoch[142](210/324): Loss: 0.5086 || Learning rate: lr=5e-05.
===> Epoch[142](220/324): Loss: 0.6090 || Learning rate: lr=5e-05.
===> Epoch[142](230/324): Loss: 0.8683 || Learning rate: lr=5e-05.
===> Epoch[142](240/324): Loss: 0.6695 || Learning rate: lr=5e-05.
===> Epoch[142](250/324): Loss: 0.5411 || Learning rate: lr=5e-05.
===> Epoch[142](260/324): Loss: 0.4940 || Learning rate: lr=5e-05.
===> Epoch[142](270/324): Loss: 0.9579 || Learning rate: lr=5e-05.
===> Epoch[142](280/324): Loss: 1.3024 || Learning rate: lr=5e-05.
===> Epoch[142](290/324): Loss: 0.7242 || Learning rate: lr=5e-05.
===> Epoch[142](300/324): Loss: 1.0226 || Learning rate: lr=5e-05.
===> Epoch[142](310/324): Loss: 1.2188 || Learning rate: lr=5e-05.
===> Epoch[142](320/324): Loss: 0.7698 || Learning rate: lr=5e-05.
===> Epoch[143](10/324): Loss: 0.6941 || Learning rate: lr=5e-05.
===> Epoch[143](20/324): Loss: 0.9342 || Learning rate: lr=5e-05.
===> Epoch[143](30/324): Loss: 0.6608 || Learning rate: lr=5e-05.
===> Epoch[143](40/324): Loss: 0.5935 || Learning rate: lr=5e-05.
===> Epoch[143](50/324): Loss: 0.5687 || Learning rate: lr=5e-05.
===> Epoch[143](60/324): Loss: 0.6799 || Learning rate: lr=5e-05.
===> Epoch[143](70/324): Loss: 0.7737 || Learning rate: lr=5e-05.
===> Epoch[143](80/324): Loss: 0.8305 || Learning rate: lr=5e-05.
===> Epoch[143](90/324): Loss: 0.6014 || Learning rate: lr=5e-05.
===> Epoch[143](100/324): Loss: 0.6070 || Learning rate: lr=5e-05.
===> Epoch[143](110/324): Loss: 0.7840 || Learning rate: lr=5e-05.
===> Epoch[143](120/324): Loss: 0.9611 || Learning rate: lr=5e-05.
===> Epoch[143](130/324): Loss: 0.6743 || Learning rate: lr=5e-05.
===> Epoch[143](140/324): Loss: 0.8660 || Learning rate: lr=5e-05.
===> Epoch[143](150/324): Loss: 0.8637 || Learning rate: lr=5e-05.
===> Epoch[143](160/324): Loss: 0.8137 || Learning rate: lr=5e-05.
===> Epoch[143](170/324): Loss: 0.7556 || Learning rate: lr=5e-05.
===> Epoch[143](180/324): Loss: 0.7785 || Learning rate: lr=5e-05.
===> Epoch[143](190/324): Loss: 0.8166 || Learning rate: lr=5e-05.
===> Epoch[143](200/324): Loss: 0.7405 || Learning rate: lr=5e-05.
===> Epoch[143](210/324): Loss: 0.7557 || Learning rate: lr=5e-05.
===> Epoch[143](220/324): Loss: 1.0864 || Learning rate: lr=5e-05.
===> Epoch[143](230/324): Loss: 0.5494 || Learning rate: lr=5e-05.
===> Epoch[143](240/324): Loss: 0.8034 || Learning rate: lr=5e-05.
===> Epoch[143](250/324): Loss: 0.7862 || Learning rate: lr=5e-05.
===> Epoch[143](260/324): Loss: 0.5297 || Learning rate: lr=5e-05.
===> Epoch[143](270/324): Loss: 0.7676 || Learning rate: lr=5e-05.
===> Epoch[143](280/324): Loss: 0.7085 || Learning rate: lr=5e-05.
===> Epoch[143](290/324): Loss: 0.7062 || Learning rate: lr=5e-05.
===> Epoch[143](300/324): Loss: 0.5603 || Learning rate: lr=5e-05.
===> Epoch[143](310/324): Loss: 1.0892 || Learning rate: lr=5e-05.
===> Epoch[143](320/324): Loss: 0.5734 || Learning rate: lr=5e-05.
===> Epoch[144](10/324): Loss: 0.8763 || Learning rate: lr=5e-05.
===> Epoch[144](20/324): Loss: 0.8095 || Learning rate: lr=5e-05.
===> Epoch[144](30/324): Loss: 1.1874 || Learning rate: lr=5e-05.
===> Epoch[144](40/324): Loss: 0.8593 || Learning rate: lr=5e-05.
===> Epoch[144](50/324): Loss: 0.9135 || Learning rate: lr=5e-05.
===> Epoch[144](60/324): Loss: 0.7424 || Learning rate: lr=5e-05.
===> Epoch[144](70/324): Loss: 0.7143 || Learning rate: lr=5e-05.
===> Epoch[144](80/324): Loss: 0.7356 || Learning rate: lr=5e-05.
===> Epoch[144](90/324): Loss: 0.7988 || Learning rate: lr=5e-05.
===> Epoch[144](100/324): Loss: 0.7172 || Learning rate: lr=5e-05.
===> Epoch[144](110/324): Loss: 0.9996 || Learning rate: lr=5e-05.
===> Epoch[144](120/324): Loss: 0.6534 || Learning rate: lr=5e-05.
===> Epoch[144](130/324): Loss: 0.8975 || Learning rate: lr=5e-05.
===> Epoch[144](140/324): Loss: 0.6187 || Learning rate: lr=5e-05.
===> Epoch[144](150/324): Loss: 0.6898 || Learning rate: lr=5e-05.
===> Epoch[144](160/324): Loss: 0.6649 || Learning rate: lr=5e-05.
===> Epoch[144](170/324): Loss: 0.6013 || Learning rate: lr=5e-05.
===> Epoch[144](180/324): Loss: 0.9328 || Learning rate: lr=5e-05.
===> Epoch[144](190/324): Loss: 0.6425 || Learning rate: lr=5e-05.
===> Epoch[144](200/324): Loss: 1.1719 || Learning rate: lr=5e-05.
===> Epoch[144](210/324): Loss: 0.7416 || Learning rate: lr=5e-05.
===> Epoch[144](220/324): Loss: 0.9321 || Learning rate: lr=5e-05.
===> Epoch[144](230/324): Loss: 0.7087 || Learning rate: lr=5e-05.
===> Epoch[144](240/324): Loss: 0.8925 || Learning rate: lr=5e-05.
===> Epoch[144](250/324): Loss: 0.6496 || Learning rate: lr=5e-05.
===> Epoch[144](260/324): Loss: 1.0766 || Learning rate: lr=5e-05.
===> Epoch[144](270/324): Loss: 0.9309 || Learning rate: lr=5e-05.
===> Epoch[144](280/324): Loss: 0.6914 || Learning rate: lr=5e-05.
===> Epoch[144](290/324): Loss: 0.6711 || Learning rate: lr=5e-05.
===> Epoch[144](300/324): Loss: 0.7416 || Learning rate: lr=5e-05.
===> Epoch[144](310/324): Loss: 0.7676 || Learning rate: lr=5e-05.
===> Epoch[144](320/324): Loss: 0.5999 || Learning rate: lr=5e-05.
===> Epoch[145](10/324): Loss: 0.7059 || Learning rate: lr=5e-05.
===> Epoch[145](20/324): Loss: 0.6046 || Learning rate: lr=5e-05.
===> Epoch[145](30/324): Loss: 0.6763 || Learning rate: lr=5e-05.
===> Epoch[145](40/324): Loss: 0.8679 || Learning rate: lr=5e-05.
===> Epoch[145](50/324): Loss: 0.6790 || Learning rate: lr=5e-05.
===> Epoch[145](60/324): Loss: 0.7596 || Learning rate: lr=5e-05.
===> Epoch[145](70/324): Loss: 0.6760 || Learning rate: lr=5e-05.
===> Epoch[145](80/324): Loss: 0.7507 || Learning rate: lr=5e-05.
===> Epoch[145](90/324): Loss: 0.6874 || Learning rate: lr=5e-05.
===> Epoch[145](100/324): Loss: 0.9138 || Learning rate: lr=5e-05.
===> Epoch[145](110/324): Loss: 0.6888 || Learning rate: lr=5e-05.
===> Epoch[145](120/324): Loss: 0.5718 || Learning rate: lr=5e-05.
===> Epoch[145](130/324): Loss: 0.8487 || Learning rate: lr=5e-05.
===> Epoch[145](140/324): Loss: 0.9047 || Learning rate: lr=5e-05.
===> Epoch[145](150/324): Loss: 1.0777 || Learning rate: lr=5e-05.
===> Epoch[145](160/324): Loss: 0.6598 || Learning rate: lr=5e-05.
===> Epoch[145](170/324): Loss: 0.7094 || Learning rate: lr=5e-05.
===> Epoch[145](180/324): Loss: 0.4469 || Learning rate: lr=5e-05.
===> Epoch[145](190/324): Loss: 1.0418 || Learning rate: lr=5e-05.
===> Epoch[145](200/324): Loss: 0.7936 || Learning rate: lr=5e-05.
===> Epoch[145](210/324): Loss: 0.7780 || Learning rate: lr=5e-05.
===> Epoch[145](220/324): Loss: 1.0098 || Learning rate: lr=5e-05.
===> Epoch[145](230/324): Loss: 0.9032 || Learning rate: lr=5e-05.
===> Epoch[145](240/324): Loss: 1.0361 || Learning rate: lr=5e-05.
===> Epoch[145](250/324): Loss: 1.4201 || Learning rate: lr=5e-05.
===> Epoch[145](260/324): Loss: 1.4410 || Learning rate: lr=5e-05.
===> Epoch[145](270/324): Loss: 0.7680 || Learning rate: lr=5e-05.
===> Epoch[145](280/324): Loss: 0.8551 || Learning rate: lr=5e-05.
===> Epoch[145](290/324): Loss: 0.8176 || Learning rate: lr=5e-05.
===> Epoch[145](300/324): Loss: 0.7290 || Learning rate: lr=5e-05.
===> Epoch[145](310/324): Loss: 0.7281 || Learning rate: lr=5e-05.
===> Epoch[145](320/324): Loss: 0.7905 || Learning rate: lr=5e-05.
===> Epoch[146](10/324): Loss: 0.8036 || Learning rate: lr=5e-05.
===> Epoch[146](20/324): Loss: 0.6686 || Learning rate: lr=5e-05.
===> Epoch[146](30/324): Loss: 0.5807 || Learning rate: lr=5e-05.
===> Epoch[146](40/324): Loss: 0.7595 || Learning rate: lr=5e-05.
===> Epoch[146](50/324): Loss: 0.6198 || Learning rate: lr=5e-05.
===> Epoch[146](60/324): Loss: 0.6711 || Learning rate: lr=5e-05.
===> Epoch[146](70/324): Loss: 0.7429 || Learning rate: lr=5e-05.
===> Epoch[146](80/324): Loss: 0.5777 || Learning rate: lr=5e-05.
===> Epoch[146](90/324): Loss: 0.5862 || Learning rate: lr=5e-05.
===> Epoch[146](100/324): Loss: 0.6529 || Learning rate: lr=5e-05.
===> Epoch[146](110/324): Loss: 0.7870 || Learning rate: lr=5e-05.
===> Epoch[146](120/324): Loss: 0.8350 || Learning rate: lr=5e-05.
===> Epoch[146](130/324): Loss: 0.8482 || Learning rate: lr=5e-05.
===> Epoch[146](140/324): Loss: 0.9619 || Learning rate: lr=5e-05.
===> Epoch[146](150/324): Loss: 0.9753 || Learning rate: lr=5e-05.
===> Epoch[146](160/324): Loss: 0.7478 || Learning rate: lr=5e-05.
===> Epoch[146](170/324): Loss: 0.6818 || Learning rate: lr=5e-05.
===> Epoch[146](180/324): Loss: 0.7931 || Learning rate: lr=5e-05.
===> Epoch[146](190/324): Loss: 0.7131 || Learning rate: lr=5e-05.
===> Epoch[146](200/324): Loss: 0.6759 || Learning rate: lr=5e-05.
===> Epoch[146](210/324): Loss: 0.5130 || Learning rate: lr=5e-05.
===> Epoch[146](220/324): Loss: 0.9675 || Learning rate: lr=5e-05.
===> Epoch[146](230/324): Loss: 0.6942 || Learning rate: lr=5e-05.
===> Epoch[146](240/324): Loss: 0.6291 || Learning rate: lr=5e-05.
===> Epoch[146](250/324): Loss: 0.8371 || Learning rate: lr=5e-05.
===> Epoch[146](260/324): Loss: 0.6749 || Learning rate: lr=5e-05.
===> Epoch[146](270/324): Loss: 1.0669 || Learning rate: lr=5e-05.
===> Epoch[146](280/324): Loss: 0.8746 || Learning rate: lr=5e-05.
===> Epoch[146](290/324): Loss: 0.8363 || Learning rate: lr=5e-05.
===> Epoch[146](300/324): Loss: 0.5482 || Learning rate: lr=5e-05.
===> Epoch[146](310/324): Loss: 0.8284 || Learning rate: lr=5e-05.
===> Epoch[146](320/324): Loss: 0.5787 || Learning rate: lr=5e-05.
===> Epoch[147](10/324): Loss: 0.6448 || Learning rate: lr=5e-05.
===> Epoch[147](20/324): Loss: 0.7715 || Learning rate: lr=5e-05.
===> Epoch[147](30/324): Loss: 0.6358 || Learning rate: lr=5e-05.
===> Epoch[147](40/324): Loss: 0.7546 || Learning rate: lr=5e-05.
===> Epoch[147](50/324): Loss: 1.0503 || Learning rate: lr=5e-05.
===> Epoch[147](60/324): Loss: 0.8871 || Learning rate: lr=5e-05.
===> Epoch[147](70/324): Loss: 0.7407 || Learning rate: lr=5e-05.
===> Epoch[147](80/324): Loss: 0.7228 || Learning rate: lr=5e-05.
===> Epoch[147](90/324): Loss: 0.6831 || Learning rate: lr=5e-05.
===> Epoch[147](100/324): Loss: 0.7177 || Learning rate: lr=5e-05.
===> Epoch[147](110/324): Loss: 0.5091 || Learning rate: lr=5e-05.
===> Epoch[147](120/324): Loss: 0.7229 || Learning rate: lr=5e-05.
===> Epoch[147](130/324): Loss: 1.0430 || Learning rate: lr=5e-05.
===> Epoch[147](140/324): Loss: 0.9283 || Learning rate: lr=5e-05.
===> Epoch[147](150/324): Loss: 0.7313 || Learning rate: lr=5e-05.
===> Epoch[147](160/324): Loss: 0.7188 || Learning rate: lr=5e-05.
===> Epoch[147](170/324): Loss: 0.7433 || Learning rate: lr=5e-05.
===> Epoch[147](180/324): Loss: 0.7686 || Learning rate: lr=5e-05.
===> Epoch[147](190/324): Loss: 0.5989 || Learning rate: lr=5e-05.
===> Epoch[147](200/324): Loss: 0.7040 || Learning rate: lr=5e-05.
===> Epoch[147](210/324): Loss: 0.4874 || Learning rate: lr=5e-05.
===> Epoch[147](220/324): Loss: 0.6058 || Learning rate: lr=5e-05.
===> Epoch[147](230/324): Loss: 0.8447 || Learning rate: lr=5e-05.
===> Epoch[147](240/324): Loss: 1.0922 || Learning rate: lr=5e-05.
===> Epoch[147](250/324): Loss: 0.7668 || Learning rate: lr=5e-05.
===> Epoch[147](260/324): Loss: 0.7239 || Learning rate: lr=5e-05.
===> Epoch[147](270/324): Loss: 0.6868 || Learning rate: lr=5e-05.
===> Epoch[147](280/324): Loss: 0.9203 || Learning rate: lr=5e-05.
===> Epoch[147](290/324): Loss: 0.8385 || Learning rate: lr=5e-05.
===> Epoch[147](300/324): Loss: 0.7615 || Learning rate: lr=5e-05.
===> Epoch[147](310/324): Loss: 0.8561 || Learning rate: lr=5e-05.
===> Epoch[147](320/324): Loss: 0.8776 || Learning rate: lr=5e-05.
===> Epoch[148](10/324): Loss: 0.8509 || Learning rate: lr=5e-05.
===> Epoch[148](20/324): Loss: 0.6681 || Learning rate: lr=5e-05.
===> Epoch[148](30/324): Loss: 0.7096 || Learning rate: lr=5e-05.
===> Epoch[148](40/324): Loss: 1.0258 || Learning rate: lr=5e-05.
===> Epoch[148](50/324): Loss: 0.9011 || Learning rate: lr=5e-05.
===> Epoch[148](60/324): Loss: 0.8133 || Learning rate: lr=5e-05.
===> Epoch[148](70/324): Loss: 1.0257 || Learning rate: lr=5e-05.
===> Epoch[148](80/324): Loss: 1.0012 || Learning rate: lr=5e-05.
===> Epoch[148](90/324): Loss: 0.7830 || Learning rate: lr=5e-05.
===> Epoch[148](100/324): Loss: 0.5518 || Learning rate: lr=5e-05.
===> Epoch[148](110/324): Loss: 0.5938 || Learning rate: lr=5e-05.
===> Epoch[148](120/324): Loss: 1.1937 || Learning rate: lr=5e-05.
===> Epoch[148](130/324): Loss: 0.7125 || Learning rate: lr=5e-05.
===> Epoch[148](140/324): Loss: 0.7025 || Learning rate: lr=5e-05.
===> Epoch[148](150/324): Loss: 0.7535 || Learning rate: lr=5e-05.
===> Epoch[148](160/324): Loss: 0.8546 || Learning rate: lr=5e-05.
===> Epoch[148](170/324): Loss: 0.6189 || Learning rate: lr=5e-05.
===> Epoch[148](180/324): Loss: 0.6612 || Learning rate: lr=5e-05.
===> Epoch[148](190/324): Loss: 0.8917 || Learning rate: lr=5e-05.
===> Epoch[148](200/324): Loss: 0.6692 || Learning rate: lr=5e-05.
===> Epoch[148](210/324): Loss: 0.5633 || Learning rate: lr=5e-05.
===> Epoch[148](220/324): Loss: 0.9731 || Learning rate: lr=5e-05.
===> Epoch[148](230/324): Loss: 0.6906 || Learning rate: lr=5e-05.
===> Epoch[148](240/324): Loss: 0.7311 || Learning rate: lr=5e-05.
===> Epoch[148](250/324): Loss: 0.7552 || Learning rate: lr=5e-05.
===> Epoch[148](260/324): Loss: 0.6251 || Learning rate: lr=5e-05.
===> Epoch[148](270/324): Loss: 0.7956 || Learning rate: lr=5e-05.
===> Epoch[148](280/324): Loss: 0.6344 || Learning rate: lr=5e-05.
===> Epoch[148](290/324): Loss: 0.7618 || Learning rate: lr=5e-05.
===> Epoch[148](300/324): Loss: 0.9875 || Learning rate: lr=5e-05.
===> Epoch[148](310/324): Loss: 0.6875 || Learning rate: lr=5e-05.
===> Epoch[148](320/324): Loss: 0.5891 || Learning rate: lr=5e-05.
===> Epoch[149](10/324): Loss: 0.5064 || Learning rate: lr=5e-05.
===> Epoch[149](20/324): Loss: 1.0463 || Learning rate: lr=5e-05.
===> Epoch[149](30/324): Loss: 0.6108 || Learning rate: lr=5e-05.
===> Epoch[149](40/324): Loss: 0.9532 || Learning rate: lr=5e-05.
===> Epoch[149](50/324): Loss: 0.7931 || Learning rate: lr=5e-05.
===> Epoch[149](60/324): Loss: 0.9479 || Learning rate: lr=5e-05.
===> Epoch[149](70/324): Loss: 0.5431 || Learning rate: lr=5e-05.
===> Epoch[149](80/324): Loss: 1.1099 || Learning rate: lr=5e-05.
===> Epoch[149](90/324): Loss: 0.7216 || Learning rate: lr=5e-05.
===> Epoch[149](100/324): Loss: 0.6688 || Learning rate: lr=5e-05.
===> Epoch[149](110/324): Loss: 0.5449 || Learning rate: lr=5e-05.
===> Epoch[149](120/324): Loss: 0.8231 || Learning rate: lr=5e-05.
===> Epoch[149](130/324): Loss: 0.9479 || Learning rate: lr=5e-05.
===> Epoch[149](140/324): Loss: 0.6355 || Learning rate: lr=5e-05.
===> Epoch[149](150/324): Loss: 0.7060 || Learning rate: lr=5e-05.
===> Epoch[149](160/324): Loss: 0.9638 || Learning rate: lr=5e-05.
===> Epoch[149](170/324): Loss: 0.7920 || Learning rate: lr=5e-05.
===> Epoch[149](180/324): Loss: 0.8218 || Learning rate: lr=5e-05.
===> Epoch[149](190/324): Loss: 0.9736 || Learning rate: lr=5e-05.
===> Epoch[149](200/324): Loss: 0.7368 || Learning rate: lr=5e-05.
===> Epoch[149](210/324): Loss: 0.7236 || Learning rate: lr=5e-05.
===> Epoch[149](220/324): Loss: 0.9063 || Learning rate: lr=5e-05.
===> Epoch[149](230/324): Loss: 0.6632 || Learning rate: lr=5e-05.
===> Epoch[149](240/324): Loss: 0.7405 || Learning rate: lr=5e-05.
===> Epoch[149](250/324): Loss: 0.4897 || Learning rate: lr=5e-05.
===> Epoch[149](260/324): Loss: 0.6151 || Learning rate: lr=5e-05.
===> Epoch[149](270/324): Loss: 0.7913 || Learning rate: lr=5e-05.
===> Epoch[149](280/324): Loss: 0.7073 || Learning rate: lr=5e-05.
===> Epoch[149](290/324): Loss: 0.6405 || Learning rate: lr=5e-05.
===> Epoch[149](300/324): Loss: 0.4965 || Learning rate: lr=5e-05.
===> Epoch[149](310/324): Loss: 0.7802 || Learning rate: lr=5e-05.
===> Epoch[149](320/324): Loss: 0.7459 || Learning rate: lr=5e-05.
===> Epoch[150](10/324): Loss: 0.9289 || Learning rate: lr=5e-05.
===> Epoch[150](20/324): Loss: 0.8243 || Learning rate: lr=5e-05.
===> Epoch[150](30/324): Loss: 0.9235 || Learning rate: lr=5e-05.
===> Epoch[150](40/324): Loss: 0.7771 || Learning rate: lr=5e-05.
===> Epoch[150](50/324): Loss: 0.7413 || Learning rate: lr=5e-05.
===> Epoch[150](60/324): Loss: 0.4825 || Learning rate: lr=5e-05.
===> Epoch[150](70/324): Loss: 0.8438 || Learning rate: lr=5e-05.
===> Epoch[150](80/324): Loss: 0.8373 || Learning rate: lr=5e-05.
===> Epoch[150](90/324): Loss: 0.7277 || Learning rate: lr=5e-05.
===> Epoch[150](100/324): Loss: 0.7874 || Learning rate: lr=5e-05.
===> Epoch[150](110/324): Loss: 0.5292 || Learning rate: lr=5e-05.
===> Epoch[150](120/324): Loss: 0.9951 || Learning rate: lr=5e-05.
===> Epoch[150](130/324): Loss: 0.5782 || Learning rate: lr=5e-05.
===> Epoch[150](140/324): Loss: 0.6030 || Learning rate: lr=5e-05.
===> Epoch[150](150/324): Loss: 0.5836 || Learning rate: lr=5e-05.
===> Epoch[150](160/324): Loss: 0.9459 || Learning rate: lr=5e-05.
===> Epoch[150](170/324): Loss: 0.6687 || Learning rate: lr=5e-05.
===> Epoch[150](180/324): Loss: 0.7379 || Learning rate: lr=5e-05.
===> Epoch[150](190/324): Loss: 0.6608 || Learning rate: lr=5e-05.
===> Epoch[150](200/324): Loss: 0.9423 || Learning rate: lr=5e-05.
===> Epoch[150](210/324): Loss: 0.6936 || Learning rate: lr=5e-05.
===> Epoch[150](220/324): Loss: 0.9597 || Learning rate: lr=5e-05.
===> Epoch[150](230/324): Loss: 1.0358 || Learning rate: lr=5e-05.
===> Epoch[150](240/324): Loss: 0.8943 || Learning rate: lr=5e-05.
===> Epoch[150](250/324): Loss: 0.9973 || Learning rate: lr=5e-05.
===> Epoch[150](260/324): Loss: 1.1129 || Learning rate: lr=5e-05.
===> Epoch[150](270/324): Loss: 0.6504 || Learning rate: lr=5e-05.
===> Epoch[150](280/324): Loss: 0.7670 || Learning rate: lr=5e-05.
===> Epoch[150](290/324): Loss: 0.9603 || Learning rate: lr=5e-05.
===> Epoch[150](300/324): Loss: 0.5273 || Learning rate: lr=5e-05.
===> Epoch[150](310/324): Loss: 0.8872 || Learning rate: lr=5e-05.
===> Epoch[150](320/324): Loss: 0.5680 || Learning rate: lr=5e-05.
===> Epoch[151](10/324): Loss: 0.9668 || Learning rate: lr=5e-05.
===> Epoch[151](20/324): Loss: 0.8372 || Learning rate: lr=5e-05.
===> Epoch[151](30/324): Loss: 0.9391 || Learning rate: lr=5e-05.
===> Epoch[151](40/324): Loss: 0.9039 || Learning rate: lr=5e-05.
===> Epoch[151](50/324): Loss: 0.9302 || Learning rate: lr=5e-05.
===> Epoch[151](60/324): Loss: 0.8369 || Learning rate: lr=5e-05.
===> Epoch[151](70/324): Loss: 1.3575 || Learning rate: lr=5e-05.
===> Epoch[151](80/324): Loss: 0.7498 || Learning rate: lr=5e-05.
===> Epoch[151](90/324): Loss: 0.9568 || Learning rate: lr=5e-05.
===> Epoch[151](100/324): Loss: 0.9460 || Learning rate: lr=5e-05.
===> Epoch[151](110/324): Loss: 0.9690 || Learning rate: lr=5e-05.
===> Epoch[151](120/324): Loss: 0.8554 || Learning rate: lr=5e-05.
===> Epoch[151](130/324): Loss: 0.6125 || Learning rate: lr=5e-05.
===> Epoch[151](140/324): Loss: 1.1380 || Learning rate: lr=5e-05.
===> Epoch[151](150/324): Loss: 0.5279 || Learning rate: lr=5e-05.
===> Epoch[151](160/324): Loss: 0.4901 || Learning rate: lr=5e-05.
===> Epoch[151](170/324): Loss: 0.7969 || Learning rate: lr=5e-05.
===> Epoch[151](180/324): Loss: 0.9485 || Learning rate: lr=5e-05.
===> Epoch[151](190/324): Loss: 0.9019 || Learning rate: lr=5e-05.
===> Epoch[151](200/324): Loss: 0.7325 || Learning rate: lr=5e-05.
===> Epoch[151](210/324): Loss: 0.9331 || Learning rate: lr=5e-05.
===> Epoch[151](220/324): Loss: 0.8921 || Learning rate: lr=5e-05.
===> Epoch[151](230/324): Loss: 0.7282 || Learning rate: lr=5e-05.
===> Epoch[151](240/324): Loss: 0.8255 || Learning rate: lr=5e-05.
===> Epoch[151](250/324): Loss: 0.5599 || Learning rate: lr=5e-05.
===> Epoch[151](260/324): Loss: 0.8829 || Learning rate: lr=5e-05.
===> Epoch[151](270/324): Loss: 0.4927 || Learning rate: lr=5e-05.
===> Epoch[151](280/324): Loss: 0.5775 || Learning rate: lr=5e-05.
===> Epoch[151](290/324): Loss: 0.6578 || Learning rate: lr=5e-05.
===> Epoch[151](300/324): Loss: 0.5877 || Learning rate: lr=5e-05.
===> Epoch[151](310/324): Loss: 0.7138 || Learning rate: lr=5e-05.
===> Epoch[151](320/324): Loss: 0.9863 || Learning rate: lr=5e-05.
===> Epoch[152](10/324): Loss: 0.5205 || Learning rate: lr=5e-05.
===> Epoch[152](20/324): Loss: 0.9559 || Learning rate: lr=5e-05.
===> Epoch[152](30/324): Loss: 0.6461 || Learning rate: lr=5e-05.
===> Epoch[152](40/324): Loss: 0.8183 || Learning rate: lr=5e-05.
===> Epoch[152](50/324): Loss: 0.8958 || Learning rate: lr=5e-05.
===> Epoch[152](60/324): Loss: 0.6994 || Learning rate: lr=5e-05.
===> Epoch[152](70/324): Loss: 0.7611 || Learning rate: lr=5e-05.
===> Epoch[152](80/324): Loss: 0.7706 || Learning rate: lr=5e-05.
===> Epoch[152](90/324): Loss: 0.7022 || Learning rate: lr=5e-05.
===> Epoch[152](100/324): Loss: 0.7794 || Learning rate: lr=5e-05.
===> Epoch[152](110/324): Loss: 0.7073 || Learning rate: lr=5e-05.
===> Epoch[152](120/324): Loss: 0.5365 || Learning rate: lr=5e-05.
===> Epoch[152](130/324): Loss: 0.7474 || Learning rate: lr=5e-05.
===> Epoch[152](140/324): Loss: 1.1689 || Learning rate: lr=5e-05.
===> Epoch[152](150/324): Loss: 0.6936 || Learning rate: lr=5e-05.
===> Epoch[152](160/324): Loss: 0.5645 || Learning rate: lr=5e-05.
===> Epoch[152](170/324): Loss: 0.6710 || Learning rate: lr=5e-05.
===> Epoch[152](180/324): Loss: 0.8259 || Learning rate: lr=5e-05.
===> Epoch[152](190/324): Loss: 0.6885 || Learning rate: lr=5e-05.
===> Epoch[152](200/324): Loss: 0.7481 || Learning rate: lr=5e-05.
===> Epoch[152](210/324): Loss: 0.8916 || Learning rate: lr=5e-05.
===> Epoch[152](220/324): Loss: 0.9317 || Learning rate: lr=5e-05.
===> Epoch[152](230/324): Loss: 0.5243 || Learning rate: lr=5e-05.
===> Epoch[152](240/324): Loss: 0.7635 || Learning rate: lr=5e-05.
===> Epoch[152](250/324): Loss: 0.5024 || Learning rate: lr=5e-05.
===> Epoch[152](260/324): Loss: 0.4361 || Learning rate: lr=5e-05.
===> Epoch[152](270/324): Loss: 0.6848 || Learning rate: lr=5e-05.
===> Epoch[152](280/324): Loss: 0.8179 || Learning rate: lr=5e-05.
===> Epoch[152](290/324): Loss: 0.6795 || Learning rate: lr=5e-05.
===> Epoch[152](300/324): Loss: 0.5034 || Learning rate: lr=5e-05.
===> Epoch[152](310/324): Loss: 1.1544 || Learning rate: lr=5e-05.
===> Epoch[152](320/324): Loss: 0.5808 || Learning rate: lr=5e-05.
===> Epoch[153](10/324): Loss: 0.6890 || Learning rate: lr=5e-05.
===> Epoch[153](20/324): Loss: 0.9042 || Learning rate: lr=5e-05.
===> Epoch[153](30/324): Loss: 1.1624 || Learning rate: lr=5e-05.
===> Epoch[153](40/324): Loss: 0.5948 || Learning rate: lr=5e-05.
===> Epoch[153](50/324): Loss: 1.0727 || Learning rate: lr=5e-05.
===> Epoch[153](60/324): Loss: 0.5978 || Learning rate: lr=5e-05.
===> Epoch[153](70/324): Loss: 0.7002 || Learning rate: lr=5e-05.
===> Epoch[153](80/324): Loss: 0.8564 || Learning rate: lr=5e-05.
===> Epoch[153](90/324): Loss: 0.7250 || Learning rate: lr=5e-05.
===> Epoch[153](100/324): Loss: 0.8493 || Learning rate: lr=5e-05.
===> Epoch[153](110/324): Loss: 0.7576 || Learning rate: lr=5e-05.
===> Epoch[153](120/324): Loss: 0.5782 || Learning rate: lr=5e-05.
===> Epoch[153](130/324): Loss: 0.8817 || Learning rate: lr=5e-05.
===> Epoch[153](140/324): Loss: 0.9534 || Learning rate: lr=5e-05.
===> Epoch[153](150/324): Loss: 0.8034 || Learning rate: lr=5e-05.
===> Epoch[153](160/324): Loss: 0.6186 || Learning rate: lr=5e-05.
===> Epoch[153](170/324): Loss: 0.4650 || Learning rate: lr=5e-05.
===> Epoch[153](180/324): Loss: 0.6014 || Learning rate: lr=5e-05.
===> Epoch[153](190/324): Loss: 0.8125 || Learning rate: lr=5e-05.
===> Epoch[153](200/324): Loss: 0.6411 || Learning rate: lr=5e-05.
===> Epoch[153](210/324): Loss: 0.6622 || Learning rate: lr=5e-05.
===> Epoch[153](220/324): Loss: 0.6247 || Learning rate: lr=5e-05.
===> Epoch[153](230/324): Loss: 0.6043 || Learning rate: lr=5e-05.
===> Epoch[153](240/324): Loss: 0.6782 || Learning rate: lr=5e-05.
===> Epoch[153](250/324): Loss: 0.7598 || Learning rate: lr=5e-05.
===> Epoch[153](260/324): Loss: 0.6715 || Learning rate: lr=5e-05.
===> Epoch[153](270/324): Loss: 0.9366 || Learning rate: lr=5e-05.
===> Epoch[153](280/324): Loss: 0.8722 || Learning rate: lr=5e-05.
===> Epoch[153](290/324): Loss: 0.7540 || Learning rate: lr=5e-05.
===> Epoch[153](300/324): Loss: 0.7288 || Learning rate: lr=5e-05.
===> Epoch[153](310/324): Loss: 1.0210 || Learning rate: lr=5e-05.
===> Epoch[153](320/324): Loss: 0.6526 || Learning rate: lr=5e-05.
===> Epoch[154](10/324): Loss: 1.0492 || Learning rate: lr=5e-05.
===> Epoch[154](20/324): Loss: 0.7631 || Learning rate: lr=5e-05.
===> Epoch[154](30/324): Loss: 0.8261 || Learning rate: lr=5e-05.
===> Epoch[154](40/324): Loss: 1.1246 || Learning rate: lr=5e-05.
===> Epoch[154](50/324): Loss: 0.6511 || Learning rate: lr=5e-05.
===> Epoch[154](60/324): Loss: 0.8050 || Learning rate: lr=5e-05.
===> Epoch[154](70/324): Loss: 0.8813 || Learning rate: lr=5e-05.
===> Epoch[154](80/324): Loss: 0.6123 || Learning rate: lr=5e-05.
===> Epoch[154](90/324): Loss: 0.6522 || Learning rate: lr=5e-05.
===> Epoch[154](100/324): Loss: 0.9510 || Learning rate: lr=5e-05.
===> Epoch[154](110/324): Loss: 0.5969 || Learning rate: lr=5e-05.
===> Epoch[154](120/324): Loss: 0.6695 || Learning rate: lr=5e-05.
===> Epoch[154](130/324): Loss: 0.9875 || Learning rate: lr=5e-05.
===> Epoch[154](140/324): Loss: 0.9088 || Learning rate: lr=5e-05.
===> Epoch[154](150/324): Loss: 0.8300 || Learning rate: lr=5e-05.
===> Epoch[154](160/324): Loss: 0.7750 || Learning rate: lr=5e-05.
===> Epoch[154](170/324): Loss: 0.7426 || Learning rate: lr=5e-05.
===> Epoch[154](180/324): Loss: 0.7354 || Learning rate: lr=5e-05.
===> Epoch[154](190/324): Loss: 0.5638 || Learning rate: lr=5e-05.
===> Epoch[154](200/324): Loss: 0.8940 || Learning rate: lr=5e-05.
===> Epoch[154](210/324): Loss: 0.8362 || Learning rate: lr=5e-05.
===> Epoch[154](220/324): Loss: 0.5442 || Learning rate: lr=5e-05.
===> Epoch[154](230/324): Loss: 1.0160 || Learning rate: lr=5e-05.
===> Epoch[154](240/324): Loss: 0.4801 || Learning rate: lr=5e-05.
===> Epoch[154](250/324): Loss: 0.7817 || Learning rate: lr=5e-05.
===> Epoch[154](260/324): Loss: 0.6497 || Learning rate: lr=5e-05.
===> Epoch[154](270/324): Loss: 0.8730 || Learning rate: lr=5e-05.
===> Epoch[154](280/324): Loss: 0.8766 || Learning rate: lr=5e-05.
===> Epoch[154](290/324): Loss: 0.7758 || Learning rate: lr=5e-05.
===> Epoch[154](300/324): Loss: 0.7818 || Learning rate: lr=5e-05.
===> Epoch[154](310/324): Loss: 0.5977 || Learning rate: lr=5e-05.
===> Epoch[154](320/324): Loss: 0.6695 || Learning rate: lr=5e-05.
===> Epoch[155](10/324): Loss: 0.7206 || Learning rate: lr=5e-05.
===> Epoch[155](20/324): Loss: 1.1273 || Learning rate: lr=5e-05.
===> Epoch[155](30/324): Loss: 0.8285 || Learning rate: lr=5e-05.
===> Epoch[155](40/324): Loss: 1.0386 || Learning rate: lr=5e-05.
===> Epoch[155](50/324): Loss: 0.8276 || Learning rate: lr=5e-05.
===> Epoch[155](60/324): Loss: 0.8599 || Learning rate: lr=5e-05.
===> Epoch[155](70/324): Loss: 0.8262 || Learning rate: lr=5e-05.
===> Epoch[155](80/324): Loss: 0.5469 || Learning rate: lr=5e-05.
===> Epoch[155](90/324): Loss: 0.5694 || Learning rate: lr=5e-05.
===> Epoch[155](100/324): Loss: 0.9234 || Learning rate: lr=5e-05.
===> Epoch[155](110/324): Loss: 0.5705 || Learning rate: lr=5e-05.
===> Epoch[155](120/324): Loss: 0.6135 || Learning rate: lr=5e-05.
===> Epoch[155](130/324): Loss: 0.9302 || Learning rate: lr=5e-05.
===> Epoch[155](140/324): Loss: 0.8548 || Learning rate: lr=5e-05.
===> Epoch[155](150/324): Loss: 0.6385 || Learning rate: lr=5e-05.
===> Epoch[155](160/324): Loss: 0.9262 || Learning rate: lr=5e-05.
===> Epoch[155](170/324): Loss: 0.6161 || Learning rate: lr=5e-05.
===> Epoch[155](180/324): Loss: 0.6791 || Learning rate: lr=5e-05.
===> Epoch[155](190/324): Loss: 0.8742 || Learning rate: lr=5e-05.
===> Epoch[155](200/324): Loss: 1.1175 || Learning rate: lr=5e-05.
===> Epoch[155](210/324): Loss: 0.6272 || Learning rate: lr=5e-05.
===> Epoch[155](220/324): Loss: 0.8957 || Learning rate: lr=5e-05.
===> Epoch[155](230/324): Loss: 0.8855 || Learning rate: lr=5e-05.
===> Epoch[155](240/324): Loss: 0.4396 || Learning rate: lr=5e-05.
===> Epoch[155](250/324): Loss: 1.1193 || Learning rate: lr=5e-05.
===> Epoch[155](260/324): Loss: 0.8742 || Learning rate: lr=5e-05.
===> Epoch[155](270/324): Loss: 0.4124 || Learning rate: lr=5e-05.
===> Epoch[155](280/324): Loss: 0.7618 || Learning rate: lr=5e-05.
===> Epoch[155](290/324): Loss: 0.5685 || Learning rate: lr=5e-05.
===> Epoch[155](300/324): Loss: 0.6674 || Learning rate: lr=5e-05.
===> Epoch[155](310/324): Loss: 0.9737 || Learning rate: lr=5e-05.
===> Epoch[155](320/324): Loss: 0.7707 || Learning rate: lr=5e-05.
===> Epoch[156](10/324): Loss: 0.7858 || Learning rate: lr=5e-05.
===> Epoch[156](20/324): Loss: 0.7816 || Learning rate: lr=5e-05.
===> Epoch[156](30/324): Loss: 0.5429 || Learning rate: lr=5e-05.
===> Epoch[156](40/324): Loss: 0.5023 || Learning rate: lr=5e-05.
===> Epoch[156](50/324): Loss: 0.7358 || Learning rate: lr=5e-05.
===> Epoch[156](60/324): Loss: 0.8141 || Learning rate: lr=5e-05.
===> Epoch[156](70/324): Loss: 0.9196 || Learning rate: lr=5e-05.
===> Epoch[156](80/324): Loss: 0.8224 || Learning rate: lr=5e-05.
===> Epoch[156](90/324): Loss: 0.9075 || Learning rate: lr=5e-05.
===> Epoch[156](100/324): Loss: 1.0279 || Learning rate: lr=5e-05.
===> Epoch[156](110/324): Loss: 0.6467 || Learning rate: lr=5e-05.
===> Epoch[156](120/324): Loss: 0.8006 || Learning rate: lr=5e-05.
===> Epoch[156](130/324): Loss: 0.8313 || Learning rate: lr=5e-05.
===> Epoch[156](140/324): Loss: 0.5672 || Learning rate: lr=5e-05.
===> Epoch[156](150/324): Loss: 0.7572 || Learning rate: lr=5e-05.
===> Epoch[156](160/324): Loss: 0.9238 || Learning rate: lr=5e-05.
===> Epoch[156](170/324): Loss: 0.7937 || Learning rate: lr=5e-05.
===> Epoch[156](180/324): Loss: 0.4747 || Learning rate: lr=5e-05.
===> Epoch[156](190/324): Loss: 0.4546 || Learning rate: lr=5e-05.
===> Epoch[156](200/324): Loss: 0.7174 || Learning rate: lr=5e-05.
===> Epoch[156](210/324): Loss: 1.2108 || Learning rate: lr=5e-05.
===> Epoch[156](220/324): Loss: 0.8537 || Learning rate: lr=5e-05.
===> Epoch[156](230/324): Loss: 0.6860 || Learning rate: lr=5e-05.
===> Epoch[156](240/324): Loss: 0.6188 || Learning rate: lr=5e-05.
===> Epoch[156](250/324): Loss: 0.7625 || Learning rate: lr=5e-05.
===> Epoch[156](260/324): Loss: 0.7966 || Learning rate: lr=5e-05.
===> Epoch[156](270/324): Loss: 0.6957 || Learning rate: lr=5e-05.
===> Epoch[156](280/324): Loss: 0.7456 || Learning rate: lr=5e-05.
===> Epoch[156](290/324): Loss: 0.6734 || Learning rate: lr=5e-05.
===> Epoch[156](300/324): Loss: 1.0323 || Learning rate: lr=5e-05.
===> Epoch[156](310/324): Loss: 1.0446 || Learning rate: lr=5e-05.
===> Epoch[156](320/324): Loss: 0.8875 || Learning rate: lr=5e-05.
===> Epoch[157](10/324): Loss: 1.4006 || Learning rate: lr=5e-05.
===> Epoch[157](20/324): Loss: 0.5929 || Learning rate: lr=5e-05.
===> Epoch[157](30/324): Loss: 0.5384 || Learning rate: lr=5e-05.
===> Epoch[157](40/324): Loss: 0.9353 || Learning rate: lr=5e-05.
===> Epoch[157](50/324): Loss: 0.7124 || Learning rate: lr=5e-05.
===> Epoch[157](60/324): Loss: 0.8366 || Learning rate: lr=5e-05.
===> Epoch[157](70/324): Loss: 0.7352 || Learning rate: lr=5e-05.
===> Epoch[157](80/324): Loss: 0.8095 || Learning rate: lr=5e-05.
===> Epoch[157](90/324): Loss: 0.8298 || Learning rate: lr=5e-05.
===> Epoch[157](100/324): Loss: 0.5857 || Learning rate: lr=5e-05.
===> Epoch[157](110/324): Loss: 0.6006 || Learning rate: lr=5e-05.
===> Epoch[157](120/324): Loss: 0.6763 || Learning rate: lr=5e-05.
===> Epoch[157](130/324): Loss: 0.5683 || Learning rate: lr=5e-05.
===> Epoch[157](140/324): Loss: 0.8599 || Learning rate: lr=5e-05.
===> Epoch[157](150/324): Loss: 0.6120 || Learning rate: lr=5e-05.
===> Epoch[157](160/324): Loss: 0.8051 || Learning rate: lr=5e-05.
===> Epoch[157](170/324): Loss: 0.4898 || Learning rate: lr=5e-05.
===> Epoch[157](180/324): Loss: 0.5520 || Learning rate: lr=5e-05.
===> Epoch[157](190/324): Loss: 0.5867 || Learning rate: lr=5e-05.
===> Epoch[157](200/324): Loss: 0.9776 || Learning rate: lr=5e-05.
===> Epoch[157](210/324): Loss: 0.7265 || Learning rate: lr=5e-05.
===> Epoch[157](220/324): Loss: 0.8127 || Learning rate: lr=5e-05.
===> Epoch[157](230/324): Loss: 1.1797 || Learning rate: lr=5e-05.
===> Epoch[157](240/324): Loss: 0.7781 || Learning rate: lr=5e-05.
===> Epoch[157](250/324): Loss: 0.4386 || Learning rate: lr=5e-05.
===> Epoch[157](260/324): Loss: 0.8733 || Learning rate: lr=5e-05.
===> Epoch[157](270/324): Loss: 0.9431 || Learning rate: lr=5e-05.
===> Epoch[157](280/324): Loss: 0.5970 || Learning rate: lr=5e-05.
===> Epoch[157](290/324): Loss: 0.7230 || Learning rate: lr=5e-05.
===> Epoch[157](300/324): Loss: 1.0320 || Learning rate: lr=5e-05.
===> Epoch[157](310/324): Loss: 0.7208 || Learning rate: lr=5e-05.
===> Epoch[157](320/324): Loss: 0.6337 || Learning rate: lr=5e-05.
===> Epoch[158](10/324): Loss: 0.8337 || Learning rate: lr=5e-05.
===> Epoch[158](20/324): Loss: 0.7474 || Learning rate: lr=5e-05.
===> Epoch[158](30/324): Loss: 0.9666 || Learning rate: lr=5e-05.
===> Epoch[158](40/324): Loss: 1.2109 || Learning rate: lr=5e-05.
===> Epoch[158](50/324): Loss: 0.7279 || Learning rate: lr=5e-05.
===> Epoch[158](60/324): Loss: 0.7015 || Learning rate: lr=5e-05.
===> Epoch[158](70/324): Loss: 0.6467 || Learning rate: lr=5e-05.
===> Epoch[158](80/324): Loss: 1.0714 || Learning rate: lr=5e-05.
===> Epoch[158](90/324): Loss: 0.7279 || Learning rate: lr=5e-05.
===> Epoch[158](100/324): Loss: 0.9152 || Learning rate: lr=5e-05.
===> Epoch[158](110/324): Loss: 0.8117 || Learning rate: lr=5e-05.
===> Epoch[158](120/324): Loss: 0.7006 || Learning rate: lr=5e-05.
===> Epoch[158](130/324): Loss: 0.8135 || Learning rate: lr=5e-05.
===> Epoch[158](140/324): Loss: 0.6645 || Learning rate: lr=5e-05.
===> Epoch[158](150/324): Loss: 0.6395 || Learning rate: lr=5e-05.
===> Epoch[158](160/324): Loss: 0.5701 || Learning rate: lr=5e-05.
===> Epoch[158](170/324): Loss: 0.7327 || Learning rate: lr=5e-05.
===> Epoch[158](180/324): Loss: 0.9927 || Learning rate: lr=5e-05.
===> Epoch[158](190/324): Loss: 0.5495 || Learning rate: lr=5e-05.
===> Epoch[158](200/324): Loss: 0.8262 || Learning rate: lr=5e-05.
===> Epoch[158](210/324): Loss: 0.7586 || Learning rate: lr=5e-05.
===> Epoch[158](220/324): Loss: 0.7004 || Learning rate: lr=5e-05.
===> Epoch[158](230/324): Loss: 0.6156 || Learning rate: lr=5e-05.
===> Epoch[158](240/324): Loss: 0.6760 || Learning rate: lr=5e-05.
===> Epoch[158](250/324): Loss: 0.6115 || Learning rate: lr=5e-05.
===> Epoch[158](260/324): Loss: 0.6619 || Learning rate: lr=5e-05.
===> Epoch[158](270/324): Loss: 0.6674 || Learning rate: lr=5e-05.
===> Epoch[158](280/324): Loss: 0.7202 || Learning rate: lr=5e-05.
===> Epoch[158](290/324): Loss: 0.6592 || Learning rate: lr=5e-05.
===> Epoch[158](300/324): Loss: 0.8162 || Learning rate: lr=5e-05.
===> Epoch[158](310/324): Loss: 0.8267 || Learning rate: lr=5e-05.
===> Epoch[158](320/324): Loss: 0.5602 || Learning rate: lr=5e-05.
===> Epoch[159](10/324): Loss: 0.8088 || Learning rate: lr=5e-05.
===> Epoch[159](20/324): Loss: 0.7236 || Learning rate: lr=5e-05.
===> Epoch[159](30/324): Loss: 0.6281 || Learning rate: lr=5e-05.
===> Epoch[159](40/324): Loss: 0.7979 || Learning rate: lr=5e-05.
===> Epoch[159](50/324): Loss: 1.0254 || Learning rate: lr=5e-05.
===> Epoch[159](60/324): Loss: 1.0362 || Learning rate: lr=5e-05.
===> Epoch[159](70/324): Loss: 0.8519 || Learning rate: lr=5e-05.
===> Epoch[159](80/324): Loss: 0.6478 || Learning rate: lr=5e-05.
===> Epoch[159](90/324): Loss: 0.7082 || Learning rate: lr=5e-05.
===> Epoch[159](100/324): Loss: 0.7524 || Learning rate: lr=5e-05.
===> Epoch[159](110/324): Loss: 0.6312 || Learning rate: lr=5e-05.
===> Epoch[159](120/324): Loss: 1.0068 || Learning rate: lr=5e-05.
===> Epoch[159](130/324): Loss: 0.5458 || Learning rate: lr=5e-05.
===> Epoch[159](140/324): Loss: 0.8231 || Learning rate: lr=5e-05.
===> Epoch[159](150/324): Loss: 0.6380 || Learning rate: lr=5e-05.
===> Epoch[159](160/324): Loss: 0.5057 || Learning rate: lr=5e-05.
===> Epoch[159](170/324): Loss: 0.6790 || Learning rate: lr=5e-05.
===> Epoch[159](180/324): Loss: 0.6171 || Learning rate: lr=5e-05.
===> Epoch[159](190/324): Loss: 0.6147 || Learning rate: lr=5e-05.
===> Epoch[159](200/324): Loss: 0.7091 || Learning rate: lr=5e-05.
===> Epoch[159](210/324): Loss: 0.7310 || Learning rate: lr=5e-05.
===> Epoch[159](220/324): Loss: 0.8555 || Learning rate: lr=5e-05.
===> Epoch[159](230/324): Loss: 0.5098 || Learning rate: lr=5e-05.
===> Epoch[159](240/324): Loss: 0.6796 || Learning rate: lr=5e-05.
===> Epoch[159](250/324): Loss: 0.4590 || Learning rate: lr=5e-05.
===> Epoch[159](260/324): Loss: 0.8581 || Learning rate: lr=5e-05.
===> Epoch[159](270/324): Loss: 0.8819 || Learning rate: lr=5e-05.
===> Epoch[159](280/324): Loss: 0.8860 || Learning rate: lr=5e-05.
===> Epoch[159](290/324): Loss: 0.8858 || Learning rate: lr=5e-05.
===> Epoch[159](300/324): Loss: 0.7729 || Learning rate: lr=5e-05.
===> Epoch[159](310/324): Loss: 0.6648 || Learning rate: lr=5e-05.
===> Epoch[159](320/324): Loss: 0.7223 || Learning rate: lr=5e-05.
===> Epoch[160](10/324): Loss: 0.4718 || Learning rate: lr=5e-05.
===> Epoch[160](20/324): Loss: 0.8139 || Learning rate: lr=5e-05.
===> Epoch[160](30/324): Loss: 0.7851 || Learning rate: lr=5e-05.
===> Epoch[160](40/324): Loss: 0.8833 || Learning rate: lr=5e-05.
===> Epoch[160](50/324): Loss: 0.6562 || Learning rate: lr=5e-05.
===> Epoch[160](60/324): Loss: 0.6141 || Learning rate: lr=5e-05.
===> Epoch[160](70/324): Loss: 0.5035 || Learning rate: lr=5e-05.
===> Epoch[160](80/324): Loss: 0.5813 || Learning rate: lr=5e-05.
===> Epoch[160](90/324): Loss: 0.9219 || Learning rate: lr=5e-05.
===> Epoch[160](100/324): Loss: 0.7820 || Learning rate: lr=5e-05.
===> Epoch[160](110/324): Loss: 0.5104 || Learning rate: lr=5e-05.
===> Epoch[160](120/324): Loss: 0.7768 || Learning rate: lr=5e-05.
===> Epoch[160](130/324): Loss: 0.9275 || Learning rate: lr=5e-05.
===> Epoch[160](140/324): Loss: 0.9534 || Learning rate: lr=5e-05.
===> Epoch[160](150/324): Loss: 0.6548 || Learning rate: lr=5e-05.
===> Epoch[160](160/324): Loss: 0.5449 || Learning rate: lr=5e-05.
===> Epoch[160](170/324): Loss: 0.5812 || Learning rate: lr=5e-05.
===> Epoch[160](180/324): Loss: 0.9144 || Learning rate: lr=5e-05.
===> Epoch[160](190/324): Loss: 0.7171 || Learning rate: lr=5e-05.
===> Epoch[160](200/324): Loss: 1.0491 || Learning rate: lr=5e-05.
===> Epoch[160](210/324): Loss: 0.4212 || Learning rate: lr=5e-05.
===> Epoch[160](220/324): Loss: 0.5004 || Learning rate: lr=5e-05.
===> Epoch[160](230/324): Loss: 0.9611 || Learning rate: lr=5e-05.
===> Epoch[160](240/324): Loss: 0.5266 || Learning rate: lr=5e-05.
===> Epoch[160](250/324): Loss: 0.8546 || Learning rate: lr=5e-05.
===> Epoch[160](260/324): Loss: 0.6858 || Learning rate: lr=5e-05.
===> Epoch[160](270/324): Loss: 0.5953 || Learning rate: lr=5e-05.
===> Epoch[160](280/324): Loss: 0.7890 || Learning rate: lr=5e-05.
===> Epoch[160](290/324): Loss: 0.9540 || Learning rate: lr=5e-05.
===> Epoch[160](300/324): Loss: 0.7319 || Learning rate: lr=5e-05.
===> Epoch[160](310/324): Loss: 0.6960 || Learning rate: lr=5e-05.
===> Epoch[160](320/324): Loss: 0.7259 || Learning rate: lr=5e-05.
Checkpoint saved to weights/epoch_v2_160.pth
===> Epoch[161](10/324): Loss: 0.8594 || Learning rate: lr=5e-05.
===> Epoch[161](20/324): Loss: 0.4827 || Learning rate: lr=5e-05.
===> Epoch[161](30/324): Loss: 0.7015 || Learning rate: lr=5e-05.
===> Epoch[161](40/324): Loss: 0.5840 || Learning rate: lr=5e-05.
===> Epoch[161](50/324): Loss: 1.0586 || Learning rate: lr=5e-05.
===> Epoch[161](60/324): Loss: 0.6970 || Learning rate: lr=5e-05.
===> Epoch[161](70/324): Loss: 0.5485 || Learning rate: lr=5e-05.
===> Epoch[161](80/324): Loss: 1.1850 || Learning rate: lr=5e-05.
===> Epoch[161](90/324): Loss: 1.0287 || Learning rate: lr=5e-05.
===> Epoch[161](100/324): Loss: 0.7964 || Learning rate: lr=5e-05.
===> Epoch[161](110/324): Loss: 0.8542 || Learning rate: lr=5e-05.
===> Epoch[161](120/324): Loss: 0.9752 || Learning rate: lr=5e-05.
===> Epoch[161](130/324): Loss: 1.0269 || Learning rate: lr=5e-05.
===> Epoch[161](140/324): Loss: 1.1221 || Learning rate: lr=5e-05.
===> Epoch[161](150/324): Loss: 0.6654 || Learning rate: lr=5e-05.
===> Epoch[161](160/324): Loss: 0.8923 || Learning rate: lr=5e-05.
===> Epoch[161](170/324): Loss: 0.4578 || Learning rate: lr=5e-05.
===> Epoch[161](180/324): Loss: 0.8021 || Learning rate: lr=5e-05.
===> Epoch[161](190/324): Loss: 0.7130 || Learning rate: lr=5e-05.
===> Epoch[161](200/324): Loss: 0.7003 || Learning rate: lr=5e-05.
===> Epoch[161](210/324): Loss: 0.7775 || Learning rate: lr=5e-05.
===> Epoch[161](220/324): Loss: 0.6033 || Learning rate: lr=5e-05.
===> Epoch[161](230/324): Loss: 1.0844 || Learning rate: lr=5e-05.
===> Epoch[161](240/324): Loss: 0.8828 || Learning rate: lr=5e-05.
===> Epoch[161](250/324): Loss: 0.8695 || Learning rate: lr=5e-05.
===> Epoch[161](260/324): Loss: 0.5466 || Learning rate: lr=5e-05.
===> Epoch[161](270/324): Loss: 0.7240 || Learning rate: lr=5e-05.
===> Epoch[161](280/324): Loss: 0.7859 || Learning rate: lr=5e-05.
===> Epoch[161](290/324): Loss: 0.7964 || Learning rate: lr=5e-05.
===> Epoch[161](300/324): Loss: 0.8843 || Learning rate: lr=5e-05.
===> Epoch[161](310/324): Loss: 1.0669 || Learning rate: lr=5e-05.
===> Epoch[161](320/324): Loss: 0.7960 || Learning rate: lr=5e-05.
===> Epoch[162](10/324): Loss: 0.7911 || Learning rate: lr=5e-05.
===> Epoch[162](20/324): Loss: 0.6147 || Learning rate: lr=5e-05.
===> Epoch[162](30/324): Loss: 0.6900 || Learning rate: lr=5e-05.
===> Epoch[162](40/324): Loss: 0.7072 || Learning rate: lr=5e-05.
===> Epoch[162](50/324): Loss: 0.4415 || Learning rate: lr=5e-05.
===> Epoch[162](60/324): Loss: 0.5157 || Learning rate: lr=5e-05.
===> Epoch[162](70/324): Loss: 0.6618 || Learning rate: lr=5e-05.
===> Epoch[162](80/324): Loss: 0.6506 || Learning rate: lr=5e-05.
===> Epoch[162](90/324): Loss: 0.7222 || Learning rate: lr=5e-05.
===> Epoch[162](100/324): Loss: 1.1336 || Learning rate: lr=5e-05.
===> Epoch[162](110/324): Loss: 1.0002 || Learning rate: lr=5e-05.
===> Epoch[162](120/324): Loss: 0.7209 || Learning rate: lr=5e-05.
===> Epoch[162](130/324): Loss: 0.6213 || Learning rate: lr=5e-05.
===> Epoch[162](140/324): Loss: 0.5808 || Learning rate: lr=5e-05.
===> Epoch[162](150/324): Loss: 0.7620 || Learning rate: lr=5e-05.
===> Epoch[162](160/324): Loss: 0.8210 || Learning rate: lr=5e-05.
===> Epoch[162](170/324): Loss: 0.7107 || Learning rate: lr=5e-05.
===> Epoch[162](180/324): Loss: 0.8168 || Learning rate: lr=5e-05.
===> Epoch[162](190/324): Loss: 0.7681 || Learning rate: lr=5e-05.
===> Epoch[162](200/324): Loss: 0.6738 || Learning rate: lr=5e-05.
===> Epoch[162](210/324): Loss: 0.8461 || Learning rate: lr=5e-05.
===> Epoch[162](220/324): Loss: 0.5848 || Learning rate: lr=5e-05.
===> Epoch[162](230/324): Loss: 0.6662 || Learning rate: lr=5e-05.
===> Epoch[162](240/324): Loss: 0.7928 || Learning rate: lr=5e-05.
===> Epoch[162](250/324): Loss: 0.8479 || Learning rate: lr=5e-05.
===> Epoch[162](260/324): Loss: 0.8243 || Learning rate: lr=5e-05.
===> Epoch[162](270/324): Loss: 0.8368 || Learning rate: lr=5e-05.
===> Epoch[162](280/324): Loss: 0.9172 || Learning rate: lr=5e-05.
===> Epoch[162](290/324): Loss: 0.7595 || Learning rate: lr=5e-05.
===> Epoch[162](300/324): Loss: 0.6566 || Learning rate: lr=5e-05.
===> Epoch[162](310/324): Loss: 0.7673 || Learning rate: lr=5e-05.
===> Epoch[162](320/324): Loss: 0.7991 || Learning rate: lr=5e-05.
===> Epoch[163](10/324): Loss: 0.5658 || Learning rate: lr=5e-05.
===> Epoch[163](20/324): Loss: 0.7426 || Learning rate: lr=5e-05.
===> Epoch[163](30/324): Loss: 0.7719 || Learning rate: lr=5e-05.
===> Epoch[163](40/324): Loss: 0.6327 || Learning rate: lr=5e-05.
===> Epoch[163](50/324): Loss: 0.7658 || Learning rate: lr=5e-05.
===> Epoch[163](60/324): Loss: 0.7639 || Learning rate: lr=5e-05.
===> Epoch[163](70/324): Loss: 0.6074 || Learning rate: lr=5e-05.
===> Epoch[163](80/324): Loss: 0.5864 || Learning rate: lr=5e-05.
===> Epoch[163](90/324): Loss: 0.9884 || Learning rate: lr=5e-05.
===> Epoch[163](100/324): Loss: 0.8852 || Learning rate: lr=5e-05.
===> Epoch[163](110/324): Loss: 0.8114 || Learning rate: lr=5e-05.
===> Epoch[163](120/324): Loss: 0.6995 || Learning rate: lr=5e-05.
===> Epoch[163](130/324): Loss: 0.6028 || Learning rate: lr=5e-05.
===> Epoch[163](140/324): Loss: 0.8151 || Learning rate: lr=5e-05.
===> Epoch[163](150/324): Loss: 0.9518 || Learning rate: lr=5e-05.
===> Epoch[163](160/324): Loss: 0.8586 || Learning rate: lr=5e-05.
===> Epoch[163](170/324): Loss: 0.8865 || Learning rate: lr=5e-05.
===> Epoch[163](180/324): Loss: 0.7447 || Learning rate: lr=5e-05.
===> Epoch[163](190/324): Loss: 0.6737 || Learning rate: lr=5e-05.
===> Epoch[163](200/324): Loss: 0.6929 || Learning rate: lr=5e-05.
===> Epoch[163](210/324): Loss: 0.7346 || Learning rate: lr=5e-05.
===> Epoch[163](220/324): Loss: 0.6856 || Learning rate: lr=5e-05.
===> Epoch[163](230/324): Loss: 0.5917 || Learning rate: lr=5e-05.
===> Epoch[163](240/324): Loss: 0.6596 || Learning rate: lr=5e-05.
===> Epoch[163](250/324): Loss: 0.8811 || Learning rate: lr=5e-05.
===> Epoch[163](260/324): Loss: 0.8397 || Learning rate: lr=5e-05.
===> Epoch[163](270/324): Loss: 0.8363 || Learning rate: lr=5e-05.
===> Epoch[163](280/324): Loss: 0.5502 || Learning rate: lr=5e-05.
===> Epoch[163](290/324): Loss: 0.5356 || Learning rate: lr=5e-05.
===> Epoch[163](300/324): Loss: 0.7045 || Learning rate: lr=5e-05.
===> Epoch[163](310/324): Loss: 0.7146 || Learning rate: lr=5e-05.
===> Epoch[163](320/324): Loss: 0.9456 || Learning rate: lr=5e-05.
===> Epoch[164](10/324): Loss: 0.8156 || Learning rate: lr=5e-05.
===> Epoch[164](20/324): Loss: 0.6774 || Learning rate: lr=5e-05.
===> Epoch[164](30/324): Loss: 0.6583 || Learning rate: lr=5e-05.
===> Epoch[164](40/324): Loss: 0.5185 || Learning rate: lr=5e-05.
===> Epoch[164](50/324): Loss: 0.8904 || Learning rate: lr=5e-05.
===> Epoch[164](60/324): Loss: 0.6745 || Learning rate: lr=5e-05.
===> Epoch[164](70/324): Loss: 1.2207 || Learning rate: lr=5e-05.
===> Epoch[164](80/324): Loss: 0.5977 || Learning rate: lr=5e-05.
===> Epoch[164](90/324): Loss: 0.6780 || Learning rate: lr=5e-05.
===> Epoch[164](100/324): Loss: 0.8488 || Learning rate: lr=5e-05.
===> Epoch[164](110/324): Loss: 1.1018 || Learning rate: lr=5e-05.
===> Epoch[164](120/324): Loss: 0.5963 || Learning rate: lr=5e-05.
===> Epoch[164](130/324): Loss: 0.6363 || Learning rate: lr=5e-05.
===> Epoch[164](140/324): Loss: 0.7876 || Learning rate: lr=5e-05.
===> Epoch[164](150/324): Loss: 0.7161 || Learning rate: lr=5e-05.
===> Epoch[164](160/324): Loss: 0.9055 || Learning rate: lr=5e-05.
===> Epoch[164](170/324): Loss: 0.9369 || Learning rate: lr=5e-05.
===> Epoch[164](180/324): Loss: 0.6325 || Learning rate: lr=5e-05.
===> Epoch[164](190/324): Loss: 0.7741 || Learning rate: lr=5e-05.
===> Epoch[164](200/324): Loss: 0.9322 || Learning rate: lr=5e-05.
===> Epoch[164](210/324): Loss: 0.6212 || Learning rate: lr=5e-05.
===> Epoch[164](220/324): Loss: 0.8467 || Learning rate: lr=5e-05.
===> Epoch[164](230/324): Loss: 0.6479 || Learning rate: lr=5e-05.
===> Epoch[164](240/324): Loss: 1.0440 || Learning rate: lr=5e-05.
===> Epoch[164](250/324): Loss: 0.4984 || Learning rate: lr=5e-05.
===> Epoch[164](260/324): Loss: 0.7341 || Learning rate: lr=5e-05.
===> Epoch[164](270/324): Loss: 1.1481 || Learning rate: lr=5e-05.
===> Epoch[164](280/324): Loss: 0.6051 || Learning rate: lr=5e-05.
===> Epoch[164](290/324): Loss: 0.6802 || Learning rate: lr=5e-05.
===> Epoch[164](300/324): Loss: 0.9750 || Learning rate: lr=5e-05.
===> Epoch[164](310/324): Loss: 0.7896 || Learning rate: lr=5e-05.
===> Epoch[164](320/324): Loss: 0.6326 || Learning rate: lr=5e-05.
===> Epoch[165](10/324): Loss: 0.4694 || Learning rate: lr=5e-05.
===> Epoch[165](20/324): Loss: 0.7047 || Learning rate: lr=5e-05.
===> Epoch[165](30/324): Loss: 0.8714 || Learning rate: lr=5e-05.
===> Epoch[165](40/324): Loss: 0.4839 || Learning rate: lr=5e-05.
===> Epoch[165](50/324): Loss: 0.9690 || Learning rate: lr=5e-05.
===> Epoch[165](60/324): Loss: 0.6158 || Learning rate: lr=5e-05.
===> Epoch[165](70/324): Loss: 0.8694 || Learning rate: lr=5e-05.
===> Epoch[165](80/324): Loss: 0.7213 || Learning rate: lr=5e-05.
===> Epoch[165](90/324): Loss: 1.0453 || Learning rate: lr=5e-05.
===> Epoch[165](100/324): Loss: 0.6639 || Learning rate: lr=5e-05.
===> Epoch[165](110/324): Loss: 0.5950 || Learning rate: lr=5e-05.
===> Epoch[165](120/324): Loss: 0.9353 || Learning rate: lr=5e-05.
===> Epoch[165](130/324): Loss: 0.6722 || Learning rate: lr=5e-05.
===> Epoch[165](140/324): Loss: 0.8446 || Learning rate: lr=5e-05.
===> Epoch[165](150/324): Loss: 1.0894 || Learning rate: lr=5e-05.
===> Epoch[165](160/324): Loss: 0.7176 || Learning rate: lr=5e-05.
===> Epoch[165](170/324): Loss: 0.6876 || Learning rate: lr=5e-05.
===> Epoch[165](180/324): Loss: 0.6302 || Learning rate: lr=5e-05.
===> Epoch[165](190/324): Loss: 1.0496 || Learning rate: lr=5e-05.
===> Epoch[165](200/324): Loss: 0.9737 || Learning rate: lr=5e-05.
===> Epoch[165](210/324): Loss: 0.6561 || Learning rate: lr=5e-05.
===> Epoch[165](220/324): Loss: 0.5510 || Learning rate: lr=5e-05.
===> Epoch[165](230/324): Loss: 0.8926 || Learning rate: lr=5e-05.
===> Epoch[165](240/324): Loss: 0.5350 || Learning rate: lr=5e-05.
===> Epoch[165](250/324): Loss: 0.9297 || Learning rate: lr=5e-05.
===> Epoch[165](260/324): Loss: 0.6669 || Learning rate: lr=5e-05.
===> Epoch[165](270/324): Loss: 0.4527 || Learning rate: lr=5e-05.
===> Epoch[165](280/324): Loss: 0.7410 || Learning rate: lr=5e-05.
===> Epoch[165](290/324): Loss: 0.7609 || Learning rate: lr=5e-05.
===> Epoch[165](300/324): Loss: 0.7103 || Learning rate: lr=5e-05.
===> Epoch[165](310/324): Loss: 0.7460 || Learning rate: lr=5e-05.
===> Epoch[165](320/324): Loss: 0.6835 || Learning rate: lr=5e-05.
===> Epoch[166](10/324): Loss: 0.9843 || Learning rate: lr=5e-05.
===> Epoch[166](20/324): Loss: 0.6031 || Learning rate: lr=5e-05.
===> Epoch[166](30/324): Loss: 0.9161 || Learning rate: lr=5e-05.
===> Epoch[166](40/324): Loss: 0.7404 || Learning rate: lr=5e-05.
===> Epoch[166](50/324): Loss: 0.5487 || Learning rate: lr=5e-05.
===> Epoch[166](60/324): Loss: 0.7169 || Learning rate: lr=5e-05.
===> Epoch[166](70/324): Loss: 0.7422 || Learning rate: lr=5e-05.
===> Epoch[166](80/324): Loss: 1.0368 || Learning rate: lr=5e-05.
===> Epoch[166](90/324): Loss: 0.7705 || Learning rate: lr=5e-05.
===> Epoch[166](100/324): Loss: 0.8402 || Learning rate: lr=5e-05.
===> Epoch[166](110/324): Loss: 0.9892 || Learning rate: lr=5e-05.
===> Epoch[166](120/324): Loss: 0.6223 || Learning rate: lr=5e-05.
===> Epoch[166](130/324): Loss: 0.6005 || Learning rate: lr=5e-05.
===> Epoch[166](140/324): Loss: 0.9850 || Learning rate: lr=5e-05.
===> Epoch[166](150/324): Loss: 0.5308 || Learning rate: lr=5e-05.
===> Epoch[166](160/324): Loss: 0.6382 || Learning rate: lr=5e-05.
===> Epoch[166](170/324): Loss: 0.6523 || Learning rate: lr=5e-05.
===> Epoch[166](180/324): Loss: 1.1545 || Learning rate: lr=5e-05.
===> Epoch[166](190/324): Loss: 0.9894 || Learning rate: lr=5e-05.
===> Epoch[166](200/324): Loss: 0.6290 || Learning rate: lr=5e-05.
===> Epoch[166](210/324): Loss: 0.6195 || Learning rate: lr=5e-05.
===> Epoch[166](220/324): Loss: 1.0572 || Learning rate: lr=5e-05.
===> Epoch[166](230/324): Loss: 0.5502 || Learning rate: lr=5e-05.
===> Epoch[166](240/324): Loss: 0.6188 || Learning rate: lr=5e-05.
===> Epoch[166](250/324): Loss: 0.6970 || Learning rate: lr=5e-05.
===> Epoch[166](260/324): Loss: 0.8941 || Learning rate: lr=5e-05.
===> Epoch[166](270/324): Loss: 0.5895 || Learning rate: lr=5e-05.
===> Epoch[166](280/324): Loss: 0.8921 || Learning rate: lr=5e-05.
===> Epoch[166](290/324): Loss: 0.6547 || Learning rate: lr=5e-05.
===> Epoch[166](300/324): Loss: 0.6076 || Learning rate: lr=5e-05.
===> Epoch[166](310/324): Loss: 0.7437 || Learning rate: lr=5e-05.
===> Epoch[166](320/324): Loss: 0.4751 || Learning rate: lr=5e-05.
===> Epoch[167](10/324): Loss: 0.7092 || Learning rate: lr=5e-05.
===> Epoch[167](20/324): Loss: 0.7534 || Learning rate: lr=5e-05.
===> Epoch[167](30/324): Loss: 0.6534 || Learning rate: lr=5e-05.
===> Epoch[167](40/324): Loss: 0.7783 || Learning rate: lr=5e-05.
===> Epoch[167](50/324): Loss: 0.7843 || Learning rate: lr=5e-05.
===> Epoch[167](60/324): Loss: 0.7486 || Learning rate: lr=5e-05.
===> Epoch[167](70/324): Loss: 1.1454 || Learning rate: lr=5e-05.
===> Epoch[167](80/324): Loss: 0.5760 || Learning rate: lr=5e-05.
===> Epoch[167](90/324): Loss: 0.7615 || Learning rate: lr=5e-05.
===> Epoch[167](100/324): Loss: 0.6509 || Learning rate: lr=5e-05.
===> Epoch[167](110/324): Loss: 0.6713 || Learning rate: lr=5e-05.
===> Epoch[167](120/324): Loss: 0.5093 || Learning rate: lr=5e-05.
===> Epoch[167](130/324): Loss: 0.7123 || Learning rate: lr=5e-05.
===> Epoch[167](140/324): Loss: 0.5438 || Learning rate: lr=5e-05.
===> Epoch[167](150/324): Loss: 0.9770 || Learning rate: lr=5e-05.
===> Epoch[167](160/324): Loss: 0.8350 || Learning rate: lr=5e-05.
===> Epoch[167](170/324): Loss: 0.9677 || Learning rate: lr=5e-05.
===> Epoch[167](180/324): Loss: 0.5580 || Learning rate: lr=5e-05.
===> Epoch[167](190/324): Loss: 0.5461 || Learning rate: lr=5e-05.
===> Epoch[167](200/324): Loss: 0.8374 || Learning rate: lr=5e-05.
===> Epoch[167](210/324): Loss: 0.7918 || Learning rate: lr=5e-05.
===> Epoch[167](220/324): Loss: 1.0864 || Learning rate: lr=5e-05.
===> Epoch[167](230/324): Loss: 0.6555 || Learning rate: lr=5e-05.
===> Epoch[167](240/324): Loss: 0.8547 || Learning rate: lr=5e-05.
===> Epoch[167](250/324): Loss: 0.8002 || Learning rate: lr=5e-05.
===> Epoch[167](260/324): Loss: 0.7902 || Learning rate: lr=5e-05.
===> Epoch[167](270/324): Loss: 0.9861 || Learning rate: lr=5e-05.
===> Epoch[167](280/324): Loss: 0.5771 || Learning rate: lr=5e-05.
===> Epoch[167](290/324): Loss: 0.8228 || Learning rate: lr=5e-05.
===> Epoch[167](300/324): Loss: 0.9737 || Learning rate: lr=5e-05.
===> Epoch[167](310/324): Loss: 0.7157 || Learning rate: lr=5e-05.
===> Epoch[167](320/324): Loss: 0.6703 || Learning rate: lr=5e-05.
===> Epoch[168](10/324): Loss: 0.8029 || Learning rate: lr=5e-05.
===> Epoch[168](20/324): Loss: 0.8174 || Learning rate: lr=5e-05.
===> Epoch[168](30/324): Loss: 1.3223 || Learning rate: lr=5e-05.
===> Epoch[168](40/324): Loss: 0.8336 || Learning rate: lr=5e-05.
===> Epoch[168](50/324): Loss: 0.6054 || Learning rate: lr=5e-05.
===> Epoch[168](60/324): Loss: 0.7547 || Learning rate: lr=5e-05.
===> Epoch[168](70/324): Loss: 1.0622 || Learning rate: lr=5e-05.
===> Epoch[168](80/324): Loss: 0.6501 || Learning rate: lr=5e-05.
===> Epoch[168](90/324): Loss: 1.2705 || Learning rate: lr=5e-05.
===> Epoch[168](100/324): Loss: 0.8209 || Learning rate: lr=5e-05.
===> Epoch[168](110/324): Loss: 1.1368 || Learning rate: lr=5e-05.
===> Epoch[168](120/324): Loss: 1.1823 || Learning rate: lr=5e-05.
===> Epoch[168](130/324): Loss: 1.0570 || Learning rate: lr=5e-05.
===> Epoch[168](140/324): Loss: 1.2812 || Learning rate: lr=5e-05.
===> Epoch[168](150/324): Loss: 1.1379 || Learning rate: lr=5e-05.
===> Epoch[168](160/324): Loss: 0.9744 || Learning rate: lr=5e-05.
===> Epoch[168](170/324): Loss: 0.8541 || Learning rate: lr=5e-05.
===> Epoch[168](180/324): Loss: 0.6596 || Learning rate: lr=5e-05.
===> Epoch[168](190/324): Loss: 0.8868 || Learning rate: lr=5e-05.
===> Epoch[168](200/324): Loss: 0.9422 || Learning rate: lr=5e-05.
===> Epoch[168](210/324): Loss: 0.9508 || Learning rate: lr=5e-05.
===> Epoch[168](220/324): Loss: 0.8016 || Learning rate: lr=5e-05.
===> Epoch[168](230/324): Loss: 0.6853 || Learning rate: lr=5e-05.
===> Epoch[168](240/324): Loss: 0.8347 || Learning rate: lr=5e-05.
===> Epoch[168](250/324): Loss: 0.9546 || Learning rate: lr=5e-05.
===> Epoch[168](260/324): Loss: 0.8648 || Learning rate: lr=5e-05.
===> Epoch[168](270/324): Loss: 1.0875 || Learning rate: lr=5e-05.
===> Epoch[168](280/324): Loss: 0.8631 || Learning rate: lr=5e-05.
===> Epoch[168](290/324): Loss: 0.8632 || Learning rate: lr=5e-05.
===> Epoch[168](300/324): Loss: 0.7074 || Learning rate: lr=5e-05.
===> Epoch[168](310/324): Loss: 0.6149 || Learning rate: lr=5e-05.
===> Epoch[168](320/324): Loss: 0.5272 || Learning rate: lr=5e-05.
===> Epoch[169](10/324): Loss: 0.6972 || Learning rate: lr=5e-05.
===> Epoch[169](20/324): Loss: 0.6934 || Learning rate: lr=5e-05.
===> Epoch[169](30/324): Loss: 1.0105 || Learning rate: lr=5e-05.
===> Epoch[169](40/324): Loss: 0.8445 || Learning rate: lr=5e-05.
===> Epoch[169](50/324): Loss: 0.9115 || Learning rate: lr=5e-05.
===> Epoch[169](60/324): Loss: 0.5052 || Learning rate: lr=5e-05.
===> Epoch[169](70/324): Loss: 0.7597 || Learning rate: lr=5e-05.
===> Epoch[169](80/324): Loss: 0.9909 || Learning rate: lr=5e-05.
===> Epoch[169](90/324): Loss: 0.7275 || Learning rate: lr=5e-05.
===> Epoch[169](100/324): Loss: 0.7956 || Learning rate: lr=5e-05.
===> Epoch[169](110/324): Loss: 0.9094 || Learning rate: lr=5e-05.
===> Epoch[169](120/324): Loss: 0.5737 || Learning rate: lr=5e-05.
===> Epoch[169](130/324): Loss: 0.5740 || Learning rate: lr=5e-05.
===> Epoch[169](140/324): Loss: 0.7271 || Learning rate: lr=5e-05.
===> Epoch[169](150/324): Loss: 0.7434 || Learning rate: lr=5e-05.
===> Epoch[169](160/324): Loss: 0.6416 || Learning rate: lr=5e-05.
===> Epoch[169](170/324): Loss: 0.6219 || Learning rate: lr=5e-05.
===> Epoch[169](180/324): Loss: 0.5368 || Learning rate: lr=5e-05.
===> Epoch[169](190/324): Loss: 0.8383 || Learning rate: lr=5e-05.
===> Epoch[169](200/324): Loss: 1.0116 || Learning rate: lr=5e-05.
===> Epoch[169](210/324): Loss: 0.8211 || Learning rate: lr=5e-05.
===> Epoch[169](220/324): Loss: 0.5358 || Learning rate: lr=5e-05.
===> Epoch[169](230/324): Loss: 0.6440 || Learning rate: lr=5e-05.
===> Epoch[169](240/324): Loss: 0.5121 || Learning rate: lr=5e-05.
===> Epoch[169](250/324): Loss: 0.8486 || Learning rate: lr=5e-05.
===> Epoch[169](260/324): Loss: 1.1337 || Learning rate: lr=5e-05.
===> Epoch[169](270/324): Loss: 0.5904 || Learning rate: lr=5e-05.
===> Epoch[169](280/324): Loss: 1.0164 || Learning rate: lr=5e-05.
===> Epoch[169](290/324): Loss: 0.5809 || Learning rate: lr=5e-05.
===> Epoch[169](300/324): Loss: 0.9572 || Learning rate: lr=5e-05.
===> Epoch[169](310/324): Loss: 0.7597 || Learning rate: lr=5e-05.
===> Epoch[169](320/324): Loss: 0.9085 || Learning rate: lr=5e-05.
===> Epoch[170](10/324): Loss: 0.5411 || Learning rate: lr=5e-05.
===> Epoch[170](20/324): Loss: 0.8683 || Learning rate: lr=5e-05.
===> Epoch[170](30/324): Loss: 0.5166 || Learning rate: lr=5e-05.
===> Epoch[170](40/324): Loss: 0.7743 || Learning rate: lr=5e-05.
===> Epoch[170](50/324): Loss: 0.7692 || Learning rate: lr=5e-05.
===> Epoch[170](60/324): Loss: 0.9049 || Learning rate: lr=5e-05.
===> Epoch[170](70/324): Loss: 0.9852 || Learning rate: lr=5e-05.
===> Epoch[170](80/324): Loss: 0.5230 || Learning rate: lr=5e-05.
===> Epoch[170](90/324): Loss: 0.7721 || Learning rate: lr=5e-05.
===> Epoch[170](100/324): Loss: 0.5668 || Learning rate: lr=5e-05.
===> Epoch[170](110/324): Loss: 0.4976 || Learning rate: lr=5e-05.
===> Epoch[170](120/324): Loss: 0.6828 || Learning rate: lr=5e-05.
===> Epoch[170](130/324): Loss: 0.6137 || Learning rate: lr=5e-05.
===> Epoch[170](140/324): Loss: 0.8674 || Learning rate: lr=5e-05.
===> Epoch[170](150/324): Loss: 0.7170 || Learning rate: lr=5e-05.
===> Epoch[170](160/324): Loss: 0.7134 || Learning rate: lr=5e-05.
===> Epoch[170](170/324): Loss: 0.7162 || Learning rate: lr=5e-05.
===> Epoch[170](180/324): Loss: 0.7837 || Learning rate: lr=5e-05.
===> Epoch[170](190/324): Loss: 0.5441 || Learning rate: lr=5e-05.
===> Epoch[170](200/324): Loss: 0.7656 || Learning rate: lr=5e-05.
===> Epoch[170](210/324): Loss: 0.7852 || Learning rate: lr=5e-05.
===> Epoch[170](220/324): Loss: 0.6482 || Learning rate: lr=5e-05.
===> Epoch[170](230/324): Loss: 0.6905 || Learning rate: lr=5e-05.
===> Epoch[170](240/324): Loss: 0.6113 || Learning rate: lr=5e-05.
===> Epoch[170](250/324): Loss: 0.5256 || Learning rate: lr=5e-05.
===> Epoch[170](260/324): Loss: 0.8776 || Learning rate: lr=5e-05.
===> Epoch[170](270/324): Loss: 0.6971 || Learning rate: lr=5e-05.
===> Epoch[170](280/324): Loss: 0.7078 || Learning rate: lr=5e-05.
===> Epoch[170](290/324): Loss: 0.8512 || Learning rate: lr=5e-05.
===> Epoch[170](300/324): Loss: 0.5805 || Learning rate: lr=5e-05.
===> Epoch[170](310/324): Loss: 0.8311 || Learning rate: lr=5e-05.
===> Epoch[170](320/324): Loss: 0.9854 || Learning rate: lr=5e-05.
===> Epoch[171](10/324): Loss: 0.5753 || Learning rate: lr=5e-05.
===> Epoch[171](20/324): Loss: 0.8132 || Learning rate: lr=5e-05.
===> Epoch[171](30/324): Loss: 0.6840 || Learning rate: lr=5e-05.
===> Epoch[171](40/324): Loss: 0.7570 || Learning rate: lr=5e-05.
===> Epoch[171](50/324): Loss: 0.4778 || Learning rate: lr=5e-05.
===> Epoch[171](60/324): Loss: 0.7022 || Learning rate: lr=5e-05.
===> Epoch[171](70/324): Loss: 0.6405 || Learning rate: lr=5e-05.
===> Epoch[171](80/324): Loss: 0.7379 || Learning rate: lr=5e-05.
===> Epoch[171](90/324): Loss: 0.6307 || Learning rate: lr=5e-05.
===> Epoch[171](100/324): Loss: 0.7689 || Learning rate: lr=5e-05.
===> Epoch[171](110/324): Loss: 0.7084 || Learning rate: lr=5e-05.
===> Epoch[171](120/324): Loss: 0.6699 || Learning rate: lr=5e-05.
===> Epoch[171](130/324): Loss: 0.9927 || Learning rate: lr=5e-05.
===> Epoch[171](140/324): Loss: 0.6295 || Learning rate: lr=5e-05.
===> Epoch[171](150/324): Loss: 0.6450 || Learning rate: lr=5e-05.
===> Epoch[171](160/324): Loss: 0.8728 || Learning rate: lr=5e-05.
===> Epoch[171](170/324): Loss: 0.7019 || Learning rate: lr=5e-05.
===> Epoch[171](180/324): Loss: 0.9915 || Learning rate: lr=5e-05.
===> Epoch[171](190/324): Loss: 0.8214 || Learning rate: lr=5e-05.
===> Epoch[171](200/324): Loss: 1.0041 || Learning rate: lr=5e-05.
===> Epoch[171](210/324): Loss: 0.6493 || Learning rate: lr=5e-05.
===> Epoch[171](220/324): Loss: 0.4601 || Learning rate: lr=5e-05.
===> Epoch[171](230/324): Loss: 0.9495 || Learning rate: lr=5e-05.
===> Epoch[171](240/324): Loss: 0.8876 || Learning rate: lr=5e-05.
===> Epoch[171](250/324): Loss: 0.6870 || Learning rate: lr=5e-05.
===> Epoch[171](260/324): Loss: 0.8674 || Learning rate: lr=5e-05.
===> Epoch[171](270/324): Loss: 0.8591 || Learning rate: lr=5e-05.
===> Epoch[171](280/324): Loss: 0.6680 || Learning rate: lr=5e-05.
===> Epoch[171](290/324): Loss: 0.9673 || Learning rate: lr=5e-05.
===> Epoch[171](300/324): Loss: 0.8419 || Learning rate: lr=5e-05.
===> Epoch[171](310/324): Loss: 1.3032 || Learning rate: lr=5e-05.
===> Epoch[171](320/324): Loss: 0.9711 || Learning rate: lr=5e-05.
===> Epoch[172](10/324): Loss: 1.1909 || Learning rate: lr=5e-05.
===> Epoch[172](20/324): Loss: 0.8832 || Learning rate: lr=5e-05.
===> Epoch[172](30/324): Loss: 3.2712 || Learning rate: lr=5e-05.
===> Epoch[172](40/324): Loss: 3.9244 || Learning rate: lr=5e-05.
===> Epoch[172](50/324): Loss: 0.8003 || Learning rate: lr=5e-05.
===> Epoch[172](60/324): Loss: 2.0157 || Learning rate: lr=5e-05.
===> Epoch[172](70/324): Loss: 2.3665 || Learning rate: lr=5e-05.
===> Epoch[172](80/324): Loss: 1.0583 || Learning rate: lr=5e-05.
===> Epoch[172](90/324): Loss: 0.7257 || Learning rate: lr=5e-05.
===> Epoch[172](100/324): Loss: 0.7276 || Learning rate: lr=5e-05.
===> Epoch[172](110/324): Loss: 0.9459 || Learning rate: lr=5e-05.
===> Epoch[172](120/324): Loss: 0.7489 || Learning rate: lr=5e-05.
===> Epoch[172](130/324): Loss: 0.9644 || Learning rate: lr=5e-05.
===> Epoch[172](140/324): Loss: 0.6461 || Learning rate: lr=5e-05.
===> Epoch[172](150/324): Loss: 0.7731 || Learning rate: lr=5e-05.
===> Epoch[172](160/324): Loss: 0.7395 || Learning rate: lr=5e-05.
===> Epoch[172](170/324): Loss: 0.7267 || Learning rate: lr=5e-05.
===> Epoch[172](180/324): Loss: 0.8827 || Learning rate: lr=5e-05.
===> Epoch[172](190/324): Loss: 0.9831 || Learning rate: lr=5e-05.
===> Epoch[172](200/324): Loss: 0.6396 || Learning rate: lr=5e-05.
===> Epoch[172](210/324): Loss: 0.4864 || Learning rate: lr=5e-05.
===> Epoch[172](220/324): Loss: 1.1881 || Learning rate: lr=5e-05.
===> Epoch[172](230/324): Loss: 0.9119 || Learning rate: lr=5e-05.
===> Epoch[172](240/324): Loss: 0.6719 || Learning rate: lr=5e-05.
===> Epoch[172](250/324): Loss: 0.7723 || Learning rate: lr=5e-05.
===> Epoch[172](260/324): Loss: 0.8605 || Learning rate: lr=5e-05.
===> Epoch[172](270/324): Loss: 0.9852 || Learning rate: lr=5e-05.
===> Epoch[172](280/324): Loss: 0.7238 || Learning rate: lr=5e-05.
===> Epoch[172](290/324): Loss: 0.6455 || Learning rate: lr=5e-05.
===> Epoch[172](300/324): Loss: 0.8420 || Learning rate: lr=5e-05.
===> Epoch[172](310/324): Loss: 0.4409 || Learning rate: lr=5e-05.
===> Epoch[172](320/324): Loss: 0.5027 || Learning rate: lr=5e-05.
===> Epoch[173](10/324): Loss: 0.5328 || Learning rate: lr=5e-05.
===> Epoch[173](20/324): Loss: 0.4000 || Learning rate: lr=5e-05.
===> Epoch[173](30/324): Loss: 1.1305 || Learning rate: lr=5e-05.
===> Epoch[173](40/324): Loss: 0.9699 || Learning rate: lr=5e-05.
===> Epoch[173](50/324): Loss: 0.5016 || Learning rate: lr=5e-05.
===> Epoch[173](60/324): Loss: 0.6539 || Learning rate: lr=5e-05.
===> Epoch[173](70/324): Loss: 0.8709 || Learning rate: lr=5e-05.
===> Epoch[173](80/324): Loss: 0.7760 || Learning rate: lr=5e-05.
===> Epoch[173](90/324): Loss: 0.8235 || Learning rate: lr=5e-05.
===> Epoch[173](100/324): Loss: 0.4895 || Learning rate: lr=5e-05.
===> Epoch[173](110/324): Loss: 0.7717 || Learning rate: lr=5e-05.
===> Epoch[173](120/324): Loss: 0.7844 || Learning rate: lr=5e-05.
===> Epoch[173](130/324): Loss: 0.5934 || Learning rate: lr=5e-05.
===> Epoch[173](140/324): Loss: 0.8901 || Learning rate: lr=5e-05.
===> Epoch[173](150/324): Loss: 0.7289 || Learning rate: lr=5e-05.
===> Epoch[173](160/324): Loss: 0.8266 || Learning rate: lr=5e-05.
===> Epoch[173](170/324): Loss: 0.6588 || Learning rate: lr=5e-05.
===> Epoch[173](180/324): Loss: 0.6703 || Learning rate: lr=5e-05.
===> Epoch[173](190/324): Loss: 0.9803 || Learning rate: lr=5e-05.
===> Epoch[173](200/324): Loss: 0.8580 || Learning rate: lr=5e-05.
===> Epoch[173](210/324): Loss: 0.6649 || Learning rate: lr=5e-05.
===> Epoch[173](220/324): Loss: 0.6447 || Learning rate: lr=5e-05.
===> Epoch[173](230/324): Loss: 1.1662 || Learning rate: lr=5e-05.
===> Epoch[173](240/324): Loss: 0.7966 || Learning rate: lr=5e-05.
===> Epoch[173](250/324): Loss: 0.7625 || Learning rate: lr=5e-05.
===> Epoch[173](260/324): Loss: 0.6365 || Learning rate: lr=5e-05.
===> Epoch[173](270/324): Loss: 0.6249 || Learning rate: lr=5e-05.
===> Epoch[173](280/324): Loss: 0.6719 || Learning rate: lr=5e-05.
===> Epoch[173](290/324): Loss: 0.6108 || Learning rate: lr=5e-05.
===> Epoch[173](300/324): Loss: 0.7135 || Learning rate: lr=5e-05.
===> Epoch[173](310/324): Loss: 0.6819 || Learning rate: lr=5e-05.
===> Epoch[173](320/324): Loss: 0.6905 || Learning rate: lr=5e-05.
===> Epoch[174](10/324): Loss: 0.5982 || Learning rate: lr=5e-05.
===> Epoch[174](20/324): Loss: 0.6962 || Learning rate: lr=5e-05.
===> Epoch[174](30/324): Loss: 0.8378 || Learning rate: lr=5e-05.
===> Epoch[174](40/324): Loss: 0.6583 || Learning rate: lr=5e-05.
===> Epoch[174](50/324): Loss: 0.6028 || Learning rate: lr=5e-05.
===> Epoch[174](60/324): Loss: 0.6553 || Learning rate: lr=5e-05.
===> Epoch[174](70/324): Loss: 0.9333 || Learning rate: lr=5e-05.
===> Epoch[174](80/324): Loss: 1.0755 || Learning rate: lr=5e-05.
===> Epoch[174](90/324): Loss: 0.8858 || Learning rate: lr=5e-05.
===> Epoch[174](100/324): Loss: 0.5186 || Learning rate: lr=5e-05.
===> Epoch[174](110/324): Loss: 0.7765 || Learning rate: lr=5e-05.
===> Epoch[174](120/324): Loss: 0.7874 || Learning rate: lr=5e-05.
===> Epoch[174](130/324): Loss: 0.8600 || Learning rate: lr=5e-05.
===> Epoch[174](140/324): Loss: 0.6350 || Learning rate: lr=5e-05.
===> Epoch[174](150/324): Loss: 0.9549 || Learning rate: lr=5e-05.
===> Epoch[174](160/324): Loss: 0.7346 || Learning rate: lr=5e-05.
===> Epoch[174](170/324): Loss: 0.5474 || Learning rate: lr=5e-05.
===> Epoch[174](180/324): Loss: 0.6783 || Learning rate: lr=5e-05.
===> Epoch[174](190/324): Loss: 0.7560 || Learning rate: lr=5e-05.
===> Epoch[174](200/324): Loss: 0.4887 || Learning rate: lr=5e-05.
===> Epoch[174](210/324): Loss: 0.6925 || Learning rate: lr=5e-05.
===> Epoch[174](220/324): Loss: 0.6511 || Learning rate: lr=5e-05.
===> Epoch[174](230/324): Loss: 0.8338 || Learning rate: lr=5e-05.
===> Epoch[174](240/324): Loss: 0.9088 || Learning rate: lr=5e-05.
===> Epoch[174](250/324): Loss: 0.7249 || Learning rate: lr=5e-05.
===> Epoch[174](260/324): Loss: 1.0089 || Learning rate: lr=5e-05.
===> Epoch[174](270/324): Loss: 0.6576 || Learning rate: lr=5e-05.
===> Epoch[174](280/324): Loss: 0.7476 || Learning rate: lr=5e-05.
===> Epoch[174](290/324): Loss: 0.9870 || Learning rate: lr=5e-05.
===> Epoch[174](300/324): Loss: 0.7140 || Learning rate: lr=5e-05.
===> Epoch[174](310/324): Loss: 0.9029 || Learning rate: lr=5e-05.
===> Epoch[174](320/324): Loss: 0.7603 || Learning rate: lr=5e-05.
===> Epoch[175](10/324): Loss: 1.0487 || Learning rate: lr=5e-05.
===> Epoch[175](20/324): Loss: 0.8626 || Learning rate: lr=5e-05.
===> Epoch[175](30/324): Loss: 0.8967 || Learning rate: lr=5e-05.
===> Epoch[175](40/324): Loss: 0.4974 || Learning rate: lr=5e-05.
===> Epoch[175](50/324): Loss: 0.5324 || Learning rate: lr=5e-05.
===> Epoch[175](60/324): Loss: 0.5078 || Learning rate: lr=5e-05.
===> Epoch[175](70/324): Loss: 0.5924 || Learning rate: lr=5e-05.
===> Epoch[175](80/324): Loss: 0.5520 || Learning rate: lr=5e-05.
===> Epoch[175](90/324): Loss: 0.7804 || Learning rate: lr=5e-05.
===> Epoch[175](100/324): Loss: 0.7552 || Learning rate: lr=5e-05.
===> Epoch[175](110/324): Loss: 0.7456 || Learning rate: lr=5e-05.
===> Epoch[175](120/324): Loss: 0.6947 || Learning rate: lr=5e-05.
===> Epoch[175](130/324): Loss: 0.8107 || Learning rate: lr=5e-05.
===> Epoch[175](140/324): Loss: 0.7876 || Learning rate: lr=5e-05.
===> Epoch[175](150/324): Loss: 0.8273 || Learning rate: lr=5e-05.
===> Epoch[175](160/324): Loss: 1.0057 || Learning rate: lr=5e-05.
===> Epoch[175](170/324): Loss: 0.7647 || Learning rate: lr=5e-05.
===> Epoch[175](180/324): Loss: 0.8991 || Learning rate: lr=5e-05.
===> Epoch[175](190/324): Loss: 0.6924 || Learning rate: lr=5e-05.
===> Epoch[175](200/324): Loss: 0.9989 || Learning rate: lr=5e-05.
===> Epoch[175](210/324): Loss: 0.7395 || Learning rate: lr=5e-05.
===> Epoch[175](220/324): Loss: 0.6508 || Learning rate: lr=5e-05.
===> Epoch[175](230/324): Loss: 0.6626 || Learning rate: lr=5e-05.
===> Epoch[175](240/324): Loss: 0.7404 || Learning rate: lr=5e-05.
===> Epoch[175](250/324): Loss: 0.8037 || Learning rate: lr=5e-05.
===> Epoch[175](260/324): Loss: 0.5340 || Learning rate: lr=5e-05.
===> Epoch[175](270/324): Loss: 0.7731 || Learning rate: lr=5e-05.
===> Epoch[175](280/324): Loss: 0.6064 || Learning rate: lr=5e-05.
===> Epoch[175](290/324): Loss: 0.5822 || Learning rate: lr=5e-05.
===> Epoch[175](300/324): Loss: 0.7431 || Learning rate: lr=5e-05.
===> Epoch[175](310/324): Loss: 0.6424 || Learning rate: lr=5e-05.
===> Epoch[175](320/324): Loss: 0.9177 || Learning rate: lr=5e-05.
===> Epoch[176](10/324): Loss: 0.5771 || Learning rate: lr=5e-05.
===> Epoch[176](20/324): Loss: 0.8007 || Learning rate: lr=5e-05.
===> Epoch[176](30/324): Loss: 0.8043 || Learning rate: lr=5e-05.
===> Epoch[176](40/324): Loss: 0.9628 || Learning rate: lr=5e-05.
===> Epoch[176](50/324): Loss: 0.6951 || Learning rate: lr=5e-05.
===> Epoch[176](60/324): Loss: 0.6580 || Learning rate: lr=5e-05.
===> Epoch[176](70/324): Loss: 0.9736 || Learning rate: lr=5e-05.
===> Epoch[176](80/324): Loss: 0.4807 || Learning rate: lr=5e-05.
===> Epoch[176](90/324): Loss: 0.4397 || Learning rate: lr=5e-05.
===> Epoch[176](100/324): Loss: 0.9229 || Learning rate: lr=5e-05.
===> Epoch[176](110/324): Loss: 0.7603 || Learning rate: lr=5e-05.
===> Epoch[176](120/324): Loss: 0.8184 || Learning rate: lr=5e-05.
===> Epoch[176](130/324): Loss: 0.9039 || Learning rate: lr=5e-05.
===> Epoch[176](140/324): Loss: 0.8534 || Learning rate: lr=5e-05.
===> Epoch[176](150/324): Loss: 0.5394 || Learning rate: lr=5e-05.
===> Epoch[176](160/324): Loss: 0.8149 || Learning rate: lr=5e-05.
===> Epoch[176](170/324): Loss: 0.7314 || Learning rate: lr=5e-05.
===> Epoch[176](180/324): Loss: 0.6556 || Learning rate: lr=5e-05.
===> Epoch[176](190/324): Loss: 0.6357 || Learning rate: lr=5e-05.
===> Epoch[176](200/324): Loss: 0.5668 || Learning rate: lr=5e-05.
===> Epoch[176](210/324): Loss: 0.7066 || Learning rate: lr=5e-05.
===> Epoch[176](220/324): Loss: 0.4823 || Learning rate: lr=5e-05.
===> Epoch[176](230/324): Loss: 0.6407 || Learning rate: lr=5e-05.
===> Epoch[176](240/324): Loss: 0.5298 || Learning rate: lr=5e-05.
===> Epoch[176](250/324): Loss: 1.0063 || Learning rate: lr=5e-05.
===> Epoch[176](260/324): Loss: 0.6436 || Learning rate: lr=5e-05.
===> Epoch[176](270/324): Loss: 1.1110 || Learning rate: lr=5e-05.
===> Epoch[176](280/324): Loss: 1.0939 || Learning rate: lr=5e-05.
===> Epoch[176](290/324): Loss: 0.7834 || Learning rate: lr=5e-05.
===> Epoch[176](300/324): Loss: 1.0536 || Learning rate: lr=5e-05.
===> Epoch[176](310/324): Loss: 0.5621 || Learning rate: lr=5e-05.
===> Epoch[176](320/324): Loss: 0.7056 || Learning rate: lr=5e-05.
===> Epoch[177](10/324): Loss: 0.7300 || Learning rate: lr=5e-05.
===> Epoch[177](20/324): Loss: 0.5815 || Learning rate: lr=5e-05.
===> Epoch[177](30/324): Loss: 0.6931 || Learning rate: lr=5e-05.
===> Epoch[177](40/324): Loss: 0.7483 || Learning rate: lr=5e-05.
===> Epoch[177](50/324): Loss: 0.9419 || Learning rate: lr=5e-05.
===> Epoch[177](60/324): Loss: 0.5587 || Learning rate: lr=5e-05.
===> Epoch[177](70/324): Loss: 0.6281 || Learning rate: lr=5e-05.
===> Epoch[177](80/324): Loss: 0.7594 || Learning rate: lr=5e-05.
===> Epoch[177](90/324): Loss: 0.7158 || Learning rate: lr=5e-05.
===> Epoch[177](100/324): Loss: 0.8716 || Learning rate: lr=5e-05.
===> Epoch[177](110/324): Loss: 0.3800 || Learning rate: lr=5e-05.
===> Epoch[177](120/324): Loss: 1.1910 || Learning rate: lr=5e-05.
===> Epoch[177](130/324): Loss: 0.7537 || Learning rate: lr=5e-05.
===> Epoch[177](140/324): Loss: 0.7798 || Learning rate: lr=5e-05.
===> Epoch[177](150/324): Loss: 0.8192 || Learning rate: lr=5e-05.
===> Epoch[177](160/324): Loss: 0.5974 || Learning rate: lr=5e-05.
===> Epoch[177](170/324): Loss: 0.9395 || Learning rate: lr=5e-05.
===> Epoch[177](180/324): Loss: 1.0043 || Learning rate: lr=5e-05.
===> Epoch[177](190/324): Loss: 0.6175 || Learning rate: lr=5e-05.
===> Epoch[177](200/324): Loss: 0.4994 || Learning rate: lr=5e-05.
===> Epoch[177](210/324): Loss: 0.7845 || Learning rate: lr=5e-05.
===> Epoch[177](220/324): Loss: 0.9737 || Learning rate: lr=5e-05.
===> Epoch[177](230/324): Loss: 0.8587 || Learning rate: lr=5e-05.
===> Epoch[177](240/324): Loss: 1.0957 || Learning rate: lr=5e-05.
===> Epoch[177](250/324): Loss: 0.7272 || Learning rate: lr=5e-05.
===> Epoch[177](260/324): Loss: 0.6316 || Learning rate: lr=5e-05.
===> Epoch[177](270/324): Loss: 0.7079 || Learning rate: lr=5e-05.
===> Epoch[177](280/324): Loss: 0.6475 || Learning rate: lr=5e-05.
===> Epoch[177](290/324): Loss: 0.5528 || Learning rate: lr=5e-05.
===> Epoch[177](300/324): Loss: 0.5641 || Learning rate: lr=5e-05.
===> Epoch[177](310/324): Loss: 0.6329 || Learning rate: lr=5e-05.
===> Epoch[177](320/324): Loss: 1.0117 || Learning rate: lr=5e-05.
===> Epoch[178](10/324): Loss: 0.7310 || Learning rate: lr=5e-05.
===> Epoch[178](20/324): Loss: 0.5616 || Learning rate: lr=5e-05.
===> Epoch[178](30/324): Loss: 0.7379 || Learning rate: lr=5e-05.
===> Epoch[178](40/324): Loss: 0.5388 || Learning rate: lr=5e-05.
===> Epoch[178](50/324): Loss: 0.7717 || Learning rate: lr=5e-05.
===> Epoch[178](60/324): Loss: 0.7501 || Learning rate: lr=5e-05.
===> Epoch[178](70/324): Loss: 0.6785 || Learning rate: lr=5e-05.
===> Epoch[178](80/324): Loss: 0.5327 || Learning rate: lr=5e-05.
===> Epoch[178](90/324): Loss: 1.0650 || Learning rate: lr=5e-05.
===> Epoch[178](100/324): Loss: 0.6737 || Learning rate: lr=5e-05.
===> Epoch[178](110/324): Loss: 0.5831 || Learning rate: lr=5e-05.
===> Epoch[178](120/324): Loss: 0.6525 || Learning rate: lr=5e-05.
===> Epoch[178](130/324): Loss: 0.5247 || Learning rate: lr=5e-05.
===> Epoch[178](140/324): Loss: 0.6206 || Learning rate: lr=5e-05.
===> Epoch[178](150/324): Loss: 0.9092 || Learning rate: lr=5e-05.
===> Epoch[178](160/324): Loss: 0.9581 || Learning rate: lr=5e-05.
===> Epoch[178](170/324): Loss: 0.6886 || Learning rate: lr=5e-05.
===> Epoch[178](180/324): Loss: 0.6028 || Learning rate: lr=5e-05.
===> Epoch[178](190/324): Loss: 0.5564 || Learning rate: lr=5e-05.
===> Epoch[178](200/324): Loss: 0.6554 || Learning rate: lr=5e-05.
===> Epoch[178](210/324): Loss: 0.9074 || Learning rate: lr=5e-05.
===> Epoch[178](220/324): Loss: 0.7137 || Learning rate: lr=5e-05.
===> Epoch[178](230/324): Loss: 0.7460 || Learning rate: lr=5e-05.
===> Epoch[178](240/324): Loss: 1.0637 || Learning rate: lr=5e-05.
===> Epoch[178](250/324): Loss: 0.5856 || Learning rate: lr=5e-05.
===> Epoch[178](260/324): Loss: 0.5917 || Learning rate: lr=5e-05.
===> Epoch[178](270/324): Loss: 0.8341 || Learning rate: lr=5e-05.
===> Epoch[178](280/324): Loss: 1.0566 || Learning rate: lr=5e-05.
===> Epoch[178](290/324): Loss: 0.6180 || Learning rate: lr=5e-05.
===> Epoch[178](300/324): Loss: 0.5959 || Learning rate: lr=5e-05.
===> Epoch[178](310/324): Loss: 0.8493 || Learning rate: lr=5e-05.
===> Epoch[178](320/324): Loss: 0.6696 || Learning rate: lr=5e-05.
===> Epoch[179](10/324): Loss: 0.6820 || Learning rate: lr=5e-05.
===> Epoch[179](20/324): Loss: 0.7890 || Learning rate: lr=5e-05.
===> Epoch[179](30/324): Loss: 0.5152 || Learning rate: lr=5e-05.
===> Epoch[179](40/324): Loss: 0.7721 || Learning rate: lr=5e-05.
===> Epoch[179](50/324): Loss: 0.6626 || Learning rate: lr=5e-05.
===> Epoch[179](60/324): Loss: 1.0144 || Learning rate: lr=5e-05.
===> Epoch[179](70/324): Loss: 0.5639 || Learning rate: lr=5e-05.
===> Epoch[179](80/324): Loss: 0.5839 || Learning rate: lr=5e-05.
===> Epoch[179](90/324): Loss: 0.5784 || Learning rate: lr=5e-05.
===> Epoch[179](100/324): Loss: 0.7015 || Learning rate: lr=5e-05.
===> Epoch[179](110/324): Loss: 0.5977 || Learning rate: lr=5e-05.
===> Epoch[179](120/324): Loss: 0.7175 || Learning rate: lr=5e-05.
===> Epoch[179](130/324): Loss: 0.6917 || Learning rate: lr=5e-05.
===> Epoch[179](140/324): Loss: 0.8818 || Learning rate: lr=5e-05.
===> Epoch[179](150/324): Loss: 0.6236 || Learning rate: lr=5e-05.
===> Epoch[179](160/324): Loss: 0.4330 || Learning rate: lr=5e-05.
===> Epoch[179](170/324): Loss: 0.6727 || Learning rate: lr=5e-05.
===> Epoch[179](180/324): Loss: 0.9158 || Learning rate: lr=5e-05.
===> Epoch[179](190/324): Loss: 0.9493 || Learning rate: lr=5e-05.
===> Epoch[179](200/324): Loss: 0.7535 || Learning rate: lr=5e-05.
===> Epoch[179](210/324): Loss: 0.6394 || Learning rate: lr=5e-05.
===> Epoch[179](220/324): Loss: 0.6793 || Learning rate: lr=5e-05.
===> Epoch[179](230/324): Loss: 1.0881 || Learning rate: lr=5e-05.
===> Epoch[179](240/324): Loss: 1.0320 || Learning rate: lr=5e-05.
===> Epoch[179](250/324): Loss: 0.6067 || Learning rate: lr=5e-05.
===> Epoch[179](260/324): Loss: 0.6776 || Learning rate: lr=5e-05.
===> Epoch[179](270/324): Loss: 0.6290 || Learning rate: lr=5e-05.
===> Epoch[179](280/324): Loss: 1.0523 || Learning rate: lr=5e-05.
===> Epoch[179](290/324): Loss: 0.6073 || Learning rate: lr=5e-05.
===> Epoch[179](300/324): Loss: 0.8562 || Learning rate: lr=5e-05.
===> Epoch[179](310/324): Loss: 0.8740 || Learning rate: lr=5e-05.
===> Epoch[179](320/324): Loss: 0.8014 || Learning rate: lr=5e-05.
===> Epoch[180](10/324): Loss: 0.8282 || Learning rate: lr=5e-05.
===> Epoch[180](20/324): Loss: 0.4394 || Learning rate: lr=5e-05.
===> Epoch[180](30/324): Loss: 0.6080 || Learning rate: lr=5e-05.
===> Epoch[180](40/324): Loss: 0.6698 || Learning rate: lr=5e-05.
===> Epoch[180](50/324): Loss: 0.7362 || Learning rate: lr=5e-05.
===> Epoch[180](60/324): Loss: 0.7015 || Learning rate: lr=5e-05.
===> Epoch[180](70/324): Loss: 0.5808 || Learning rate: lr=5e-05.
===> Epoch[180](80/324): Loss: 0.6400 || Learning rate: lr=5e-05.
===> Epoch[180](90/324): Loss: 0.7956 || Learning rate: lr=5e-05.
===> Epoch[180](100/324): Loss: 0.8378 || Learning rate: lr=5e-05.
===> Epoch[180](110/324): Loss: 0.8239 || Learning rate: lr=5e-05.
===> Epoch[180](120/324): Loss: 0.6897 || Learning rate: lr=5e-05.
===> Epoch[180](130/324): Loss: 0.6330 || Learning rate: lr=5e-05.
===> Epoch[180](140/324): Loss: 0.4554 || Learning rate: lr=5e-05.
===> Epoch[180](150/324): Loss: 0.7629 || Learning rate: lr=5e-05.
===> Epoch[180](160/324): Loss: 0.3784 || Learning rate: lr=5e-05.
===> Epoch[180](170/324): Loss: 0.6000 || Learning rate: lr=5e-05.
===> Epoch[180](180/324): Loss: 1.1408 || Learning rate: lr=5e-05.
===> Epoch[180](190/324): Loss: 0.5309 || Learning rate: lr=5e-05.
===> Epoch[180](200/324): Loss: 0.7460 || Learning rate: lr=5e-05.
===> Epoch[180](210/324): Loss: 0.9099 || Learning rate: lr=5e-05.
===> Epoch[180](220/324): Loss: 0.7921 || Learning rate: lr=5e-05.
===> Epoch[180](230/324): Loss: 0.8337 || Learning rate: lr=5e-05.
===> Epoch[180](240/324): Loss: 1.0958 || Learning rate: lr=5e-05.
===> Epoch[180](250/324): Loss: 0.8082 || Learning rate: lr=5e-05.
===> Epoch[180](260/324): Loss: 0.8092 || Learning rate: lr=5e-05.
===> Epoch[180](270/324): Loss: 0.8291 || Learning rate: lr=5e-05.
===> Epoch[180](280/324): Loss: 0.8955 || Learning rate: lr=5e-05.
===> Epoch[180](290/324): Loss: 0.5885 || Learning rate: lr=5e-05.
===> Epoch[180](300/324): Loss: 0.7323 || Learning rate: lr=5e-05.
===> Epoch[180](310/324): Loss: 0.8466 || Learning rate: lr=5e-05.
===> Epoch[180](320/324): Loss: 0.5261 || Learning rate: lr=5e-05.
Checkpoint saved to weights/epoch_v2_180.pth
===> Epoch[181](10/324): Loss: 0.6517 || Learning rate: lr=5e-05.
===> Epoch[181](20/324): Loss: 0.6414 || Learning rate: lr=5e-05.
===> Epoch[181](30/324): Loss: 0.6103 || Learning rate: lr=5e-05.
===> Epoch[181](40/324): Loss: 0.7711 || Learning rate: lr=5e-05.
===> Epoch[181](50/324): Loss: 0.8719 || Learning rate: lr=5e-05.
===> Epoch[181](60/324): Loss: 0.8316 || Learning rate: lr=5e-05.
===> Epoch[181](70/324): Loss: 0.6840 || Learning rate: lr=5e-05.
===> Epoch[181](80/324): Loss: 0.7893 || Learning rate: lr=5e-05.
===> Epoch[181](90/324): Loss: 0.6043 || Learning rate: lr=5e-05.
===> Epoch[181](100/324): Loss: 0.6344 || Learning rate: lr=5e-05.
===> Epoch[181](110/324): Loss: 0.7889 || Learning rate: lr=5e-05.
===> Epoch[181](120/324): Loss: 0.7560 || Learning rate: lr=5e-05.
===> Epoch[181](130/324): Loss: 0.8441 || Learning rate: lr=5e-05.
===> Epoch[181](140/324): Loss: 1.0125 || Learning rate: lr=5e-05.
===> Epoch[181](150/324): Loss: 0.9302 || Learning rate: lr=5e-05.
===> Epoch[181](160/324): Loss: 0.6136 || Learning rate: lr=5e-05.
===> Epoch[181](170/324): Loss: 0.7133 || Learning rate: lr=5e-05.
===> Epoch[181](180/324): Loss: 1.4577 || Learning rate: lr=5e-05.
===> Epoch[181](190/324): Loss: 0.7474 || Learning rate: lr=5e-05.
===> Epoch[181](200/324): Loss: 0.8078 || Learning rate: lr=5e-05.
===> Epoch[181](210/324): Loss: 1.0238 || Learning rate: lr=5e-05.
===> Epoch[181](220/324): Loss: 0.4591 || Learning rate: lr=5e-05.
===> Epoch[181](230/324): Loss: 0.5271 || Learning rate: lr=5e-05.
===> Epoch[181](240/324): Loss: 0.8111 || Learning rate: lr=5e-05.
===> Epoch[181](250/324): Loss: 0.6675 || Learning rate: lr=5e-05.
===> Epoch[181](260/324): Loss: 0.6085 || Learning rate: lr=5e-05.
===> Epoch[181](270/324): Loss: 0.6119 || Learning rate: lr=5e-05.
===> Epoch[181](280/324): Loss: 0.6139 || Learning rate: lr=5e-05.
===> Epoch[181](290/324): Loss: 0.9126 || Learning rate: lr=5e-05.
===> Epoch[181](300/324): Loss: 0.7969 || Learning rate: lr=5e-05.
===> Epoch[181](310/324): Loss: 0.7978 || Learning rate: lr=5e-05.
===> Epoch[181](320/324): Loss: 0.6416 || Learning rate: lr=5e-05.
===> Epoch[182](10/324): Loss: 1.0029 || Learning rate: lr=5e-05.
===> Epoch[182](20/324): Loss: 0.7759 || Learning rate: lr=5e-05.
===> Epoch[182](30/324): Loss: 1.0168 || Learning rate: lr=5e-05.
===> Epoch[182](40/324): Loss: 0.6306 || Learning rate: lr=5e-05.
===> Epoch[182](50/324): Loss: 0.7134 || Learning rate: lr=5e-05.
===> Epoch[182](60/324): Loss: 0.8127 || Learning rate: lr=5e-05.
===> Epoch[182](70/324): Loss: 0.4839 || Learning rate: lr=5e-05.
===> Epoch[182](80/324): Loss: 0.8028 || Learning rate: lr=5e-05.
===> Epoch[182](90/324): Loss: 0.6864 || Learning rate: lr=5e-05.
===> Epoch[182](100/324): Loss: 0.7474 || Learning rate: lr=5e-05.
===> Epoch[182](110/324): Loss: 0.8304 || Learning rate: lr=5e-05.
===> Epoch[182](120/324): Loss: 0.7016 || Learning rate: lr=5e-05.
===> Epoch[182](130/324): Loss: 0.4574 || Learning rate: lr=5e-05.
===> Epoch[182](140/324): Loss: 0.7101 || Learning rate: lr=5e-05.
===> Epoch[182](150/324): Loss: 0.6656 || Learning rate: lr=5e-05.
===> Epoch[182](160/324): Loss: 0.6376 || Learning rate: lr=5e-05.
===> Epoch[182](170/324): Loss: 0.7219 || Learning rate: lr=5e-05.
===> Epoch[182](180/324): Loss: 0.7081 || Learning rate: lr=5e-05.
===> Epoch[182](190/324): Loss: 0.7813 || Learning rate: lr=5e-05.
===> Epoch[182](200/324): Loss: 0.6905 || Learning rate: lr=5e-05.
===> Epoch[182](210/324): Loss: 0.6117 || Learning rate: lr=5e-05.
===> Epoch[182](220/324): Loss: 0.9336 || Learning rate: lr=5e-05.
===> Epoch[182](230/324): Loss: 0.9492 || Learning rate: lr=5e-05.
===> Epoch[182](240/324): Loss: 0.6383 || Learning rate: lr=5e-05.
===> Epoch[182](250/324): Loss: 0.5436 || Learning rate: lr=5e-05.
===> Epoch[182](260/324): Loss: 0.8175 || Learning rate: lr=5e-05.
===> Epoch[182](270/324): Loss: 0.6661 || Learning rate: lr=5e-05.
===> Epoch[182](280/324): Loss: 0.8739 || Learning rate: lr=5e-05.
===> Epoch[182](290/324): Loss: 1.0295 || Learning rate: lr=5e-05.
===> Epoch[182](300/324): Loss: 0.5959 || Learning rate: lr=5e-05.
===> Epoch[182](310/324): Loss: 1.0125 || Learning rate: lr=5e-05.
===> Epoch[182](320/324): Loss: 0.8616 || Learning rate: lr=5e-05.
===> Epoch[183](10/324): Loss: 0.7057 || Learning rate: lr=5e-05.
===> Epoch[183](20/324): Loss: 0.6794 || Learning rate: lr=5e-05.
===> Epoch[183](30/324): Loss: 0.7292 || Learning rate: lr=5e-05.
===> Epoch[183](40/324): Loss: 0.7976 || Learning rate: lr=5e-05.
===> Epoch[183](50/324): Loss: 0.7613 || Learning rate: lr=5e-05.
===> Epoch[183](60/324): Loss: 0.7120 || Learning rate: lr=5e-05.
===> Epoch[183](70/324): Loss: 0.7214 || Learning rate: lr=5e-05.
===> Epoch[183](80/324): Loss: 0.6068 || Learning rate: lr=5e-05.
===> Epoch[183](90/324): Loss: 0.7450 || Learning rate: lr=5e-05.
===> Epoch[183](100/324): Loss: 0.7600 || Learning rate: lr=5e-05.
===> Epoch[183](110/324): Loss: 0.9328 || Learning rate: lr=5e-05.
===> Epoch[183](120/324): Loss: 1.0079 || Learning rate: lr=5e-05.
===> Epoch[183](130/324): Loss: 0.7346 || Learning rate: lr=5e-05.
===> Epoch[183](140/324): Loss: 0.5207 || Learning rate: lr=5e-05.
===> Epoch[183](150/324): Loss: 0.9208 || Learning rate: lr=5e-05.
===> Epoch[183](160/324): Loss: 0.6507 || Learning rate: lr=5e-05.
===> Epoch[183](170/324): Loss: 0.7348 || Learning rate: lr=5e-05.
===> Epoch[183](180/324): Loss: 0.9039 || Learning rate: lr=5e-05.
===> Epoch[183](190/324): Loss: 0.4283 || Learning rate: lr=5e-05.
===> Epoch[183](200/324): Loss: 0.6677 || Learning rate: lr=5e-05.
===> Epoch[183](210/324): Loss: 0.8226 || Learning rate: lr=5e-05.
===> Epoch[183](220/324): Loss: 0.6637 || Learning rate: lr=5e-05.
===> Epoch[183](230/324): Loss: 0.6543 || Learning rate: lr=5e-05.
===> Epoch[183](240/324): Loss: 0.7559 || Learning rate: lr=5e-05.
===> Epoch[183](250/324): Loss: 0.6160 || Learning rate: lr=5e-05.
===> Epoch[183](260/324): Loss: 0.6658 || Learning rate: lr=5e-05.
===> Epoch[183](270/324): Loss: 0.7511 || Learning rate: lr=5e-05.
===> Epoch[183](280/324): Loss: 0.5566 || Learning rate: lr=5e-05.
===> Epoch[183](290/324): Loss: 0.5064 || Learning rate: lr=5e-05.
===> Epoch[183](300/324): Loss: 0.5474 || Learning rate: lr=5e-05.
===> Epoch[183](310/324): Loss: 0.5572 || Learning rate: lr=5e-05.
===> Epoch[183](320/324): Loss: 0.9774 || Learning rate: lr=5e-05.
===> Epoch[184](10/324): Loss: 0.9174 || Learning rate: lr=5e-05.
===> Epoch[184](20/324): Loss: 0.8672 || Learning rate: lr=5e-05.
===> Epoch[184](30/324): Loss: 0.7928 || Learning rate: lr=5e-05.
===> Epoch[184](40/324): Loss: 0.5807 || Learning rate: lr=5e-05.
===> Epoch[184](50/324): Loss: 0.6288 || Learning rate: lr=5e-05.
===> Epoch[184](60/324): Loss: 1.1022 || Learning rate: lr=5e-05.
===> Epoch[184](70/324): Loss: 0.7850 || Learning rate: lr=5e-05.
===> Epoch[184](80/324): Loss: 0.6398 || Learning rate: lr=5e-05.
===> Epoch[184](90/324): Loss: 0.6230 || Learning rate: lr=5e-05.
===> Epoch[184](100/324): Loss: 0.6402 || Learning rate: lr=5e-05.
===> Epoch[184](110/324): Loss: 0.7819 || Learning rate: lr=5e-05.
===> Epoch[184](120/324): Loss: 0.8592 || Learning rate: lr=5e-05.
===> Epoch[184](130/324): Loss: 0.6287 || Learning rate: lr=5e-05.
===> Epoch[184](140/324): Loss: 0.7327 || Learning rate: lr=5e-05.
===> Epoch[184](150/324): Loss: 1.0228 || Learning rate: lr=5e-05.
===> Epoch[184](160/324): Loss: 0.6460 || Learning rate: lr=5e-05.
===> Epoch[184](170/324): Loss: 0.9950 || Learning rate: lr=5e-05.
===> Epoch[184](180/324): Loss: 1.0118 || Learning rate: lr=5e-05.
===> Epoch[184](190/324): Loss: 0.8054 || Learning rate: lr=5e-05.
===> Epoch[184](200/324): Loss: 0.7234 || Learning rate: lr=5e-05.
===> Epoch[184](210/324): Loss: 0.6473 || Learning rate: lr=5e-05.
===> Epoch[184](220/324): Loss: 0.9071 || Learning rate: lr=5e-05.
===> Epoch[184](230/324): Loss: 0.7085 || Learning rate: lr=5e-05.
===> Epoch[184](240/324): Loss: 0.9879 || Learning rate: lr=5e-05.
===> Epoch[184](250/324): Loss: 0.6516 || Learning rate: lr=5e-05.
===> Epoch[184](260/324): Loss: 0.8039 || Learning rate: lr=5e-05.
===> Epoch[184](270/324): Loss: 0.4258 || Learning rate: lr=5e-05.
===> Epoch[184](280/324): Loss: 0.6915 || Learning rate: lr=5e-05.
===> Epoch[184](290/324): Loss: 0.6211 || Learning rate: lr=5e-05.
===> Epoch[184](300/324): Loss: 0.6587 || Learning rate: lr=5e-05.
===> Epoch[184](310/324): Loss: 0.8662 || Learning rate: lr=5e-05.
===> Epoch[184](320/324): Loss: 0.5216 || Learning rate: lr=5e-05.
===> Epoch[185](10/324): Loss: 0.6236 || Learning rate: lr=5e-05.
===> Epoch[185](20/324): Loss: 0.7210 || Learning rate: lr=5e-05.
===> Epoch[185](30/324): Loss: 1.0545 || Learning rate: lr=5e-05.
===> Epoch[185](40/324): Loss: 0.9020 || Learning rate: lr=5e-05.
===> Epoch[185](50/324): Loss: 1.0067 || Learning rate: lr=5e-05.
===> Epoch[185](60/324): Loss: 0.6625 || Learning rate: lr=5e-05.
===> Epoch[185](70/324): Loss: 0.6390 || Learning rate: lr=5e-05.
===> Epoch[185](80/324): Loss: 0.5309 || Learning rate: lr=5e-05.
===> Epoch[185](90/324): Loss: 0.6459 || Learning rate: lr=5e-05.
===> Epoch[185](100/324): Loss: 0.8822 || Learning rate: lr=5e-05.
===> Epoch[185](110/324): Loss: 0.9894 || Learning rate: lr=5e-05.
===> Epoch[185](120/324): Loss: 0.5407 || Learning rate: lr=5e-05.
===> Epoch[185](130/324): Loss: 0.7802 || Learning rate: lr=5e-05.
===> Epoch[185](140/324): Loss: 0.8767 || Learning rate: lr=5e-05.
===> Epoch[185](150/324): Loss: 0.8479 || Learning rate: lr=5e-05.
===> Epoch[185](160/324): Loss: 0.5550 || Learning rate: lr=5e-05.
===> Epoch[185](170/324): Loss: 0.8350 || Learning rate: lr=5e-05.
===> Epoch[185](180/324): Loss: 0.6416 || Learning rate: lr=5e-05.
===> Epoch[185](190/324): Loss: 0.5398 || Learning rate: lr=5e-05.
===> Epoch[185](200/324): Loss: 0.6917 || Learning rate: lr=5e-05.
===> Epoch[185](210/324): Loss: 1.1959 || Learning rate: lr=5e-05.
===> Epoch[185](220/324): Loss: 1.2015 || Learning rate: lr=5e-05.
===> Epoch[185](230/324): Loss: 0.6208 || Learning rate: lr=5e-05.
===> Epoch[185](240/324): Loss: 0.4453 || Learning rate: lr=5e-05.
===> Epoch[185](250/324): Loss: 0.9074 || Learning rate: lr=5e-05.
===> Epoch[185](260/324): Loss: 0.7395 || Learning rate: lr=5e-05.
===> Epoch[185](270/324): Loss: 0.6990 || Learning rate: lr=5e-05.
===> Epoch[185](280/324): Loss: 0.6244 || Learning rate: lr=5e-05.
===> Epoch[185](290/324): Loss: 0.8038 || Learning rate: lr=5e-05.
===> Epoch[185](300/324): Loss: 0.8783 || Learning rate: lr=5e-05.
===> Epoch[185](310/324): Loss: 0.7386 || Learning rate: lr=5e-05.
===> Epoch[185](320/324): Loss: 1.3133 || Learning rate: lr=5e-05.
===> Epoch[186](10/324): Loss: 0.7031 || Learning rate: lr=5e-05.
===> Epoch[186](20/324): Loss: 0.4832 || Learning rate: lr=5e-05.
===> Epoch[186](30/324): Loss: 0.7852 || Learning rate: lr=5e-05.
===> Epoch[186](40/324): Loss: 0.6958 || Learning rate: lr=5e-05.
===> Epoch[186](50/324): Loss: 1.0192 || Learning rate: lr=5e-05.
===> Epoch[186](60/324): Loss: 0.6915 || Learning rate: lr=5e-05.
===> Epoch[186](70/324): Loss: 0.9202 || Learning rate: lr=5e-05.
===> Epoch[186](80/324): Loss: 0.4733 || Learning rate: lr=5e-05.
===> Epoch[186](90/324): Loss: 0.4831 || Learning rate: lr=5e-05.
===> Epoch[186](100/324): Loss: 0.8290 || Learning rate: lr=5e-05.
===> Epoch[186](110/324): Loss: 0.8674 || Learning rate: lr=5e-05.
===> Epoch[186](120/324): Loss: 0.6947 || Learning rate: lr=5e-05.
===> Epoch[186](130/324): Loss: 0.6010 || Learning rate: lr=5e-05.
===> Epoch[186](140/324): Loss: 0.8544 || Learning rate: lr=5e-05.
===> Epoch[186](150/324): Loss: 1.1417 || Learning rate: lr=5e-05.
===> Epoch[186](160/324): Loss: 0.8209 || Learning rate: lr=5e-05.
===> Epoch[186](170/324): Loss: 0.6375 || Learning rate: lr=5e-05.
===> Epoch[186](180/324): Loss: 0.7579 || Learning rate: lr=5e-05.
===> Epoch[186](190/324): Loss: 0.5730 || Learning rate: lr=5e-05.
===> Epoch[186](200/324): Loss: 0.6999 || Learning rate: lr=5e-05.
===> Epoch[186](210/324): Loss: 0.5205 || Learning rate: lr=5e-05.
===> Epoch[186](220/324): Loss: 0.7671 || Learning rate: lr=5e-05.
===> Epoch[186](230/324): Loss: 1.3790 || Learning rate: lr=5e-05.
===> Epoch[186](240/324): Loss: 1.0742 || Learning rate: lr=5e-05.
===> Epoch[186](250/324): Loss: 0.6277 || Learning rate: lr=5e-05.
===> Epoch[186](260/324): Loss: 0.6774 || Learning rate: lr=5e-05.
===> Epoch[186](270/324): Loss: 0.8325 || Learning rate: lr=5e-05.
===> Epoch[186](280/324): Loss: 0.9095 || Learning rate: lr=5e-05.
===> Epoch[186](290/324): Loss: 0.5328 || Learning rate: lr=5e-05.
===> Epoch[186](300/324): Loss: 0.6696 || Learning rate: lr=5e-05.
===> Epoch[186](310/324): Loss: 0.6929 || Learning rate: lr=5e-05.
===> Epoch[186](320/324): Loss: 0.7373 || Learning rate: lr=5e-05.
===> Epoch[187](10/324): Loss: 0.6797 || Learning rate: lr=5e-05.
===> Epoch[187](20/324): Loss: 0.7783 || Learning rate: lr=5e-05.
===> Epoch[187](30/324): Loss: 0.7192 || Learning rate: lr=5e-05.
===> Epoch[187](40/324): Loss: 0.8936 || Learning rate: lr=5e-05.
===> Epoch[187](50/324): Loss: 0.6461 || Learning rate: lr=5e-05.
===> Epoch[187](60/324): Loss: 0.5296 || Learning rate: lr=5e-05.
===> Epoch[187](70/324): Loss: 0.6336 || Learning rate: lr=5e-05.
===> Epoch[187](80/324): Loss: 0.6979 || Learning rate: lr=5e-05.
===> Epoch[187](90/324): Loss: 0.4973 || Learning rate: lr=5e-05.
===> Epoch[187](100/324): Loss: 0.6781 || Learning rate: lr=5e-05.
===> Epoch[187](110/324): Loss: 0.7911 || Learning rate: lr=5e-05.
===> Epoch[187](120/324): Loss: 0.6489 || Learning rate: lr=5e-05.
===> Epoch[187](130/324): Loss: 0.8627 || Learning rate: lr=5e-05.
===> Epoch[187](140/324): Loss: 0.8699 || Learning rate: lr=5e-05.
===> Epoch[187](150/324): Loss: 0.7729 || Learning rate: lr=5e-05.
===> Epoch[187](160/324): Loss: 1.0914 || Learning rate: lr=5e-05.
===> Epoch[187](170/324): Loss: 0.4718 || Learning rate: lr=5e-05.
===> Epoch[187](180/324): Loss: 0.6181 || Learning rate: lr=5e-05.
===> Epoch[187](190/324): Loss: 0.8411 || Learning rate: lr=5e-05.
===> Epoch[187](200/324): Loss: 0.8223 || Learning rate: lr=5e-05.
===> Epoch[187](210/324): Loss: 0.8127 || Learning rate: lr=5e-05.
===> Epoch[187](220/324): Loss: 0.6085 || Learning rate: lr=5e-05.
===> Epoch[187](230/324): Loss: 0.7417 || Learning rate: lr=5e-05.
===> Epoch[187](240/324): Loss: 0.6877 || Learning rate: lr=5e-05.
===> Epoch[187](250/324): Loss: 0.4604 || Learning rate: lr=5e-05.
===> Epoch[187](260/324): Loss: 0.7495 || Learning rate: lr=5e-05.
===> Epoch[187](270/324): Loss: 0.8017 || Learning rate: lr=5e-05.
===> Epoch[187](280/324): Loss: 0.8124 || Learning rate: lr=5e-05.
===> Epoch[187](290/324): Loss: 0.5797 || Learning rate: lr=5e-05.
===> Epoch[187](300/324): Loss: 0.7926 || Learning rate: lr=5e-05.
===> Epoch[187](310/324): Loss: 1.0482 || Learning rate: lr=5e-05.
===> Epoch[187](320/324): Loss: 0.7791 || Learning rate: lr=5e-05.
===> Epoch[188](10/324): Loss: 0.7386 || Learning rate: lr=5e-05.
===> Epoch[188](20/324): Loss: 0.6491 || Learning rate: lr=5e-05.
===> Epoch[188](30/324): Loss: 0.7719 || Learning rate: lr=5e-05.
===> Epoch[188](40/324): Loss: 0.8232 || Learning rate: lr=5e-05.
===> Epoch[188](50/324): Loss: 0.8033 || Learning rate: lr=5e-05.
===> Epoch[188](60/324): Loss: 0.5750 || Learning rate: lr=5e-05.
===> Epoch[188](70/324): Loss: 0.6330 || Learning rate: lr=5e-05.
===> Epoch[188](80/324): Loss: 1.3967 || Learning rate: lr=5e-05.
===> Epoch[188](90/324): Loss: 0.9709 || Learning rate: lr=5e-05.
===> Epoch[188](100/324): Loss: 0.8109 || Learning rate: lr=5e-05.
===> Epoch[188](110/324): Loss: 0.7265 || Learning rate: lr=5e-05.
===> Epoch[188](120/324): Loss: 0.8527 || Learning rate: lr=5e-05.
===> Epoch[188](130/324): Loss: 0.8941 || Learning rate: lr=5e-05.
===> Epoch[188](140/324): Loss: 0.6297 || Learning rate: lr=5e-05.
===> Epoch[188](150/324): Loss: 1.0414 || Learning rate: lr=5e-05.
===> Epoch[188](160/324): Loss: 0.6547 || Learning rate: lr=5e-05.
===> Epoch[188](170/324): Loss: 0.7252 || Learning rate: lr=5e-05.
===> Epoch[188](180/324): Loss: 1.0626 || Learning rate: lr=5e-05.
===> Epoch[188](190/324): Loss: 0.7658 || Learning rate: lr=5e-05.
===> Epoch[188](200/324): Loss: 0.8856 || Learning rate: lr=5e-05.
===> Epoch[188](210/324): Loss: 0.4181 || Learning rate: lr=5e-05.
===> Epoch[188](220/324): Loss: 0.5484 || Learning rate: lr=5e-05.
===> Epoch[188](230/324): Loss: 0.7869 || Learning rate: lr=5e-05.
===> Epoch[188](240/324): Loss: 0.5789 || Learning rate: lr=5e-05.
===> Epoch[188](250/324): Loss: 0.6626 || Learning rate: lr=5e-05.
===> Epoch[188](260/324): Loss: 0.6649 || Learning rate: lr=5e-05.
===> Epoch[188](270/324): Loss: 0.4674 || Learning rate: lr=5e-05.
===> Epoch[188](280/324): Loss: 0.6163 || Learning rate: lr=5e-05.
===> Epoch[188](290/324): Loss: 0.8704 || Learning rate: lr=5e-05.
===> Epoch[188](300/324): Loss: 0.8264 || Learning rate: lr=5e-05.
===> Epoch[188](310/324): Loss: 0.5856 || Learning rate: lr=5e-05.
===> Epoch[188](320/324): Loss: 0.7309 || Learning rate: lr=5e-05.
===> Epoch[189](10/324): Loss: 1.1566 || Learning rate: lr=5e-05.
===> Epoch[189](20/324): Loss: 0.7366 || Learning rate: lr=5e-05.
===> Epoch[189](30/324): Loss: 0.7042 || Learning rate: lr=5e-05.
===> Epoch[189](40/324): Loss: 1.0419 || Learning rate: lr=5e-05.
===> Epoch[189](50/324): Loss: 0.8065 || Learning rate: lr=5e-05.
===> Epoch[189](60/324): Loss: 0.7751 || Learning rate: lr=5e-05.
===> Epoch[189](70/324): Loss: 0.7697 || Learning rate: lr=5e-05.
===> Epoch[189](80/324): Loss: 0.6840 || Learning rate: lr=5e-05.
===> Epoch[189](90/324): Loss: 0.5935 || Learning rate: lr=5e-05.
===> Epoch[189](100/324): Loss: 0.6992 || Learning rate: lr=5e-05.
===> Epoch[189](110/324): Loss: 0.8709 || Learning rate: lr=5e-05.
===> Epoch[189](120/324): Loss: 0.6748 || Learning rate: lr=5e-05.
===> Epoch[189](130/324): Loss: 0.6905 || Learning rate: lr=5e-05.
===> Epoch[189](140/324): Loss: 1.0994 || Learning rate: lr=5e-05.
===> Epoch[189](150/324): Loss: 1.0539 || Learning rate: lr=5e-05.
===> Epoch[189](160/324): Loss: 0.6795 || Learning rate: lr=5e-05.
===> Epoch[189](170/324): Loss: 0.7740 || Learning rate: lr=5e-05.
===> Epoch[189](180/324): Loss: 0.5602 || Learning rate: lr=5e-05.
===> Epoch[189](190/324): Loss: 1.0247 || Learning rate: lr=5e-05.
===> Epoch[189](200/324): Loss: 0.6765 || Learning rate: lr=5e-05.
===> Epoch[189](210/324): Loss: 0.6990 || Learning rate: lr=5e-05.
===> Epoch[189](220/324): Loss: 0.5086 || Learning rate: lr=5e-05.
===> Epoch[189](230/324): Loss: 0.9021 || Learning rate: lr=5e-05.
===> Epoch[189](240/324): Loss: 0.8611 || Learning rate: lr=5e-05.
===> Epoch[189](250/324): Loss: 0.7717 || Learning rate: lr=5e-05.
===> Epoch[189](260/324): Loss: 0.5985 || Learning rate: lr=5e-05.
===> Epoch[189](270/324): Loss: 0.8283 || Learning rate: lr=5e-05.
===> Epoch[189](280/324): Loss: 0.7439 || Learning rate: lr=5e-05.
===> Epoch[189](290/324): Loss: 0.6162 || Learning rate: lr=5e-05.
===> Epoch[189](300/324): Loss: 0.9708 || Learning rate: lr=5e-05.
===> Epoch[189](310/324): Loss: 0.5612 || Learning rate: lr=5e-05.
===> Epoch[189](320/324): Loss: 0.5619 || Learning rate: lr=5e-05.
===> Epoch[190](10/324): Loss: 1.1436 || Learning rate: lr=5e-05.
===> Epoch[190](20/324): Loss: 0.5273 || Learning rate: lr=5e-05.
===> Epoch[190](30/324): Loss: 0.8107 || Learning rate: lr=5e-05.
===> Epoch[190](40/324): Loss: 0.5967 || Learning rate: lr=5e-05.
===> Epoch[190](50/324): Loss: 0.8657 || Learning rate: lr=5e-05.
===> Epoch[190](60/324): Loss: 0.6704 || Learning rate: lr=5e-05.
===> Epoch[190](70/324): Loss: 0.9264 || Learning rate: lr=5e-05.
===> Epoch[190](80/324): Loss: 0.7667 || Learning rate: lr=5e-05.
===> Epoch[190](90/324): Loss: 0.6354 || Learning rate: lr=5e-05.
===> Epoch[190](100/324): Loss: 0.6264 || Learning rate: lr=5e-05.
===> Epoch[190](110/324): Loss: 0.5222 || Learning rate: lr=5e-05.
===> Epoch[190](120/324): Loss: 1.2074 || Learning rate: lr=5e-05.
===> Epoch[190](130/324): Loss: 0.8803 || Learning rate: lr=5e-05.
===> Epoch[190](140/324): Loss: 0.8573 || Learning rate: lr=5e-05.
===> Epoch[190](150/324): Loss: 0.6270 || Learning rate: lr=5e-05.
===> Epoch[190](160/324): Loss: 0.7580 || Learning rate: lr=5e-05.
===> Epoch[190](170/324): Loss: 0.8175 || Learning rate: lr=5e-05.
===> Epoch[190](180/324): Loss: 0.8236 || Learning rate: lr=5e-05.
===> Epoch[190](190/324): Loss: 0.5683 || Learning rate: lr=5e-05.
===> Epoch[190](200/324): Loss: 0.7397 || Learning rate: lr=5e-05.
===> Epoch[190](210/324): Loss: 0.5598 || Learning rate: lr=5e-05.
===> Epoch[190](220/324): Loss: 0.6770 || Learning rate: lr=5e-05.
===> Epoch[190](230/324): Loss: 0.5423 || Learning rate: lr=5e-05.
===> Epoch[190](240/324): Loss: 0.7540 || Learning rate: lr=5e-05.
===> Epoch[190](250/324): Loss: 0.5320 || Learning rate: lr=5e-05.
===> Epoch[190](260/324): Loss: 0.5176 || Learning rate: lr=5e-05.
===> Epoch[190](270/324): Loss: 0.5595 || Learning rate: lr=5e-05.
===> Epoch[190](280/324): Loss: 0.8996 || Learning rate: lr=5e-05.
===> Epoch[190](290/324): Loss: 0.6075 || Learning rate: lr=5e-05.
===> Epoch[190](300/324): Loss: 0.5443 || Learning rate: lr=5e-05.
===> Epoch[190](310/324): Loss: 0.5725 || Learning rate: lr=5e-05.
===> Epoch[190](320/324): Loss: 0.5795 || Learning rate: lr=5e-05.
===> Epoch[191](10/324): Loss: 0.4571 || Learning rate: lr=5e-05.
===> Epoch[191](20/324): Loss: 0.3566 || Learning rate: lr=5e-05.
===> Epoch[191](30/324): Loss: 0.7524 || Learning rate: lr=5e-05.
===> Epoch[191](40/324): Loss: 0.7230 || Learning rate: lr=5e-05.
===> Epoch[191](50/324): Loss: 0.8552 || Learning rate: lr=5e-05.
===> Epoch[191](60/324): Loss: 0.7474 || Learning rate: lr=5e-05.
===> Epoch[191](70/324): Loss: 0.8716 || Learning rate: lr=5e-05.
===> Epoch[191](80/324): Loss: 0.6513 || Learning rate: lr=5e-05.
===> Epoch[191](90/324): Loss: 0.7394 || Learning rate: lr=5e-05.
===> Epoch[191](100/324): Loss: 0.8206 || Learning rate: lr=5e-05.
===> Epoch[191](110/324): Loss: 1.2821 || Learning rate: lr=5e-05.
===> Epoch[191](120/324): Loss: 0.5941 || Learning rate: lr=5e-05.
===> Epoch[191](130/324): Loss: 0.6455 || Learning rate: lr=5e-05.
===> Epoch[191](140/324): Loss: 0.5967 || Learning rate: lr=5e-05.
===> Epoch[191](150/324): Loss: 0.9290 || Learning rate: lr=5e-05.
===> Epoch[191](160/324): Loss: 0.8497 || Learning rate: lr=5e-05.
===> Epoch[191](170/324): Loss: 0.6710 || Learning rate: lr=5e-05.
===> Epoch[191](180/324): Loss: 0.6304 || Learning rate: lr=5e-05.
===> Epoch[191](190/324): Loss: 0.8426 || Learning rate: lr=5e-05.
===> Epoch[191](200/324): Loss: 0.6899 || Learning rate: lr=5e-05.
===> Epoch[191](210/324): Loss: 0.9993 || Learning rate: lr=5e-05.
===> Epoch[191](220/324): Loss: 0.8987 || Learning rate: lr=5e-05.
===> Epoch[191](230/324): Loss: 0.7611 || Learning rate: lr=5e-05.
===> Epoch[191](240/324): Loss: 0.6762 || Learning rate: lr=5e-05.
===> Epoch[191](250/324): Loss: 1.2188 || Learning rate: lr=5e-05.
===> Epoch[191](260/324): Loss: 0.7599 || Learning rate: lr=5e-05.
===> Epoch[191](270/324): Loss: 0.7740 || Learning rate: lr=5e-05.
===> Epoch[191](280/324): Loss: 0.9105 || Learning rate: lr=5e-05.
===> Epoch[191](290/324): Loss: 1.3134 || Learning rate: lr=5e-05.
===> Epoch[191](300/324): Loss: 1.0674 || Learning rate: lr=5e-05.
===> Epoch[191](310/324): Loss: 0.7940 || Learning rate: lr=5e-05.
===> Epoch[191](320/324): Loss: 0.9248 || Learning rate: lr=5e-05.
===> Epoch[192](10/324): Loss: 0.9791 || Learning rate: lr=5e-05.
===> Epoch[192](20/324): Loss: 0.6832 || Learning rate: lr=5e-05.
===> Epoch[192](30/324): Loss: 0.5886 || Learning rate: lr=5e-05.
===> Epoch[192](40/324): Loss: 0.6268 || Learning rate: lr=5e-05.
===> Epoch[192](50/324): Loss: 0.6573 || Learning rate: lr=5e-05.
===> Epoch[192](60/324): Loss: 0.4932 || Learning rate: lr=5e-05.
===> Epoch[192](70/324): Loss: 0.6986 || Learning rate: lr=5e-05.
===> Epoch[192](80/324): Loss: 0.9180 || Learning rate: lr=5e-05.
===> Epoch[192](90/324): Loss: 0.7173 || Learning rate: lr=5e-05.
===> Epoch[192](100/324): Loss: 0.6468 || Learning rate: lr=5e-05.
===> Epoch[192](110/324): Loss: 0.8572 || Learning rate: lr=5e-05.
===> Epoch[192](120/324): Loss: 0.7568 || Learning rate: lr=5e-05.
===> Epoch[192](130/324): Loss: 0.6145 || Learning rate: lr=5e-05.
===> Epoch[192](140/324): Loss: 0.7332 || Learning rate: lr=5e-05.
===> Epoch[192](150/324): Loss: 0.6745 || Learning rate: lr=5e-05.
===> Epoch[192](160/324): Loss: 0.4435 || Learning rate: lr=5e-05.
===> Epoch[192](170/324): Loss: 0.7692 || Learning rate: lr=5e-05.
===> Epoch[192](180/324): Loss: 0.8222 || Learning rate: lr=5e-05.
===> Epoch[192](190/324): Loss: 0.5959 || Learning rate: lr=5e-05.
===> Epoch[192](200/324): Loss: 1.0431 || Learning rate: lr=5e-05.
===> Epoch[192](210/324): Loss: 0.6603 || Learning rate: lr=5e-05.
===> Epoch[192](220/324): Loss: 0.6848 || Learning rate: lr=5e-05.
===> Epoch[192](230/324): Loss: 0.9125 || Learning rate: lr=5e-05.
===> Epoch[192](240/324): Loss: 0.9075 || Learning rate: lr=5e-05.
===> Epoch[192](250/324): Loss: 0.6356 || Learning rate: lr=5e-05.
===> Epoch[192](260/324): Loss: 0.7053 || Learning rate: lr=5e-05.
===> Epoch[192](270/324): Loss: 0.6062 || Learning rate: lr=5e-05.
===> Epoch[192](280/324): Loss: 0.7397 || Learning rate: lr=5e-05.
===> Epoch[192](290/324): Loss: 0.5890 || Learning rate: lr=5e-05.
===> Epoch[192](300/324): Loss: 0.8671 || Learning rate: lr=5e-05.
===> Epoch[192](310/324): Loss: 0.8301 || Learning rate: lr=5e-05.
===> Epoch[192](320/324): Loss: 0.5787 || Learning rate: lr=5e-05.
===> Epoch[193](10/324): Loss: 0.7988 || Learning rate: lr=5e-05.
===> Epoch[193](20/324): Loss: 1.0913 || Learning rate: lr=5e-05.
===> Epoch[193](30/324): Loss: 0.5712 || Learning rate: lr=5e-05.
===> Epoch[193](40/324): Loss: 0.6884 || Learning rate: lr=5e-05.
===> Epoch[193](50/324): Loss: 0.6671 || Learning rate: lr=5e-05.
===> Epoch[193](60/324): Loss: 0.8130 || Learning rate: lr=5e-05.
===> Epoch[193](70/324): Loss: 0.6864 || Learning rate: lr=5e-05.
===> Epoch[193](80/324): Loss: 0.8173 || Learning rate: lr=5e-05.
===> Epoch[193](90/324): Loss: 0.9054 || Learning rate: lr=5e-05.
===> Epoch[193](100/324): Loss: 0.7442 || Learning rate: lr=5e-05.
===> Epoch[193](110/324): Loss: 0.8646 || Learning rate: lr=5e-05.
===> Epoch[193](120/324): Loss: 0.8723 || Learning rate: lr=5e-05.
===> Epoch[193](130/324): Loss: 1.0448 || Learning rate: lr=5e-05.
===> Epoch[193](140/324): Loss: 0.7579 || Learning rate: lr=5e-05.
===> Epoch[193](150/324): Loss: 0.6816 || Learning rate: lr=5e-05.
===> Epoch[193](160/324): Loss: 0.5741 || Learning rate: lr=5e-05.
===> Epoch[193](170/324): Loss: 0.6167 || Learning rate: lr=5e-05.
===> Epoch[193](180/324): Loss: 0.7534 || Learning rate: lr=5e-05.
===> Epoch[193](190/324): Loss: 0.4326 || Learning rate: lr=5e-05.
===> Epoch[193](200/324): Loss: 0.5957 || Learning rate: lr=5e-05.
===> Epoch[193](210/324): Loss: 0.6769 || Learning rate: lr=5e-05.
===> Epoch[193](220/324): Loss: 0.8690 || Learning rate: lr=5e-05.
===> Epoch[193](230/324): Loss: 0.8925 || Learning rate: lr=5e-05.
===> Epoch[193](240/324): Loss: 0.6128 || Learning rate: lr=5e-05.
===> Epoch[193](250/324): Loss: 0.7102 || Learning rate: lr=5e-05.
===> Epoch[193](260/324): Loss: 0.6929 || Learning rate: lr=5e-05.
===> Epoch[193](270/324): Loss: 0.6048 || Learning rate: lr=5e-05.
===> Epoch[193](280/324): Loss: 1.0591 || Learning rate: lr=5e-05.
===> Epoch[193](290/324): Loss: 0.6882 || Learning rate: lr=5e-05.
===> Epoch[193](300/324): Loss: 0.6018 || Learning rate: lr=5e-05.
===> Epoch[193](310/324): Loss: 0.7951 || Learning rate: lr=5e-05.
===> Epoch[193](320/324): Loss: 0.9346 || Learning rate: lr=5e-05.
===> Epoch[194](10/324): Loss: 0.6645 || Learning rate: lr=5e-05.
===> Epoch[194](20/324): Loss: 1.1535 || Learning rate: lr=5e-05.
===> Epoch[194](30/324): Loss: 0.6265 || Learning rate: lr=5e-05.
===> Epoch[194](40/324): Loss: 0.7308 || Learning rate: lr=5e-05.
===> Epoch[194](50/324): Loss: 0.6139 || Learning rate: lr=5e-05.
===> Epoch[194](60/324): Loss: 0.5603 || Learning rate: lr=5e-05.
===> Epoch[194](70/324): Loss: 0.8927 || Learning rate: lr=5e-05.
===> Epoch[194](80/324): Loss: 0.6641 || Learning rate: lr=5e-05.
===> Epoch[194](90/324): Loss: 0.4985 || Learning rate: lr=5e-05.
===> Epoch[194](100/324): Loss: 1.2143 || Learning rate: lr=5e-05.
===> Epoch[194](110/324): Loss: 0.7578 || Learning rate: lr=5e-05.
===> Epoch[194](120/324): Loss: 0.8752 || Learning rate: lr=5e-05.
===> Epoch[194](130/324): Loss: 0.5286 || Learning rate: lr=5e-05.
===> Epoch[194](140/324): Loss: 0.7736 || Learning rate: lr=5e-05.
===> Epoch[194](150/324): Loss: 0.4453 || Learning rate: lr=5e-05.
===> Epoch[194](160/324): Loss: 0.8323 || Learning rate: lr=5e-05.
===> Epoch[194](170/324): Loss: 0.8488 || Learning rate: lr=5e-05.
===> Epoch[194](180/324): Loss: 0.9439 || Learning rate: lr=5e-05.
===> Epoch[194](190/324): Loss: 0.7567 || Learning rate: lr=5e-05.
===> Epoch[194](200/324): Loss: 0.6502 || Learning rate: lr=5e-05.
===> Epoch[194](210/324): Loss: 0.7969 || Learning rate: lr=5e-05.
===> Epoch[194](220/324): Loss: 0.8477 || Learning rate: lr=5e-05.
===> Epoch[194](230/324): Loss: 0.6524 || Learning rate: lr=5e-05.
===> Epoch[194](240/324): Loss: 0.6709 || Learning rate: lr=5e-05.
===> Epoch[194](250/324): Loss: 0.5987 || Learning rate: lr=5e-05.
===> Epoch[194](260/324): Loss: 0.6999 || Learning rate: lr=5e-05.
===> Epoch[194](270/324): Loss: 0.9705 || Learning rate: lr=5e-05.
===> Epoch[194](280/324): Loss: 1.2596 || Learning rate: lr=5e-05.
===> Epoch[194](290/324): Loss: 0.8071 || Learning rate: lr=5e-05.
===> Epoch[194](300/324): Loss: 0.9097 || Learning rate: lr=5e-05.
===> Epoch[194](310/324): Loss: 0.7568 || Learning rate: lr=5e-05.
===> Epoch[194](320/324): Loss: 0.7329 || Learning rate: lr=5e-05.
===> Epoch[195](10/324): Loss: 0.7182 || Learning rate: lr=5e-05.
===> Epoch[195](20/324): Loss: 0.9206 || Learning rate: lr=5e-05.
===> Epoch[195](30/324): Loss: 0.8116 || Learning rate: lr=5e-05.
===> Epoch[195](40/324): Loss: 0.7390 || Learning rate: lr=5e-05.
===> Epoch[195](50/324): Loss: 0.6813 || Learning rate: lr=5e-05.
===> Epoch[195](60/324): Loss: 0.5015 || Learning rate: lr=5e-05.
===> Epoch[195](70/324): Loss: 0.6430 || Learning rate: lr=5e-05.
===> Epoch[195](80/324): Loss: 0.6882 || Learning rate: lr=5e-05.
===> Epoch[195](90/324): Loss: 0.8471 || Learning rate: lr=5e-05.
===> Epoch[195](100/324): Loss: 0.7762 || Learning rate: lr=5e-05.
===> Epoch[195](110/324): Loss: 0.8279 || Learning rate: lr=5e-05.
===> Epoch[195](120/324): Loss: 0.5604 || Learning rate: lr=5e-05.
===> Epoch[195](130/324): Loss: 0.5288 || Learning rate: lr=5e-05.
===> Epoch[195](140/324): Loss: 0.7512 || Learning rate: lr=5e-05.
===> Epoch[195](150/324): Loss: 0.8834 || Learning rate: lr=5e-05.
===> Epoch[195](160/324): Loss: 0.7119 || Learning rate: lr=5e-05.
===> Epoch[195](170/324): Loss: 0.7416 || Learning rate: lr=5e-05.
===> Epoch[195](180/324): Loss: 0.7027 || Learning rate: lr=5e-05.
===> Epoch[195](190/324): Loss: 0.7178 || Learning rate: lr=5e-05.
===> Epoch[195](200/324): Loss: 0.8770 || Learning rate: lr=5e-05.
===> Epoch[195](210/324): Loss: 0.5708 || Learning rate: lr=5e-05.
===> Epoch[195](220/324): Loss: 0.7921 || Learning rate: lr=5e-05.
===> Epoch[195](230/324): Loss: 0.8159 || Learning rate: lr=5e-05.
===> Epoch[195](240/324): Loss: 0.6225 || Learning rate: lr=5e-05.
===> Epoch[195](250/324): Loss: 0.6965 || Learning rate: lr=5e-05.
===> Epoch[195](260/324): Loss: 0.6070 || Learning rate: lr=5e-05.
===> Epoch[195](270/324): Loss: 0.6721 || Learning rate: lr=5e-05.
===> Epoch[195](280/324): Loss: 1.2646 || Learning rate: lr=5e-05.
===> Epoch[195](290/324): Loss: 0.8788 || Learning rate: lr=5e-05.
===> Epoch[195](300/324): Loss: 0.9223 || Learning rate: lr=5e-05.
===> Epoch[195](310/324): Loss: 0.7883 || Learning rate: lr=5e-05.
===> Epoch[195](320/324): Loss: 0.7319 || Learning rate: lr=5e-05.
===> Epoch[196](10/324): Loss: 0.9199 || Learning rate: lr=5e-05.
===> Epoch[196](20/324): Loss: 0.9544 || Learning rate: lr=5e-05.
===> Epoch[196](30/324): Loss: 0.6014 || Learning rate: lr=5e-05.
===> Epoch[196](40/324): Loss: 0.6338 || Learning rate: lr=5e-05.
===> Epoch[196](50/324): Loss: 0.7468 || Learning rate: lr=5e-05.
===> Epoch[196](60/324): Loss: 0.8700 || Learning rate: lr=5e-05.
===> Epoch[196](70/324): Loss: 0.8061 || Learning rate: lr=5e-05.
===> Epoch[196](80/324): Loss: 1.0214 || Learning rate: lr=5e-05.
===> Epoch[196](90/324): Loss: 0.5327 || Learning rate: lr=5e-05.
===> Epoch[196](100/324): Loss: 0.5643 || Learning rate: lr=5e-05.
===> Epoch[196](110/324): Loss: 0.6577 || Learning rate: lr=5e-05.
===> Epoch[196](120/324): Loss: 0.7959 || Learning rate: lr=5e-05.
===> Epoch[196](130/324): Loss: 0.5402 || Learning rate: lr=5e-05.
===> Epoch[196](140/324): Loss: 0.8430 || Learning rate: lr=5e-05.
===> Epoch[196](150/324): Loss: 0.7072 || Learning rate: lr=5e-05.
===> Epoch[196](160/324): Loss: 0.9580 || Learning rate: lr=5e-05.
===> Epoch[196](170/324): Loss: 0.4380 || Learning rate: lr=5e-05.
===> Epoch[196](180/324): Loss: 0.9401 || Learning rate: lr=5e-05.
===> Epoch[196](190/324): Loss: 0.6197 || Learning rate: lr=5e-05.
===> Epoch[196](200/324): Loss: 1.0073 || Learning rate: lr=5e-05.
===> Epoch[196](210/324): Loss: 0.8754 || Learning rate: lr=5e-05.
===> Epoch[196](220/324): Loss: 0.6959 || Learning rate: lr=5e-05.
===> Epoch[196](230/324): Loss: 0.8460 || Learning rate: lr=5e-05.
===> Epoch[196](240/324): Loss: 0.7829 || Learning rate: lr=5e-05.
===> Epoch[196](250/324): Loss: 0.7341 || Learning rate: lr=5e-05.
===> Epoch[196](260/324): Loss: 0.6658 || Learning rate: lr=5e-05.
===> Epoch[196](270/324): Loss: 0.7119 || Learning rate: lr=5e-05.
===> Epoch[196](280/324): Loss: 0.5268 || Learning rate: lr=5e-05.
===> Epoch[196](290/324): Loss: 0.7626 || Learning rate: lr=5e-05.
===> Epoch[196](300/324): Loss: 0.7315 || Learning rate: lr=5e-05.
===> Epoch[196](310/324): Loss: 0.8943 || Learning rate: lr=5e-05.
===> Epoch[196](320/324): Loss: 0.5918 || Learning rate: lr=5e-05.
===> Epoch[197](10/324): Loss: 0.7024 || Learning rate: lr=5e-05.
===> Epoch[197](20/324): Loss: 0.8439 || Learning rate: lr=5e-05.
===> Epoch[197](30/324): Loss: 0.6174 || Learning rate: lr=5e-05.
===> Epoch[197](40/324): Loss: 0.4671 || Learning rate: lr=5e-05.
===> Epoch[197](50/324): Loss: 0.6729 || Learning rate: lr=5e-05.
===> Epoch[197](60/324): Loss: 0.7313 || Learning rate: lr=5e-05.
===> Epoch[197](70/324): Loss: 0.9263 || Learning rate: lr=5e-05.
===> Epoch[197](80/324): Loss: 0.8802 || Learning rate: lr=5e-05.
===> Epoch[197](90/324): Loss: 0.6554 || Learning rate: lr=5e-05.
===> Epoch[197](100/324): Loss: 0.6524 || Learning rate: lr=5e-05.
===> Epoch[197](110/324): Loss: 0.9792 || Learning rate: lr=5e-05.
===> Epoch[197](120/324): Loss: 0.7360 || Learning rate: lr=5e-05.
===> Epoch[197](130/324): Loss: 0.8637 || Learning rate: lr=5e-05.
===> Epoch[197](140/324): Loss: 0.6087 || Learning rate: lr=5e-05.
===> Epoch[197](150/324): Loss: 0.5305 || Learning rate: lr=5e-05.
===> Epoch[197](160/324): Loss: 0.7809 || Learning rate: lr=5e-05.
===> Epoch[197](170/324): Loss: 1.1202 || Learning rate: lr=5e-05.
===> Epoch[197](180/324): Loss: 0.5853 || Learning rate: lr=5e-05.
===> Epoch[197](190/324): Loss: 0.7106 || Learning rate: lr=5e-05.
===> Epoch[197](200/324): Loss: 0.9816 || Learning rate: lr=5e-05.
===> Epoch[197](210/324): Loss: 0.6848 || Learning rate: lr=5e-05.
===> Epoch[197](220/324): Loss: 0.7779 || Learning rate: lr=5e-05.
===> Epoch[197](230/324): Loss: 0.6200 || Learning rate: lr=5e-05.
===> Epoch[197](240/324): Loss: 0.6863 || Learning rate: lr=5e-05.
===> Epoch[197](250/324): Loss: 0.6117 || Learning rate: lr=5e-05.
===> Epoch[197](260/324): Loss: 0.6254 || Learning rate: lr=5e-05.
===> Epoch[197](270/324): Loss: 1.1388 || Learning rate: lr=5e-05.
===> Epoch[197](280/324): Loss: 0.6758 || Learning rate: lr=5e-05.
===> Epoch[197](290/324): Loss: 0.5178 || Learning rate: lr=5e-05.
===> Epoch[197](300/324): Loss: 0.4448 || Learning rate: lr=5e-05.
===> Epoch[197](310/324): Loss: 0.9296 || Learning rate: lr=5e-05.
===> Epoch[197](320/324): Loss: 0.7421 || Learning rate: lr=5e-05.
===> Epoch[198](10/324): Loss: 0.6940 || Learning rate: lr=5e-05.
===> Epoch[198](20/324): Loss: 0.8829 || Learning rate: lr=5e-05.
===> Epoch[198](30/324): Loss: 0.6865 || Learning rate: lr=5e-05.
===> Epoch[198](40/324): Loss: 0.9308 || Learning rate: lr=5e-05.
===> Epoch[198](50/324): Loss: 0.6343 || Learning rate: lr=5e-05.
===> Epoch[198](60/324): Loss: 0.5345 || Learning rate: lr=5e-05.
===> Epoch[198](70/324): Loss: 0.5994 || Learning rate: lr=5e-05.
===> Epoch[198](80/324): Loss: 0.8611 || Learning rate: lr=5e-05.
===> Epoch[198](90/324): Loss: 0.7306 || Learning rate: lr=5e-05.
===> Epoch[198](100/324): Loss: 0.5533 || Learning rate: lr=5e-05.
===> Epoch[198](110/324): Loss: 0.8096 || Learning rate: lr=5e-05.
===> Epoch[198](120/324): Loss: 0.8602 || Learning rate: lr=5e-05.
===> Epoch[198](130/324): Loss: 0.7058 || Learning rate: lr=5e-05.
===> Epoch[198](140/324): Loss: 0.8309 || Learning rate: lr=5e-05.
===> Epoch[198](150/324): Loss: 0.7243 || Learning rate: lr=5e-05.
===> Epoch[198](160/324): Loss: 0.5073 || Learning rate: lr=5e-05.
===> Epoch[198](170/324): Loss: 0.7991 || Learning rate: lr=5e-05.
===> Epoch[198](180/324): Loss: 0.6459 || Learning rate: lr=5e-05.
===> Epoch[198](190/324): Loss: 0.8168 || Learning rate: lr=5e-05.
===> Epoch[198](200/324): Loss: 0.9219 || Learning rate: lr=5e-05.
===> Epoch[198](210/324): Loss: 0.7506 || Learning rate: lr=5e-05.
===> Epoch[198](220/324): Loss: 0.8356 || Learning rate: lr=5e-05.
===> Epoch[198](230/324): Loss: 1.0833 || Learning rate: lr=5e-05.
===> Epoch[198](240/324): Loss: 0.6805 || Learning rate: lr=5e-05.
===> Epoch[198](250/324): Loss: 0.7131 || Learning rate: lr=5e-05.
===> Epoch[198](260/324): Loss: 0.6688 || Learning rate: lr=5e-05.
===> Epoch[198](270/324): Loss: 0.4703 || Learning rate: lr=5e-05.
===> Epoch[198](280/324): Loss: 1.0153 || Learning rate: lr=5e-05.
===> Epoch[198](290/324): Loss: 0.7085 || Learning rate: lr=5e-05.
===> Epoch[198](300/324): Loss: 0.6890 || Learning rate: lr=5e-05.
===> Epoch[198](310/324): Loss: 0.6305 || Learning rate: lr=5e-05.
===> Epoch[198](320/324): Loss: 0.8070 || Learning rate: lr=5e-05.
===> Epoch[199](10/324): Loss: 0.9806 || Learning rate: lr=5e-05.
===> Epoch[199](20/324): Loss: 0.8336 || Learning rate: lr=5e-05.
===> Epoch[199](30/324): Loss: 0.8608 || Learning rate: lr=5e-05.
===> Epoch[199](40/324): Loss: 0.8304 || Learning rate: lr=5e-05.
===> Epoch[199](50/324): Loss: 0.8128 || Learning rate: lr=5e-05.
===> Epoch[199](60/324): Loss: 0.7340 || Learning rate: lr=5e-05.
===> Epoch[199](70/324): Loss: 0.6098 || Learning rate: lr=5e-05.
===> Epoch[199](80/324): Loss: 0.6147 || Learning rate: lr=5e-05.
===> Epoch[199](90/324): Loss: 0.8323 || Learning rate: lr=5e-05.
===> Epoch[199](100/324): Loss: 0.5531 || Learning rate: lr=5e-05.
===> Epoch[199](110/324): Loss: 0.7033 || Learning rate: lr=5e-05.
===> Epoch[199](120/324): Loss: 0.8456 || Learning rate: lr=5e-05.
===> Epoch[199](130/324): Loss: 0.7139 || Learning rate: lr=5e-05.
===> Epoch[199](140/324): Loss: 1.0902 || Learning rate: lr=5e-05.
===> Epoch[199](150/324): Loss: 0.8564 || Learning rate: lr=5e-05.
===> Epoch[199](160/324): Loss: 0.3493 || Learning rate: lr=5e-05.
===> Epoch[199](170/324): Loss: 0.7410 || Learning rate: lr=5e-05.
===> Epoch[199](180/324): Loss: 0.7543 || Learning rate: lr=5e-05.
===> Epoch[199](190/324): Loss: 0.9191 || Learning rate: lr=5e-05.
===> Epoch[199](200/324): Loss: 0.5713 || Learning rate: lr=5e-05.
===> Epoch[199](210/324): Loss: 0.8014 || Learning rate: lr=5e-05.
===> Epoch[199](220/324): Loss: 0.8302 || Learning rate: lr=5e-05.
===> Epoch[199](230/324): Loss: 0.8498 || Learning rate: lr=5e-05.
===> Epoch[199](240/324): Loss: 0.8168 || Learning rate: lr=5e-05.
===> Epoch[199](250/324): Loss: 0.5042 || Learning rate: lr=5e-05.
===> Epoch[199](260/324): Loss: 0.8731 || Learning rate: lr=5e-05.
===> Epoch[199](270/324): Loss: 0.9951 || Learning rate: lr=5e-05.
===> Epoch[199](280/324): Loss: 0.7902 || Learning rate: lr=5e-05.
===> Epoch[199](290/324): Loss: 0.6207 || Learning rate: lr=5e-05.
===> Epoch[199](300/324): Loss: 0.7632 || Learning rate: lr=5e-05.
===> Epoch[199](310/324): Loss: 0.9167 || Learning rate: lr=5e-05.
===> Epoch[199](320/324): Loss: 0.6498 || Learning rate: lr=5e-05.
===> Epoch[200](10/324): Loss: 0.7372 || Learning rate: lr=5e-05.
===> Epoch[200](20/324): Loss: 0.7449 || Learning rate: lr=5e-05.
===> Epoch[200](30/324): Loss: 0.7101 || Learning rate: lr=5e-05.
===> Epoch[200](40/324): Loss: 0.6526 || Learning rate: lr=5e-05.
===> Epoch[200](50/324): Loss: 0.8979 || Learning rate: lr=5e-05.
===> Epoch[200](60/324): Loss: 0.8908 || Learning rate: lr=5e-05.
===> Epoch[200](70/324): Loss: 0.6923 || Learning rate: lr=5e-05.
===> Epoch[200](80/324): Loss: 0.7186 || Learning rate: lr=5e-05.
===> Epoch[200](90/324): Loss: 0.6262 || Learning rate: lr=5e-05.
===> Epoch[200](100/324): Loss: 0.6427 || Learning rate: lr=5e-05.
===> Epoch[200](110/324): Loss: 1.1272 || Learning rate: lr=5e-05.
===> Epoch[200](120/324): Loss: 0.9847 || Learning rate: lr=5e-05.
===> Epoch[200](130/324): Loss: 1.1945 || Learning rate: lr=5e-05.
===> Epoch[200](140/324): Loss: 0.7510 || Learning rate: lr=5e-05.
===> Epoch[200](150/324): Loss: 0.4772 || Learning rate: lr=5e-05.
===> Epoch[200](160/324): Loss: 0.6505 || Learning rate: lr=5e-05.
===> Epoch[200](170/324): Loss: 1.0059 || Learning rate: lr=5e-05.
===> Epoch[200](180/324): Loss: 0.7440 || Learning rate: lr=5e-05.
===> Epoch[200](190/324): Loss: 0.8700 || Learning rate: lr=5e-05.
===> Epoch[200](200/324): Loss: 0.7097 || Learning rate: lr=5e-05.
===> Epoch[200](210/324): Loss: 0.8356 || Learning rate: lr=5e-05.
===> Epoch[200](220/324): Loss: 0.7030 || Learning rate: lr=5e-05.
===> Epoch[200](230/324): Loss: 1.0528 || Learning rate: lr=5e-05.
===> Epoch[200](240/324): Loss: 0.6352 || Learning rate: lr=5e-05.
===> Epoch[200](250/324): Loss: 0.6941 || Learning rate: lr=5e-05.
===> Epoch[200](260/324): Loss: 0.8975 || Learning rate: lr=5e-05.
===> Epoch[200](270/324): Loss: 0.8619 || Learning rate: lr=5e-05.
===> Epoch[200](280/324): Loss: 0.7854 || Learning rate: lr=5e-05.
===> Epoch[200](290/324): Loss: 0.7513 || Learning rate: lr=5e-05.
===> Epoch[200](300/324): Loss: 1.1579 || Learning rate: lr=5e-05.
===> Epoch[200](310/324): Loss: 1.1864 || Learning rate: lr=5e-05.
===> Epoch[200](320/324): Loss: 0.8480 || Learning rate: lr=5e-05.
Checkpoint saved to weights/epoch_v2_200.pth
===> Epoch[201](10/324): Loss: 0.9050 || Learning rate: lr=2.5e-05.
===> Epoch[201](20/324): Loss: 0.9256 || Learning rate: lr=2.5e-05.
===> Epoch[201](30/324): Loss: 0.5631 || Learning rate: lr=2.5e-05.
===> Epoch[201](40/324): Loss: 0.7589 || Learning rate: lr=2.5e-05.
===> Epoch[201](50/324): Loss: 0.7785 || Learning rate: lr=2.5e-05.
===> Epoch[201](60/324): Loss: 1.1166 || Learning rate: lr=2.5e-05.
===> Epoch[201](70/324): Loss: 0.4986 || Learning rate: lr=2.5e-05.
===> Epoch[201](80/324): Loss: 0.6351 || Learning rate: lr=2.5e-05.
===> Epoch[201](90/324): Loss: 0.7389 || Learning rate: lr=2.5e-05.
===> Epoch[201](100/324): Loss: 0.7168 || Learning rate: lr=2.5e-05.
===> Epoch[201](110/324): Loss: 0.7107 || Learning rate: lr=2.5e-05.
===> Epoch[201](120/324): Loss: 0.8300 || Learning rate: lr=2.5e-05.
===> Epoch[201](130/324): Loss: 0.5769 || Learning rate: lr=2.5e-05.
===> Epoch[201](140/324): Loss: 1.2165 || Learning rate: lr=2.5e-05.
===> Epoch[201](150/324): Loss: 0.4042 || Learning rate: lr=2.5e-05.
===> Epoch[201](160/324): Loss: 0.6558 || Learning rate: lr=2.5e-05.
===> Epoch[201](170/324): Loss: 0.3642 || Learning rate: lr=2.5e-05.
===> Epoch[201](180/324): Loss: 0.8678 || Learning rate: lr=2.5e-05.
===> Epoch[201](190/324): Loss: 0.5686 || Learning rate: lr=2.5e-05.
===> Epoch[201](200/324): Loss: 0.6190 || Learning rate: lr=2.5e-05.
===> Epoch[201](210/324): Loss: 0.6785 || Learning rate: lr=2.5e-05.
===> Epoch[201](220/324): Loss: 0.7874 || Learning rate: lr=2.5e-05.
===> Epoch[201](230/324): Loss: 0.5720 || Learning rate: lr=2.5e-05.
===> Epoch[201](240/324): Loss: 0.4219 || Learning rate: lr=2.5e-05.
===> Epoch[201](250/324): Loss: 0.7265 || Learning rate: lr=2.5e-05.
===> Epoch[201](260/324): Loss: 0.9194 || Learning rate: lr=2.5e-05.
===> Epoch[201](270/324): Loss: 0.6286 || Learning rate: lr=2.5e-05.
===> Epoch[201](280/324): Loss: 0.6805 || Learning rate: lr=2.5e-05.
===> Epoch[201](290/324): Loss: 0.7570 || Learning rate: lr=2.5e-05.
===> Epoch[201](300/324): Loss: 0.6650 || Learning rate: lr=2.5e-05.
===> Epoch[201](310/324): Loss: 0.9913 || Learning rate: lr=2.5e-05.
===> Epoch[201](320/324): Loss: 0.7615 || Learning rate: lr=2.5e-05.
===> Epoch[202](10/324): Loss: 0.6846 || Learning rate: lr=2.5e-05.
===> Epoch[202](20/324): Loss: 0.6612 || Learning rate: lr=2.5e-05.
===> Epoch[202](30/324): Loss: 0.6531 || Learning rate: lr=2.5e-05.
===> Epoch[202](40/324): Loss: 0.6110 || Learning rate: lr=2.5e-05.
===> Epoch[202](50/324): Loss: 0.9122 || Learning rate: lr=2.5e-05.
===> Epoch[202](60/324): Loss: 0.4626 || Learning rate: lr=2.5e-05.
===> Epoch[202](70/324): Loss: 0.7652 || Learning rate: lr=2.5e-05.
===> Epoch[202](80/324): Loss: 0.5894 || Learning rate: lr=2.5e-05.
===> Epoch[202](90/324): Loss: 0.6797 || Learning rate: lr=2.5e-05.
===> Epoch[202](100/324): Loss: 0.7568 || Learning rate: lr=2.5e-05.
===> Epoch[202](110/324): Loss: 0.5421 || Learning rate: lr=2.5e-05.
===> Epoch[202](120/324): Loss: 0.6680 || Learning rate: lr=2.5e-05.
===> Epoch[202](130/324): Loss: 0.8546 || Learning rate: lr=2.5e-05.
===> Epoch[202](140/324): Loss: 0.9262 || Learning rate: lr=2.5e-05.
===> Epoch[202](150/324): Loss: 0.6366 || Learning rate: lr=2.5e-05.
===> Epoch[202](160/324): Loss: 0.7030 || Learning rate: lr=2.5e-05.
===> Epoch[202](170/324): Loss: 0.7593 || Learning rate: lr=2.5e-05.
===> Epoch[202](180/324): Loss: 0.8446 || Learning rate: lr=2.5e-05.
===> Epoch[202](190/324): Loss: 0.4750 || Learning rate: lr=2.5e-05.
===> Epoch[202](200/324): Loss: 0.7696 || Learning rate: lr=2.5e-05.
===> Epoch[202](210/324): Loss: 0.7401 || Learning rate: lr=2.5e-05.
===> Epoch[202](220/324): Loss: 0.6568 || Learning rate: lr=2.5e-05.
===> Epoch[202](230/324): Loss: 0.6223 || Learning rate: lr=2.5e-05.
===> Epoch[202](240/324): Loss: 1.0547 || Learning rate: lr=2.5e-05.
===> Epoch[202](250/324): Loss: 0.5756 || Learning rate: lr=2.5e-05.
===> Epoch[202](260/324): Loss: 0.4061 || Learning rate: lr=2.5e-05.
===> Epoch[202](270/324): Loss: 0.8775 || Learning rate: lr=2.5e-05.
===> Epoch[202](280/324): Loss: 0.7002 || Learning rate: lr=2.5e-05.
===> Epoch[202](290/324): Loss: 0.7234 || Learning rate: lr=2.5e-05.
===> Epoch[202](300/324): Loss: 0.7933 || Learning rate: lr=2.5e-05.
===> Epoch[202](310/324): Loss: 0.6785 || Learning rate: lr=2.5e-05.
===> Epoch[202](320/324): Loss: 0.7461 || Learning rate: lr=2.5e-05.
===> Epoch[203](10/324): Loss: 0.5943 || Learning rate: lr=2.5e-05.
===> Epoch[203](20/324): Loss: 0.7330 || Learning rate: lr=2.5e-05.
===> Epoch[203](30/324): Loss: 0.7091 || Learning rate: lr=2.5e-05.
===> Epoch[203](40/324): Loss: 0.5818 || Learning rate: lr=2.5e-05.
===> Epoch[203](50/324): Loss: 0.5389 || Learning rate: lr=2.5e-05.
===> Epoch[203](60/324): Loss: 0.7962 || Learning rate: lr=2.5e-05.
===> Epoch[203](70/324): Loss: 0.9332 || Learning rate: lr=2.5e-05.
===> Epoch[203](80/324): Loss: 0.7181 || Learning rate: lr=2.5e-05.
===> Epoch[203](90/324): Loss: 0.9070 || Learning rate: lr=2.5e-05.
===> Epoch[203](100/324): Loss: 0.8165 || Learning rate: lr=2.5e-05.
===> Epoch[203](110/324): Loss: 0.5556 || Learning rate: lr=2.5e-05.
===> Epoch[203](120/324): Loss: 0.7229 || Learning rate: lr=2.5e-05.
===> Epoch[203](130/324): Loss: 0.8664 || Learning rate: lr=2.5e-05.
===> Epoch[203](140/324): Loss: 0.7533 || Learning rate: lr=2.5e-05.
===> Epoch[203](150/324): Loss: 0.8237 || Learning rate: lr=2.5e-05.
===> Epoch[203](160/324): Loss: 0.5436 || Learning rate: lr=2.5e-05.
===> Epoch[203](170/324): Loss: 0.6188 || Learning rate: lr=2.5e-05.
===> Epoch[203](180/324): Loss: 0.5193 || Learning rate: lr=2.5e-05.
===> Epoch[203](190/324): Loss: 0.6509 || Learning rate: lr=2.5e-05.
===> Epoch[203](200/324): Loss: 0.6771 || Learning rate: lr=2.5e-05.
===> Epoch[203](210/324): Loss: 0.5599 || Learning rate: lr=2.5e-05.
===> Epoch[203](220/324): Loss: 0.6679 || Learning rate: lr=2.5e-05.
===> Epoch[203](230/324): Loss: 0.5906 || Learning rate: lr=2.5e-05.
===> Epoch[203](240/324): Loss: 0.6843 || Learning rate: lr=2.5e-05.
===> Epoch[203](250/324): Loss: 0.7669 || Learning rate: lr=2.5e-05.
===> Epoch[203](260/324): Loss: 0.4738 || Learning rate: lr=2.5e-05.
===> Epoch[203](270/324): Loss: 0.7766 || Learning rate: lr=2.5e-05.
===> Epoch[203](280/324): Loss: 0.5625 || Learning rate: lr=2.5e-05.
===> Epoch[203](290/324): Loss: 0.5938 || Learning rate: lr=2.5e-05.
===> Epoch[203](300/324): Loss: 0.8309 || Learning rate: lr=2.5e-05.
===> Epoch[203](310/324): Loss: 0.8728 || Learning rate: lr=2.5e-05.
===> Epoch[203](320/324): Loss: 0.6927 || Learning rate: lr=2.5e-05.
===> Epoch[204](10/324): Loss: 0.7292 || Learning rate: lr=2.5e-05.
===> Epoch[204](20/324): Loss: 0.6834 || Learning rate: lr=2.5e-05.
===> Epoch[204](30/324): Loss: 0.6269 || Learning rate: lr=2.5e-05.
===> Epoch[204](40/324): Loss: 0.6301 || Learning rate: lr=2.5e-05.
===> Epoch[204](50/324): Loss: 0.5965 || Learning rate: lr=2.5e-05.
===> Epoch[204](60/324): Loss: 0.6996 || Learning rate: lr=2.5e-05.
===> Epoch[204](70/324): Loss: 0.7390 || Learning rate: lr=2.5e-05.
===> Epoch[204](80/324): Loss: 0.5934 || Learning rate: lr=2.5e-05.
===> Epoch[204](90/324): Loss: 0.4887 || Learning rate: lr=2.5e-05.
===> Epoch[204](100/324): Loss: 0.4486 || Learning rate: lr=2.5e-05.
===> Epoch[204](110/324): Loss: 0.6738 || Learning rate: lr=2.5e-05.
===> Epoch[204](120/324): Loss: 1.0660 || Learning rate: lr=2.5e-05.
===> Epoch[204](130/324): Loss: 0.5156 || Learning rate: lr=2.5e-05.
===> Epoch[204](140/324): Loss: 0.7550 || Learning rate: lr=2.5e-05.
===> Epoch[204](150/324): Loss: 0.8151 || Learning rate: lr=2.5e-05.
===> Epoch[204](160/324): Loss: 0.7449 || Learning rate: lr=2.5e-05.
===> Epoch[204](170/324): Loss: 0.9358 || Learning rate: lr=2.5e-05.
===> Epoch[204](180/324): Loss: 0.9055 || Learning rate: lr=2.5e-05.
===> Epoch[204](190/324): Loss: 0.4516 || Learning rate: lr=2.5e-05.
===> Epoch[204](200/324): Loss: 0.5400 || Learning rate: lr=2.5e-05.
===> Epoch[204](210/324): Loss: 0.6877 || Learning rate: lr=2.5e-05.
===> Epoch[204](220/324): Loss: 0.5144 || Learning rate: lr=2.5e-05.
===> Epoch[204](230/324): Loss: 0.7517 || Learning rate: lr=2.5e-05.
===> Epoch[204](240/324): Loss: 0.6534 || Learning rate: lr=2.5e-05.
===> Epoch[204](250/324): Loss: 0.5067 || Learning rate: lr=2.5e-05.
===> Epoch[204](260/324): Loss: 0.9754 || Learning rate: lr=2.5e-05.
===> Epoch[204](270/324): Loss: 0.7348 || Learning rate: lr=2.5e-05.
===> Epoch[204](280/324): Loss: 0.8804 || Learning rate: lr=2.5e-05.
===> Epoch[204](290/324): Loss: 0.6384 || Learning rate: lr=2.5e-05.
===> Epoch[204](300/324): Loss: 0.6739 || Learning rate: lr=2.5e-05.
===> Epoch[204](310/324): Loss: 0.5291 || Learning rate: lr=2.5e-05.
===> Epoch[204](320/324): Loss: 0.9659 || Learning rate: lr=2.5e-05.
===> Epoch[205](10/324): Loss: 0.4024 || Learning rate: lr=2.5e-05.
===> Epoch[205](20/324): Loss: 0.8084 || Learning rate: lr=2.5e-05.
===> Epoch[205](30/324): Loss: 0.5489 || Learning rate: lr=2.5e-05.
===> Epoch[205](40/324): Loss: 0.6275 || Learning rate: lr=2.5e-05.
===> Epoch[205](50/324): Loss: 0.4762 || Learning rate: lr=2.5e-05.
===> Epoch[205](60/324): Loss: 0.7630 || Learning rate: lr=2.5e-05.
===> Epoch[205](70/324): Loss: 0.4356 || Learning rate: lr=2.5e-05.
===> Epoch[205](80/324): Loss: 0.6534 || Learning rate: lr=2.5e-05.
===> Epoch[205](90/324): Loss: 0.5636 || Learning rate: lr=2.5e-05.
===> Epoch[205](100/324): Loss: 0.6852 || Learning rate: lr=2.5e-05.
===> Epoch[205](110/324): Loss: 0.8056 || Learning rate: lr=2.5e-05.
===> Epoch[205](120/324): Loss: 0.6580 || Learning rate: lr=2.5e-05.
===> Epoch[205](130/324): Loss: 0.8283 || Learning rate: lr=2.5e-05.
===> Epoch[205](140/324): Loss: 0.9925 || Learning rate: lr=2.5e-05.
===> Epoch[205](150/324): Loss: 0.4472 || Learning rate: lr=2.5e-05.
===> Epoch[205](160/324): Loss: 0.8492 || Learning rate: lr=2.5e-05.
===> Epoch[205](170/324): Loss: 0.8315 || Learning rate: lr=2.5e-05.
===> Epoch[205](180/324): Loss: 0.5944 || Learning rate: lr=2.5e-05.
===> Epoch[205](190/324): Loss: 0.6599 || Learning rate: lr=2.5e-05.
===> Epoch[205](200/324): Loss: 0.8313 || Learning rate: lr=2.5e-05.
===> Epoch[205](210/324): Loss: 0.8140 || Learning rate: lr=2.5e-05.
===> Epoch[205](220/324): Loss: 0.7373 || Learning rate: lr=2.5e-05.
===> Epoch[205](230/324): Loss: 0.6420 || Learning rate: lr=2.5e-05.
===> Epoch[205](240/324): Loss: 0.4419 || Learning rate: lr=2.5e-05.
===> Epoch[205](250/324): Loss: 0.6094 || Learning rate: lr=2.5e-05.
===> Epoch[205](260/324): Loss: 0.5203 || Learning rate: lr=2.5e-05.
===> Epoch[205](270/324): Loss: 0.6326 || Learning rate: lr=2.5e-05.
===> Epoch[205](280/324): Loss: 0.7250 || Learning rate: lr=2.5e-05.
===> Epoch[205](290/324): Loss: 0.6928 || Learning rate: lr=2.5e-05.
===> Epoch[205](300/324): Loss: 0.5805 || Learning rate: lr=2.5e-05.
===> Epoch[205](310/324): Loss: 0.6009 || Learning rate: lr=2.5e-05.
===> Epoch[205](320/324): Loss: 0.5123 || Learning rate: lr=2.5e-05.
===> Epoch[206](10/324): Loss: 0.8717 || Learning rate: lr=2.5e-05.
===> Epoch[206](20/324): Loss: 0.7182 || Learning rate: lr=2.5e-05.
===> Epoch[206](30/324): Loss: 0.7230 || Learning rate: lr=2.5e-05.
===> Epoch[206](40/324): Loss: 0.6597 || Learning rate: lr=2.5e-05.
===> Epoch[206](50/324): Loss: 0.5192 || Learning rate: lr=2.5e-05.
===> Epoch[206](60/324): Loss: 0.6651 || Learning rate: lr=2.5e-05.
===> Epoch[206](70/324): Loss: 0.8588 || Learning rate: lr=2.5e-05.
===> Epoch[206](80/324): Loss: 0.7241 || Learning rate: lr=2.5e-05.
===> Epoch[206](90/324): Loss: 0.6780 || Learning rate: lr=2.5e-05.
===> Epoch[206](100/324): Loss: 0.9054 || Learning rate: lr=2.5e-05.
===> Epoch[206](110/324): Loss: 0.8353 || Learning rate: lr=2.5e-05.
===> Epoch[206](120/324): Loss: 0.8630 || Learning rate: lr=2.5e-05.
===> Epoch[206](130/324): Loss: 0.6630 || Learning rate: lr=2.5e-05.
===> Epoch[206](140/324): Loss: 0.9674 || Learning rate: lr=2.5e-05.
===> Epoch[206](150/324): Loss: 0.5114 || Learning rate: lr=2.5e-05.
===> Epoch[206](160/324): Loss: 1.1093 || Learning rate: lr=2.5e-05.
===> Epoch[206](170/324): Loss: 0.5942 || Learning rate: lr=2.5e-05.
===> Epoch[206](180/324): Loss: 0.6356 || Learning rate: lr=2.5e-05.
===> Epoch[206](190/324): Loss: 0.6295 || Learning rate: lr=2.5e-05.
===> Epoch[206](200/324): Loss: 0.7495 || Learning rate: lr=2.5e-05.
===> Epoch[206](210/324): Loss: 0.9115 || Learning rate: lr=2.5e-05.
===> Epoch[206](220/324): Loss: 0.8165 || Learning rate: lr=2.5e-05.
===> Epoch[206](230/324): Loss: 0.4310 || Learning rate: lr=2.5e-05.
===> Epoch[206](240/324): Loss: 0.5998 || Learning rate: lr=2.5e-05.
===> Epoch[206](250/324): Loss: 0.6776 || Learning rate: lr=2.5e-05.
===> Epoch[206](260/324): Loss: 0.7702 || Learning rate: lr=2.5e-05.
===> Epoch[206](270/324): Loss: 0.7957 || Learning rate: lr=2.5e-05.
===> Epoch[206](280/324): Loss: 0.6135 || Learning rate: lr=2.5e-05.
===> Epoch[206](290/324): Loss: 0.5174 || Learning rate: lr=2.5e-05.
===> Epoch[206](300/324): Loss: 0.6408 || Learning rate: lr=2.5e-05.
===> Epoch[206](310/324): Loss: 0.6901 || Learning rate: lr=2.5e-05.
===> Epoch[206](320/324): Loss: 0.8860 || Learning rate: lr=2.5e-05.
===> Epoch[207](10/324): Loss: 0.7242 || Learning rate: lr=2.5e-05.
===> Epoch[207](20/324): Loss: 0.9187 || Learning rate: lr=2.5e-05.
===> Epoch[207](30/324): Loss: 0.5949 || Learning rate: lr=2.5e-05.
===> Epoch[207](40/324): Loss: 0.8568 || Learning rate: lr=2.5e-05.
===> Epoch[207](50/324): Loss: 0.4233 || Learning rate: lr=2.5e-05.
===> Epoch[207](60/324): Loss: 0.6962 || Learning rate: lr=2.5e-05.
===> Epoch[207](70/324): Loss: 0.6793 || Learning rate: lr=2.5e-05.
===> Epoch[207](80/324): Loss: 0.6822 || Learning rate: lr=2.5e-05.
===> Epoch[207](90/324): Loss: 0.5419 || Learning rate: lr=2.5e-05.
===> Epoch[207](100/324): Loss: 0.5112 || Learning rate: lr=2.5e-05.
===> Epoch[207](110/324): Loss: 0.6960 || Learning rate: lr=2.5e-05.
===> Epoch[207](120/324): Loss: 0.5154 || Learning rate: lr=2.5e-05.
===> Epoch[207](130/324): Loss: 0.7047 || Learning rate: lr=2.5e-05.
===> Epoch[207](140/324): Loss: 0.7483 || Learning rate: lr=2.5e-05.
===> Epoch[207](150/324): Loss: 0.6553 || Learning rate: lr=2.5e-05.
===> Epoch[207](160/324): Loss: 0.6410 || Learning rate: lr=2.5e-05.
===> Epoch[207](170/324): Loss: 0.9799 || Learning rate: lr=2.5e-05.
===> Epoch[207](180/324): Loss: 0.9783 || Learning rate: lr=2.5e-05.
===> Epoch[207](190/324): Loss: 0.6095 || Learning rate: lr=2.5e-05.
===> Epoch[207](200/324): Loss: 0.7083 || Learning rate: lr=2.5e-05.
===> Epoch[207](210/324): Loss: 0.8208 || Learning rate: lr=2.5e-05.
===> Epoch[207](220/324): Loss: 0.5928 || Learning rate: lr=2.5e-05.
===> Epoch[207](230/324): Loss: 0.5858 || Learning rate: lr=2.5e-05.
===> Epoch[207](240/324): Loss: 0.9384 || Learning rate: lr=2.5e-05.
===> Epoch[207](250/324): Loss: 0.6225 || Learning rate: lr=2.5e-05.
===> Epoch[207](260/324): Loss: 0.8090 || Learning rate: lr=2.5e-05.
===> Epoch[207](270/324): Loss: 0.5864 || Learning rate: lr=2.5e-05.
===> Epoch[207](280/324): Loss: 0.8729 || Learning rate: lr=2.5e-05.
===> Epoch[207](290/324): Loss: 0.7725 || Learning rate: lr=2.5e-05.
===> Epoch[207](300/324): Loss: 0.6601 || Learning rate: lr=2.5e-05.
===> Epoch[207](310/324): Loss: 0.6690 || Learning rate: lr=2.5e-05.
===> Epoch[207](320/324): Loss: 0.6131 || Learning rate: lr=2.5e-05.
===> Epoch[208](10/324): Loss: 0.5372 || Learning rate: lr=2.5e-05.
===> Epoch[208](20/324): Loss: 0.6034 || Learning rate: lr=2.5e-05.
===> Epoch[208](30/324): Loss: 0.8735 || Learning rate: lr=2.5e-05.
===> Epoch[208](40/324): Loss: 0.7039 || Learning rate: lr=2.5e-05.
===> Epoch[208](50/324): Loss: 0.8254 || Learning rate: lr=2.5e-05.
===> Epoch[208](60/324): Loss: 0.8183 || Learning rate: lr=2.5e-05.
===> Epoch[208](70/324): Loss: 0.9632 || Learning rate: lr=2.5e-05.
===> Epoch[208](80/324): Loss: 0.6962 || Learning rate: lr=2.5e-05.
===> Epoch[208](90/324): Loss: 0.6374 || Learning rate: lr=2.5e-05.
===> Epoch[208](100/324): Loss: 1.1699 || Learning rate: lr=2.5e-05.
===> Epoch[208](110/324): Loss: 0.3895 || Learning rate: lr=2.5e-05.
===> Epoch[208](120/324): Loss: 0.6141 || Learning rate: lr=2.5e-05.
===> Epoch[208](130/324): Loss: 0.5931 || Learning rate: lr=2.5e-05.
===> Epoch[208](140/324): Loss: 0.7775 || Learning rate: lr=2.5e-05.
===> Epoch[208](150/324): Loss: 0.8598 || Learning rate: lr=2.5e-05.
===> Epoch[208](160/324): Loss: 0.6915 || Learning rate: lr=2.5e-05.
===> Epoch[208](170/324): Loss: 0.5896 || Learning rate: lr=2.5e-05.
===> Epoch[208](180/324): Loss: 0.5198 || Learning rate: lr=2.5e-05.
===> Epoch[208](190/324): Loss: 0.6494 || Learning rate: lr=2.5e-05.
===> Epoch[208](200/324): Loss: 0.6160 || Learning rate: lr=2.5e-05.
===> Epoch[208](210/324): Loss: 0.6193 || Learning rate: lr=2.5e-05.
===> Epoch[208](220/324): Loss: 0.5746 || Learning rate: lr=2.5e-05.
===> Epoch[208](230/324): Loss: 0.7021 || Learning rate: lr=2.5e-05.
===> Epoch[208](240/324): Loss: 0.5123 || Learning rate: lr=2.5e-05.
===> Epoch[208](250/324): Loss: 0.5987 || Learning rate: lr=2.5e-05.
===> Epoch[208](260/324): Loss: 0.5359 || Learning rate: lr=2.5e-05.
===> Epoch[208](270/324): Loss: 0.9361 || Learning rate: lr=2.5e-05.
===> Epoch[208](280/324): Loss: 0.4225 || Learning rate: lr=2.5e-05.
===> Epoch[208](290/324): Loss: 0.9189 || Learning rate: lr=2.5e-05.
===> Epoch[208](300/324): Loss: 0.7007 || Learning rate: lr=2.5e-05.
===> Epoch[208](310/324): Loss: 0.6923 || Learning rate: lr=2.5e-05.
===> Epoch[208](320/324): Loss: 0.5155 || Learning rate: lr=2.5e-05.
===> Epoch[209](10/324): Loss: 0.4610 || Learning rate: lr=2.5e-05.
===> Epoch[209](20/324): Loss: 0.7383 || Learning rate: lr=2.5e-05.
===> Epoch[209](30/324): Loss: 1.0203 || Learning rate: lr=2.5e-05.
===> Epoch[209](40/324): Loss: 0.3987 || Learning rate: lr=2.5e-05.
===> Epoch[209](50/324): Loss: 0.9226 || Learning rate: lr=2.5e-05.
===> Epoch[209](60/324): Loss: 0.6678 || Learning rate: lr=2.5e-05.
===> Epoch[209](70/324): Loss: 0.9089 || Learning rate: lr=2.5e-05.
===> Epoch[209](80/324): Loss: 0.4707 || Learning rate: lr=2.5e-05.
===> Epoch[209](90/324): Loss: 0.6795 || Learning rate: lr=2.5e-05.
===> Epoch[209](100/324): Loss: 0.6088 || Learning rate: lr=2.5e-05.
===> Epoch[209](110/324): Loss: 0.4472 || Learning rate: lr=2.5e-05.
===> Epoch[209](120/324): Loss: 0.5995 || Learning rate: lr=2.5e-05.
===> Epoch[209](130/324): Loss: 0.4240 || Learning rate: lr=2.5e-05.
===> Epoch[209](140/324): Loss: 0.6345 || Learning rate: lr=2.5e-05.
===> Epoch[209](150/324): Loss: 0.6169 || Learning rate: lr=2.5e-05.
===> Epoch[209](160/324): Loss: 0.8926 || Learning rate: lr=2.5e-05.
===> Epoch[209](170/324): Loss: 0.4765 || Learning rate: lr=2.5e-05.
===> Epoch[209](180/324): Loss: 0.5126 || Learning rate: lr=2.5e-05.
===> Epoch[209](190/324): Loss: 0.6969 || Learning rate: lr=2.5e-05.
===> Epoch[209](200/324): Loss: 0.5747 || Learning rate: lr=2.5e-05.
===> Epoch[209](210/324): Loss: 0.5662 || Learning rate: lr=2.5e-05.
===> Epoch[209](220/324): Loss: 0.7637 || Learning rate: lr=2.5e-05.
===> Epoch[209](230/324): Loss: 0.8055 || Learning rate: lr=2.5e-05.
===> Epoch[209](240/324): Loss: 0.8480 || Learning rate: lr=2.5e-05.
===> Epoch[209](250/324): Loss: 1.2912 || Learning rate: lr=2.5e-05.
===> Epoch[209](260/324): Loss: 0.7558 || Learning rate: lr=2.5e-05.
===> Epoch[209](270/324): Loss: 0.5939 || Learning rate: lr=2.5e-05.
===> Epoch[209](280/324): Loss: 0.7070 || Learning rate: lr=2.5e-05.
===> Epoch[209](290/324): Loss: 1.0908 || Learning rate: lr=2.5e-05.
===> Epoch[209](300/324): Loss: 1.0754 || Learning rate: lr=2.5e-05.
===> Epoch[209](310/324): Loss: 0.6578 || Learning rate: lr=2.5e-05.
===> Epoch[209](320/324): Loss: 0.7545 || Learning rate: lr=2.5e-05.
===> Epoch[210](10/324): Loss: 0.6620 || Learning rate: lr=2.5e-05.
===> Epoch[210](20/324): Loss: 1.0126 || Learning rate: lr=2.5e-05.
===> Epoch[210](30/324): Loss: 0.7515 || Learning rate: lr=2.5e-05.
===> Epoch[210](40/324): Loss: 0.6545 || Learning rate: lr=2.5e-05.
===> Epoch[210](50/324): Loss: 0.7360 || Learning rate: lr=2.5e-05.
===> Epoch[210](60/324): Loss: 0.8215 || Learning rate: lr=2.5e-05.
===> Epoch[210](70/324): Loss: 0.7122 || Learning rate: lr=2.5e-05.
===> Epoch[210](80/324): Loss: 0.4244 || Learning rate: lr=2.5e-05.
===> Epoch[210](90/324): Loss: 0.6878 || Learning rate: lr=2.5e-05.
===> Epoch[210](100/324): Loss: 0.9314 || Learning rate: lr=2.5e-05.
===> Epoch[210](110/324): Loss: 0.5348 || Learning rate: lr=2.5e-05.
===> Epoch[210](120/324): Loss: 0.5517 || Learning rate: lr=2.5e-05.
===> Epoch[210](130/324): Loss: 0.6566 || Learning rate: lr=2.5e-05.
===> Epoch[210](140/324): Loss: 0.8121 || Learning rate: lr=2.5e-05.
===> Epoch[210](150/324): Loss: 0.8312 || Learning rate: lr=2.5e-05.
===> Epoch[210](160/324): Loss: 0.5643 || Learning rate: lr=2.5e-05.
===> Epoch[210](170/324): Loss: 0.5907 || Learning rate: lr=2.5e-05.
===> Epoch[210](180/324): Loss: 0.6682 || Learning rate: lr=2.5e-05.
===> Epoch[210](190/324): Loss: 0.6148 || Learning rate: lr=2.5e-05.
===> Epoch[210](200/324): Loss: 0.8950 || Learning rate: lr=2.5e-05.
===> Epoch[210](210/324): Loss: 0.6406 || Learning rate: lr=2.5e-05.
===> Epoch[210](220/324): Loss: 0.9322 || Learning rate: lr=2.5e-05.
===> Epoch[210](230/324): Loss: 0.7548 || Learning rate: lr=2.5e-05.
===> Epoch[210](240/324): Loss: 0.8075 || Learning rate: lr=2.5e-05.
===> Epoch[210](250/324): Loss: 0.5793 || Learning rate: lr=2.5e-05.
===> Epoch[210](260/324): Loss: 0.7650 || Learning rate: lr=2.5e-05.
===> Epoch[210](270/324): Loss: 0.7543 || Learning rate: lr=2.5e-05.
===> Epoch[210](280/324): Loss: 0.5427 || Learning rate: lr=2.5e-05.
===> Epoch[210](290/324): Loss: 0.5923 || Learning rate: lr=2.5e-05.
===> Epoch[210](300/324): Loss: 0.5915 || Learning rate: lr=2.5e-05.
===> Epoch[210](310/324): Loss: 0.4655 || Learning rate: lr=2.5e-05.
===> Epoch[210](320/324): Loss: 0.9814 || Learning rate: lr=2.5e-05.
===> Epoch[211](10/324): Loss: 0.5670 || Learning rate: lr=2.5e-05.
===> Epoch[211](20/324): Loss: 0.6701 || Learning rate: lr=2.5e-05.
===> Epoch[211](30/324): Loss: 0.6946 || Learning rate: lr=2.5e-05.
===> Epoch[211](40/324): Loss: 0.8305 || Learning rate: lr=2.5e-05.
===> Epoch[211](50/324): Loss: 0.7906 || Learning rate: lr=2.5e-05.
===> Epoch[211](60/324): Loss: 0.8191 || Learning rate: lr=2.5e-05.
===> Epoch[211](70/324): Loss: 0.5840 || Learning rate: lr=2.5e-05.
===> Epoch[211](80/324): Loss: 0.4166 || Learning rate: lr=2.5e-05.
===> Epoch[211](90/324): Loss: 0.5363 || Learning rate: lr=2.5e-05.
===> Epoch[211](100/324): Loss: 0.9858 || Learning rate: lr=2.5e-05.
===> Epoch[211](110/324): Loss: 0.7143 || Learning rate: lr=2.5e-05.
===> Epoch[211](120/324): Loss: 0.7485 || Learning rate: lr=2.5e-05.
===> Epoch[211](130/324): Loss: 0.5836 || Learning rate: lr=2.5e-05.
===> Epoch[211](140/324): Loss: 0.7610 || Learning rate: lr=2.5e-05.
===> Epoch[211](150/324): Loss: 0.7142 || Learning rate: lr=2.5e-05.
===> Epoch[211](160/324): Loss: 0.6018 || Learning rate: lr=2.5e-05.
===> Epoch[211](170/324): Loss: 0.6094 || Learning rate: lr=2.5e-05.
===> Epoch[211](180/324): Loss: 0.5652 || Learning rate: lr=2.5e-05.
===> Epoch[211](190/324): Loss: 0.6763 || Learning rate: lr=2.5e-05.
===> Epoch[211](200/324): Loss: 0.8015 || Learning rate: lr=2.5e-05.
===> Epoch[211](210/324): Loss: 0.9483 || Learning rate: lr=2.5e-05.
===> Epoch[211](220/324): Loss: 0.4232 || Learning rate: lr=2.5e-05.
===> Epoch[211](230/324): Loss: 0.6159 || Learning rate: lr=2.5e-05.
===> Epoch[211](240/324): Loss: 0.6939 || Learning rate: lr=2.5e-05.
===> Epoch[211](250/324): Loss: 0.5800 || Learning rate: lr=2.5e-05.
===> Epoch[211](260/324): Loss: 0.4164 || Learning rate: lr=2.5e-05.
===> Epoch[211](270/324): Loss: 0.8359 || Learning rate: lr=2.5e-05.
===> Epoch[211](280/324): Loss: 0.8074 || Learning rate: lr=2.5e-05.
===> Epoch[211](290/324): Loss: 0.6555 || Learning rate: lr=2.5e-05.
===> Epoch[211](300/324): Loss: 0.6490 || Learning rate: lr=2.5e-05.
===> Epoch[211](310/324): Loss: 0.7607 || Learning rate: lr=2.5e-05.
===> Epoch[211](320/324): Loss: 0.6383 || Learning rate: lr=2.5e-05.
===> Epoch[212](10/324): Loss: 0.8016 || Learning rate: lr=2.5e-05.
===> Epoch[212](20/324): Loss: 0.8605 || Learning rate: lr=2.5e-05.
===> Epoch[212](30/324): Loss: 0.5424 || Learning rate: lr=2.5e-05.
===> Epoch[212](40/324): Loss: 0.6243 || Learning rate: lr=2.5e-05.
===> Epoch[212](50/324): Loss: 0.7604 || Learning rate: lr=2.5e-05.
===> Epoch[212](60/324): Loss: 0.5894 || Learning rate: lr=2.5e-05.
===> Epoch[212](70/324): Loss: 0.6340 || Learning rate: lr=2.5e-05.
===> Epoch[212](80/324): Loss: 0.4479 || Learning rate: lr=2.5e-05.
===> Epoch[212](90/324): Loss: 0.5999 || Learning rate: lr=2.5e-05.
===> Epoch[212](100/324): Loss: 0.6166 || Learning rate: lr=2.5e-05.
===> Epoch[212](110/324): Loss: 0.8172 || Learning rate: lr=2.5e-05.
===> Epoch[212](120/324): Loss: 0.7054 || Learning rate: lr=2.5e-05.
===> Epoch[212](130/324): Loss: 0.6145 || Learning rate: lr=2.5e-05.
===> Epoch[212](140/324): Loss: 0.7630 || Learning rate: lr=2.5e-05.
===> Epoch[212](150/324): Loss: 0.5698 || Learning rate: lr=2.5e-05.
===> Epoch[212](160/324): Loss: 0.6733 || Learning rate: lr=2.5e-05.
===> Epoch[212](170/324): Loss: 0.6806 || Learning rate: lr=2.5e-05.
===> Epoch[212](180/324): Loss: 0.5649 || Learning rate: lr=2.5e-05.
===> Epoch[212](190/324): Loss: 0.6285 || Learning rate: lr=2.5e-05.
===> Epoch[212](200/324): Loss: 0.6370 || Learning rate: lr=2.5e-05.
===> Epoch[212](210/324): Loss: 0.4579 || Learning rate: lr=2.5e-05.
===> Epoch[212](220/324): Loss: 0.6044 || Learning rate: lr=2.5e-05.
===> Epoch[212](230/324): Loss: 0.8208 || Learning rate: lr=2.5e-05.
===> Epoch[212](240/324): Loss: 0.8280 || Learning rate: lr=2.5e-05.
===> Epoch[212](250/324): Loss: 0.8722 || Learning rate: lr=2.5e-05.
===> Epoch[212](260/324): Loss: 1.0370 || Learning rate: lr=2.5e-05.
===> Epoch[212](270/324): Loss: 0.5925 || Learning rate: lr=2.5e-05.
===> Epoch[212](280/324): Loss: 0.6666 || Learning rate: lr=2.5e-05.
===> Epoch[212](290/324): Loss: 0.9692 || Learning rate: lr=2.5e-05.
===> Epoch[212](300/324): Loss: 0.7284 || Learning rate: lr=2.5e-05.
===> Epoch[212](310/324): Loss: 0.8150 || Learning rate: lr=2.5e-05.
===> Epoch[212](320/324): Loss: 0.8194 || Learning rate: lr=2.5e-05.
===> Epoch[213](10/324): Loss: 0.8778 || Learning rate: lr=2.5e-05.
===> Epoch[213](20/324): Loss: 0.9773 || Learning rate: lr=2.5e-05.
===> Epoch[213](30/324): Loss: 0.7457 || Learning rate: lr=2.5e-05.
===> Epoch[213](40/324): Loss: 0.6517 || Learning rate: lr=2.5e-05.
===> Epoch[213](50/324): Loss: 0.8033 || Learning rate: lr=2.5e-05.
===> Epoch[213](60/324): Loss: 0.5926 || Learning rate: lr=2.5e-05.
===> Epoch[213](70/324): Loss: 0.5929 || Learning rate: lr=2.5e-05.
===> Epoch[213](80/324): Loss: 0.7350 || Learning rate: lr=2.5e-05.
===> Epoch[213](90/324): Loss: 0.4673 || Learning rate: lr=2.5e-05.
===> Epoch[213](100/324): Loss: 0.8482 || Learning rate: lr=2.5e-05.
===> Epoch[213](110/324): Loss: 0.7906 || Learning rate: lr=2.5e-05.
===> Epoch[213](120/324): Loss: 0.6166 || Learning rate: lr=2.5e-05.
===> Epoch[213](130/324): Loss: 0.6464 || Learning rate: lr=2.5e-05.
===> Epoch[213](140/324): Loss: 1.1339 || Learning rate: lr=2.5e-05.
===> Epoch[213](150/324): Loss: 0.8677 || Learning rate: lr=2.5e-05.
===> Epoch[213](160/324): Loss: 0.6305 || Learning rate: lr=2.5e-05.
===> Epoch[213](170/324): Loss: 0.8753 || Learning rate: lr=2.5e-05.
===> Epoch[213](180/324): Loss: 0.7926 || Learning rate: lr=2.5e-05.
===> Epoch[213](190/324): Loss: 0.4683 || Learning rate: lr=2.5e-05.
===> Epoch[213](200/324): Loss: 0.7868 || Learning rate: lr=2.5e-05.
===> Epoch[213](210/324): Loss: 0.5713 || Learning rate: lr=2.5e-05.
===> Epoch[213](220/324): Loss: 0.5174 || Learning rate: lr=2.5e-05.
===> Epoch[213](230/324): Loss: 0.6153 || Learning rate: lr=2.5e-05.
===> Epoch[213](240/324): Loss: 0.9349 || Learning rate: lr=2.5e-05.
===> Epoch[213](250/324): Loss: 0.9530 || Learning rate: lr=2.5e-05.
===> Epoch[213](260/324): Loss: 0.6729 || Learning rate: lr=2.5e-05.
===> Epoch[213](270/324): Loss: 0.5431 || Learning rate: lr=2.5e-05.
===> Epoch[213](280/324): Loss: 0.7283 || Learning rate: lr=2.5e-05.
===> Epoch[213](290/324): Loss: 0.7597 || Learning rate: lr=2.5e-05.
===> Epoch[213](300/324): Loss: 0.5159 || Learning rate: lr=2.5e-05.
===> Epoch[213](310/324): Loss: 0.4813 || Learning rate: lr=2.5e-05.
===> Epoch[213](320/324): Loss: 0.7452 || Learning rate: lr=2.5e-05.
===> Epoch[214](10/324): Loss: 0.5572 || Learning rate: lr=2.5e-05.
===> Epoch[214](20/324): Loss: 0.6802 || Learning rate: lr=2.5e-05.
===> Epoch[214](30/324): Loss: 0.6680 || Learning rate: lr=2.5e-05.
===> Epoch[214](40/324): Loss: 0.8250 || Learning rate: lr=2.5e-05.
===> Epoch[214](50/324): Loss: 0.9841 || Learning rate: lr=2.5e-05.
===> Epoch[214](60/324): Loss: 0.4359 || Learning rate: lr=2.5e-05.
===> Epoch[214](70/324): Loss: 0.8782 || Learning rate: lr=2.5e-05.
===> Epoch[214](80/324): Loss: 0.6222 || Learning rate: lr=2.5e-05.
===> Epoch[214](90/324): Loss: 0.6392 || Learning rate: lr=2.5e-05.
===> Epoch[214](100/324): Loss: 0.7623 || Learning rate: lr=2.5e-05.
===> Epoch[214](110/324): Loss: 0.7647 || Learning rate: lr=2.5e-05.
===> Epoch[214](120/324): Loss: 0.5704 || Learning rate: lr=2.5e-05.
===> Epoch[214](130/324): Loss: 0.6123 || Learning rate: lr=2.5e-05.
===> Epoch[214](140/324): Loss: 0.4477 || Learning rate: lr=2.5e-05.
===> Epoch[214](150/324): Loss: 0.8047 || Learning rate: lr=2.5e-05.
===> Epoch[214](160/324): Loss: 0.7329 || Learning rate: lr=2.5e-05.
===> Epoch[214](170/324): Loss: 0.6996 || Learning rate: lr=2.5e-05.
===> Epoch[214](180/324): Loss: 0.7600 || Learning rate: lr=2.5e-05.
===> Epoch[214](190/324): Loss: 0.7811 || Learning rate: lr=2.5e-05.
===> Epoch[214](200/324): Loss: 0.6277 || Learning rate: lr=2.5e-05.
===> Epoch[214](210/324): Loss: 1.1277 || Learning rate: lr=2.5e-05.
===> Epoch[214](220/324): Loss: 0.5841 || Learning rate: lr=2.5e-05.
===> Epoch[214](230/324): Loss: 0.6714 || Learning rate: lr=2.5e-05.
===> Epoch[214](240/324): Loss: 0.7729 || Learning rate: lr=2.5e-05.
===> Epoch[214](250/324): Loss: 1.0677 || Learning rate: lr=2.5e-05.
===> Epoch[214](260/324): Loss: 0.5232 || Learning rate: lr=2.5e-05.
===> Epoch[214](270/324): Loss: 0.4207 || Learning rate: lr=2.5e-05.
===> Epoch[214](280/324): Loss: 0.5495 || Learning rate: lr=2.5e-05.
===> Epoch[214](290/324): Loss: 0.6309 || Learning rate: lr=2.5e-05.
===> Epoch[214](300/324): Loss: 0.7137 || Learning rate: lr=2.5e-05.
===> Epoch[214](310/324): Loss: 0.7880 || Learning rate: lr=2.5e-05.
===> Epoch[214](320/324): Loss: 0.5785 || Learning rate: lr=2.5e-05.
===> Epoch[215](10/324): Loss: 0.6094 || Learning rate: lr=2.5e-05.
===> Epoch[215](20/324): Loss: 0.5531 || Learning rate: lr=2.5e-05.
===> Epoch[215](30/324): Loss: 0.5182 || Learning rate: lr=2.5e-05.
===> Epoch[215](40/324): Loss: 0.5819 || Learning rate: lr=2.5e-05.
===> Epoch[215](50/324): Loss: 1.0596 || Learning rate: lr=2.5e-05.
===> Epoch[215](60/324): Loss: 0.6397 || Learning rate: lr=2.5e-05.
===> Epoch[215](70/324): Loss: 0.5886 || Learning rate: lr=2.5e-05.
===> Epoch[215](80/324): Loss: 0.5989 || Learning rate: lr=2.5e-05.
===> Epoch[215](90/324): Loss: 0.9367 || Learning rate: lr=2.5e-05.
===> Epoch[215](100/324): Loss: 0.6174 || Learning rate: lr=2.5e-05.
===> Epoch[215](110/324): Loss: 0.5675 || Learning rate: lr=2.5e-05.
===> Epoch[215](120/324): Loss: 0.5871 || Learning rate: lr=2.5e-05.
===> Epoch[215](130/324): Loss: 0.6419 || Learning rate: lr=2.5e-05.
===> Epoch[215](140/324): Loss: 0.8710 || Learning rate: lr=2.5e-05.
===> Epoch[215](150/324): Loss: 1.1667 || Learning rate: lr=2.5e-05.
===> Epoch[215](160/324): Loss: 0.4862 || Learning rate: lr=2.5e-05.
===> Epoch[215](170/324): Loss: 0.9206 || Learning rate: lr=2.5e-05.
===> Epoch[215](180/324): Loss: 0.5299 || Learning rate: lr=2.5e-05.
===> Epoch[215](190/324): Loss: 0.6437 || Learning rate: lr=2.5e-05.
===> Epoch[215](200/324): Loss: 0.5318 || Learning rate: lr=2.5e-05.
===> Epoch[215](210/324): Loss: 0.8947 || Learning rate: lr=2.5e-05.
===> Epoch[215](220/324): Loss: 0.4818 || Learning rate: lr=2.5e-05.
===> Epoch[215](230/324): Loss: 1.0285 || Learning rate: lr=2.5e-05.
===> Epoch[215](240/324): Loss: 0.6962 || Learning rate: lr=2.5e-05.
===> Epoch[215](250/324): Loss: 0.4973 || Learning rate: lr=2.5e-05.
===> Epoch[215](260/324): Loss: 0.5861 || Learning rate: lr=2.5e-05.
===> Epoch[215](270/324): Loss: 0.6850 || Learning rate: lr=2.5e-05.
===> Epoch[215](280/324): Loss: 0.9931 || Learning rate: lr=2.5e-05.
===> Epoch[215](290/324): Loss: 0.5986 || Learning rate: lr=2.5e-05.
===> Epoch[215](300/324): Loss: 0.5462 || Learning rate: lr=2.5e-05.
===> Epoch[215](310/324): Loss: 0.5200 || Learning rate: lr=2.5e-05.
===> Epoch[215](320/324): Loss: 0.4717 || Learning rate: lr=2.5e-05.
===> Epoch[216](10/324): Loss: 0.9096 || Learning rate: lr=2.5e-05.
===> Epoch[216](20/324): Loss: 0.6468 || Learning rate: lr=2.5e-05.
===> Epoch[216](30/324): Loss: 0.8456 || Learning rate: lr=2.5e-05.
===> Epoch[216](40/324): Loss: 0.4889 || Learning rate: lr=2.5e-05.
===> Epoch[216](50/324): Loss: 0.7430 || Learning rate: lr=2.5e-05.
===> Epoch[216](60/324): Loss: 0.7168 || Learning rate: lr=2.5e-05.
===> Epoch[216](70/324): Loss: 0.7222 || Learning rate: lr=2.5e-05.
===> Epoch[216](80/324): Loss: 0.5281 || Learning rate: lr=2.5e-05.
===> Epoch[216](90/324): Loss: 0.7299 || Learning rate: lr=2.5e-05.
===> Epoch[216](100/324): Loss: 0.4918 || Learning rate: lr=2.5e-05.
===> Epoch[216](110/324): Loss: 0.9092 || Learning rate: lr=2.5e-05.
===> Epoch[216](120/324): Loss: 0.9699 || Learning rate: lr=2.5e-05.
===> Epoch[216](130/324): Loss: 0.9105 || Learning rate: lr=2.5e-05.
===> Epoch[216](140/324): Loss: 0.5720 || Learning rate: lr=2.5e-05.
===> Epoch[216](150/324): Loss: 0.5819 || Learning rate: lr=2.5e-05.
===> Epoch[216](160/324): Loss: 0.4357 || Learning rate: lr=2.5e-05.
===> Epoch[216](170/324): Loss: 0.7982 || Learning rate: lr=2.5e-05.
===> Epoch[216](180/324): Loss: 0.6970 || Learning rate: lr=2.5e-05.
===> Epoch[216](190/324): Loss: 0.7392 || Learning rate: lr=2.5e-05.
===> Epoch[216](200/324): Loss: 0.6285 || Learning rate: lr=2.5e-05.
===> Epoch[216](210/324): Loss: 0.7031 || Learning rate: lr=2.5e-05.
===> Epoch[216](220/324): Loss: 0.6917 || Learning rate: lr=2.5e-05.
===> Epoch[216](230/324): Loss: 0.6441 || Learning rate: lr=2.5e-05.
===> Epoch[216](240/324): Loss: 0.5451 || Learning rate: lr=2.5e-05.
===> Epoch[216](250/324): Loss: 1.1849 || Learning rate: lr=2.5e-05.
===> Epoch[216](260/324): Loss: 0.7464 || Learning rate: lr=2.5e-05.
===> Epoch[216](270/324): Loss: 0.6604 || Learning rate: lr=2.5e-05.
===> Epoch[216](280/324): Loss: 0.4655 || Learning rate: lr=2.5e-05.
===> Epoch[216](290/324): Loss: 0.9518 || Learning rate: lr=2.5e-05.
===> Epoch[216](300/324): Loss: 0.6277 || Learning rate: lr=2.5e-05.
===> Epoch[216](310/324): Loss: 0.6463 || Learning rate: lr=2.5e-05.
===> Epoch[216](320/324): Loss: 0.7475 || Learning rate: lr=2.5e-05.
===> Epoch[217](10/324): Loss: 0.9539 || Learning rate: lr=2.5e-05.
===> Epoch[217](20/324): Loss: 0.8386 || Learning rate: lr=2.5e-05.
===> Epoch[217](30/324): Loss: 0.8493 || Learning rate: lr=2.5e-05.
===> Epoch[217](40/324): Loss: 0.5894 || Learning rate: lr=2.5e-05.
===> Epoch[217](50/324): Loss: 0.5272 || Learning rate: lr=2.5e-05.
===> Epoch[217](60/324): Loss: 0.7475 || Learning rate: lr=2.5e-05.
===> Epoch[217](70/324): Loss: 0.6927 || Learning rate: lr=2.5e-05.
===> Epoch[217](80/324): Loss: 0.5412 || Learning rate: lr=2.5e-05.
===> Epoch[217](90/324): Loss: 0.8261 || Learning rate: lr=2.5e-05.
===> Epoch[217](100/324): Loss: 0.5968 || Learning rate: lr=2.5e-05.
===> Epoch[217](110/324): Loss: 0.5275 || Learning rate: lr=2.5e-05.
===> Epoch[217](120/324): Loss: 0.5816 || Learning rate: lr=2.5e-05.
===> Epoch[217](130/324): Loss: 0.7553 || Learning rate: lr=2.5e-05.
===> Epoch[217](140/324): Loss: 0.6391 || Learning rate: lr=2.5e-05.
===> Epoch[217](150/324): Loss: 0.9219 || Learning rate: lr=2.5e-05.
===> Epoch[217](160/324): Loss: 0.4191 || Learning rate: lr=2.5e-05.
===> Epoch[217](170/324): Loss: 0.5836 || Learning rate: lr=2.5e-05.
===> Epoch[217](180/324): Loss: 0.7159 || Learning rate: lr=2.5e-05.
===> Epoch[217](190/324): Loss: 0.7533 || Learning rate: lr=2.5e-05.
===> Epoch[217](200/324): Loss: 0.8889 || Learning rate: lr=2.5e-05.
===> Epoch[217](210/324): Loss: 0.6315 || Learning rate: lr=2.5e-05.
===> Epoch[217](220/324): Loss: 0.5826 || Learning rate: lr=2.5e-05.
===> Epoch[217](230/324): Loss: 0.6540 || Learning rate: lr=2.5e-05.
===> Epoch[217](240/324): Loss: 0.6022 || Learning rate: lr=2.5e-05.
===> Epoch[217](250/324): Loss: 0.6713 || Learning rate: lr=2.5e-05.
===> Epoch[217](260/324): Loss: 0.5515 || Learning rate: lr=2.5e-05.
===> Epoch[217](270/324): Loss: 0.6687 || Learning rate: lr=2.5e-05.
===> Epoch[217](280/324): Loss: 0.7609 || Learning rate: lr=2.5e-05.
===> Epoch[217](290/324): Loss: 0.8892 || Learning rate: lr=2.5e-05.
===> Epoch[217](300/324): Loss: 0.6539 || Learning rate: lr=2.5e-05.
===> Epoch[217](310/324): Loss: 0.8329 || Learning rate: lr=2.5e-05.
===> Epoch[217](320/324): Loss: 0.5857 || Learning rate: lr=2.5e-05.
===> Epoch[218](10/324): Loss: 0.4396 || Learning rate: lr=2.5e-05.
===> Epoch[218](20/324): Loss: 0.6140 || Learning rate: lr=2.5e-05.
===> Epoch[218](30/324): Loss: 0.7854 || Learning rate: lr=2.5e-05.
===> Epoch[218](40/324): Loss: 0.6512 || Learning rate: lr=2.5e-05.
===> Epoch[218](50/324): Loss: 0.8360 || Learning rate: lr=2.5e-05.
===> Epoch[218](60/324): Loss: 1.1361 || Learning rate: lr=2.5e-05.
===> Epoch[218](70/324): Loss: 0.7272 || Learning rate: lr=2.5e-05.
===> Epoch[218](80/324): Loss: 0.6810 || Learning rate: lr=2.5e-05.
===> Epoch[218](90/324): Loss: 0.5992 || Learning rate: lr=2.5e-05.
===> Epoch[218](100/324): Loss: 0.7643 || Learning rate: lr=2.5e-05.
===> Epoch[218](110/324): Loss: 0.6918 || Learning rate: lr=2.5e-05.
===> Epoch[218](120/324): Loss: 0.7775 || Learning rate: lr=2.5e-05.
===> Epoch[218](130/324): Loss: 0.6157 || Learning rate: lr=2.5e-05.
===> Epoch[218](140/324): Loss: 0.5024 || Learning rate: lr=2.5e-05.
===> Epoch[218](150/324): Loss: 0.7024 || Learning rate: lr=2.5e-05.
===> Epoch[218](160/324): Loss: 0.8127 || Learning rate: lr=2.5e-05.
===> Epoch[218](170/324): Loss: 0.7552 || Learning rate: lr=2.5e-05.
===> Epoch[218](180/324): Loss: 0.7976 || Learning rate: lr=2.5e-05.
===> Epoch[218](190/324): Loss: 0.5210 || Learning rate: lr=2.5e-05.
===> Epoch[218](200/324): Loss: 0.7421 || Learning rate: lr=2.5e-05.
===> Epoch[218](210/324): Loss: 0.5887 || Learning rate: lr=2.5e-05.
===> Epoch[218](220/324): Loss: 0.5509 || Learning rate: lr=2.5e-05.
===> Epoch[218](230/324): Loss: 0.7170 || Learning rate: lr=2.5e-05.
===> Epoch[218](240/324): Loss: 0.6880 || Learning rate: lr=2.5e-05.
===> Epoch[218](250/324): Loss: 0.6573 || Learning rate: lr=2.5e-05.
===> Epoch[218](260/324): Loss: 0.9174 || Learning rate: lr=2.5e-05.
===> Epoch[218](270/324): Loss: 0.7440 || Learning rate: lr=2.5e-05.
===> Epoch[218](280/324): Loss: 0.5914 || Learning rate: lr=2.5e-05.
===> Epoch[218](290/324): Loss: 0.6974 || Learning rate: lr=2.5e-05.
===> Epoch[218](300/324): Loss: 0.6301 || Learning rate: lr=2.5e-05.
===> Epoch[218](310/324): Loss: 0.6156 || Learning rate: lr=2.5e-05.
===> Epoch[218](320/324): Loss: 0.6539 || Learning rate: lr=2.5e-05.
===> Epoch[219](10/324): Loss: 0.8003 || Learning rate: lr=2.5e-05.
===> Epoch[219](20/324): Loss: 0.9985 || Learning rate: lr=2.5e-05.
===> Epoch[219](30/324): Loss: 0.5933 || Learning rate: lr=2.5e-05.
===> Epoch[219](40/324): Loss: 0.6781 || Learning rate: lr=2.5e-05.
===> Epoch[219](50/324): Loss: 0.5476 || Learning rate: lr=2.5e-05.
===> Epoch[219](60/324): Loss: 0.8182 || Learning rate: lr=2.5e-05.
===> Epoch[219](70/324): Loss: 0.6866 || Learning rate: lr=2.5e-05.
===> Epoch[219](80/324): Loss: 0.6116 || Learning rate: lr=2.5e-05.
===> Epoch[219](90/324): Loss: 0.5343 || Learning rate: lr=2.5e-05.
===> Epoch[219](100/324): Loss: 0.7725 || Learning rate: lr=2.5e-05.
===> Epoch[219](110/324): Loss: 0.6588 || Learning rate: lr=2.5e-05.
===> Epoch[219](120/324): Loss: 0.4313 || Learning rate: lr=2.5e-05.
===> Epoch[219](130/324): Loss: 0.8880 || Learning rate: lr=2.5e-05.
===> Epoch[219](140/324): Loss: 0.9921 || Learning rate: lr=2.5e-05.
===> Epoch[219](150/324): Loss: 0.7991 || Learning rate: lr=2.5e-05.
===> Epoch[219](160/324): Loss: 0.5896 || Learning rate: lr=2.5e-05.
===> Epoch[219](170/324): Loss: 0.5305 || Learning rate: lr=2.5e-05.
===> Epoch[219](180/324): Loss: 0.8081 || Learning rate: lr=2.5e-05.
===> Epoch[219](190/324): Loss: 0.5763 || Learning rate: lr=2.5e-05.
===> Epoch[219](200/324): Loss: 0.7616 || Learning rate: lr=2.5e-05.
===> Epoch[219](210/324): Loss: 0.5967 || Learning rate: lr=2.5e-05.
===> Epoch[219](220/324): Loss: 0.8060 || Learning rate: lr=2.5e-05.
===> Epoch[219](230/324): Loss: 0.6092 || Learning rate: lr=2.5e-05.
===> Epoch[219](240/324): Loss: 0.7198 || Learning rate: lr=2.5e-05.
===> Epoch[219](250/324): Loss: 0.9225 || Learning rate: lr=2.5e-05.
===> Epoch[219](260/324): Loss: 0.5113 || Learning rate: lr=2.5e-05.
===> Epoch[219](270/324): Loss: 0.8287 || Learning rate: lr=2.5e-05.
===> Epoch[219](280/324): Loss: 0.5882 || Learning rate: lr=2.5e-05.
===> Epoch[219](290/324): Loss: 0.6862 || Learning rate: lr=2.5e-05.
===> Epoch[219](300/324): Loss: 0.6214 || Learning rate: lr=2.5e-05.
===> Epoch[219](310/324): Loss: 1.0469 || Learning rate: lr=2.5e-05.
===> Epoch[219](320/324): Loss: 0.4965 || Learning rate: lr=2.5e-05.
===> Epoch[220](10/324): Loss: 0.8551 || Learning rate: lr=2.5e-05.
===> Epoch[220](20/324): Loss: 0.8071 || Learning rate: lr=2.5e-05.
===> Epoch[220](30/324): Loss: 0.5542 || Learning rate: lr=2.5e-05.
===> Epoch[220](40/324): Loss: 0.5289 || Learning rate: lr=2.5e-05.
===> Epoch[220](50/324): Loss: 0.8841 || Learning rate: lr=2.5e-05.
===> Epoch[220](60/324): Loss: 0.6884 || Learning rate: lr=2.5e-05.
===> Epoch[220](70/324): Loss: 0.7201 || Learning rate: lr=2.5e-05.
===> Epoch[220](80/324): Loss: 0.6543 || Learning rate: lr=2.5e-05.
===> Epoch[220](90/324): Loss: 0.4728 || Learning rate: lr=2.5e-05.
===> Epoch[220](100/324): Loss: 0.5327 || Learning rate: lr=2.5e-05.
===> Epoch[220](110/324): Loss: 0.6751 || Learning rate: lr=2.5e-05.
===> Epoch[220](120/324): Loss: 0.9632 || Learning rate: lr=2.5e-05.
===> Epoch[220](130/324): Loss: 0.8514 || Learning rate: lr=2.5e-05.
===> Epoch[220](140/324): Loss: 0.7387 || Learning rate: lr=2.5e-05.
===> Epoch[220](150/324): Loss: 0.7080 || Learning rate: lr=2.5e-05.
===> Epoch[220](160/324): Loss: 1.1508 || Learning rate: lr=2.5e-05.
===> Epoch[220](170/324): Loss: 0.7364 || Learning rate: lr=2.5e-05.
===> Epoch[220](180/324): Loss: 0.6910 || Learning rate: lr=2.5e-05.
===> Epoch[220](190/324): Loss: 0.5637 || Learning rate: lr=2.5e-05.
===> Epoch[220](200/324): Loss: 0.7952 || Learning rate: lr=2.5e-05.
===> Epoch[220](210/324): Loss: 0.5469 || Learning rate: lr=2.5e-05.
===> Epoch[220](220/324): Loss: 0.8273 || Learning rate: lr=2.5e-05.
===> Epoch[220](230/324): Loss: 0.5930 || Learning rate: lr=2.5e-05.
===> Epoch[220](240/324): Loss: 0.8546 || Learning rate: lr=2.5e-05.
===> Epoch[220](250/324): Loss: 0.8961 || Learning rate: lr=2.5e-05.
===> Epoch[220](260/324): Loss: 0.8018 || Learning rate: lr=2.5e-05.
===> Epoch[220](270/324): Loss: 0.6144 || Learning rate: lr=2.5e-05.
===> Epoch[220](280/324): Loss: 0.6823 || Learning rate: lr=2.5e-05.
===> Epoch[220](290/324): Loss: 0.5964 || Learning rate: lr=2.5e-05.
===> Epoch[220](300/324): Loss: 0.8054 || Learning rate: lr=2.5e-05.
===> Epoch[220](310/324): Loss: 0.8383 || Learning rate: lr=2.5e-05.
===> Epoch[220](320/324): Loss: 0.5692 || Learning rate: lr=2.5e-05.
Checkpoint saved to weights/epoch_v2_220.pth
===> Epoch[221](10/324): Loss: 0.8190 || Learning rate: lr=2.5e-05.
===> Epoch[221](20/324): Loss: 0.5465 || Learning rate: lr=2.5e-05.
===> Epoch[221](30/324): Loss: 0.7333 || Learning rate: lr=2.5e-05.
===> Epoch[221](40/324): Loss: 0.8101 || Learning rate: lr=2.5e-05.
===> Epoch[221](50/324): Loss: 0.7432 || Learning rate: lr=2.5e-05.
===> Epoch[221](60/324): Loss: 0.9824 || Learning rate: lr=2.5e-05.
===> Epoch[221](70/324): Loss: 0.8131 || Learning rate: lr=2.5e-05.
===> Epoch[221](80/324): Loss: 0.6522 || Learning rate: lr=2.5e-05.
===> Epoch[221](90/324): Loss: 0.9410 || Learning rate: lr=2.5e-05.
===> Epoch[221](100/324): Loss: 0.7785 || Learning rate: lr=2.5e-05.
===> Epoch[221](110/324): Loss: 0.4917 || Learning rate: lr=2.5e-05.
===> Epoch[221](120/324): Loss: 0.6347 || Learning rate: lr=2.5e-05.
===> Epoch[221](130/324): Loss: 0.7588 || Learning rate: lr=2.5e-05.
===> Epoch[221](140/324): Loss: 0.6339 || Learning rate: lr=2.5e-05.
===> Epoch[221](150/324): Loss: 0.6488 || Learning rate: lr=2.5e-05.
===> Epoch[221](160/324): Loss: 0.9559 || Learning rate: lr=2.5e-05.
===> Epoch[221](170/324): Loss: 0.7545 || Learning rate: lr=2.5e-05.
===> Epoch[221](180/324): Loss: 0.7757 || Learning rate: lr=2.5e-05.
===> Epoch[221](190/324): Loss: 0.5403 || Learning rate: lr=2.5e-05.
===> Epoch[221](200/324): Loss: 0.7394 || Learning rate: lr=2.5e-05.
===> Epoch[221](210/324): Loss: 0.6102 || Learning rate: lr=2.5e-05.
===> Epoch[221](220/324): Loss: 0.5028 || Learning rate: lr=2.5e-05.
===> Epoch[221](230/324): Loss: 0.6441 || Learning rate: lr=2.5e-05.
===> Epoch[221](240/324): Loss: 0.8914 || Learning rate: lr=2.5e-05.
===> Epoch[221](250/324): Loss: 0.6796 || Learning rate: lr=2.5e-05.
===> Epoch[221](260/324): Loss: 0.6031 || Learning rate: lr=2.5e-05.
===> Epoch[221](270/324): Loss: 0.6243 || Learning rate: lr=2.5e-05.
===> Epoch[221](280/324): Loss: 0.5602 || Learning rate: lr=2.5e-05.
===> Epoch[221](290/324): Loss: 1.1517 || Learning rate: lr=2.5e-05.
===> Epoch[221](300/324): Loss: 0.4196 || Learning rate: lr=2.5e-05.
===> Epoch[221](310/324): Loss: 0.7529 || Learning rate: lr=2.5e-05.
===> Epoch[221](320/324): Loss: 0.7709 || Learning rate: lr=2.5e-05.
===> Epoch[222](10/324): Loss: 0.6799 || Learning rate: lr=2.5e-05.
===> Epoch[222](20/324): Loss: 0.9425 || Learning rate: lr=2.5e-05.
===> Epoch[222](30/324): Loss: 0.6162 || Learning rate: lr=2.5e-05.
===> Epoch[222](40/324): Loss: 0.7493 || Learning rate: lr=2.5e-05.
===> Epoch[222](50/324): Loss: 0.8481 || Learning rate: lr=2.5e-05.
===> Epoch[222](60/324): Loss: 0.6088 || Learning rate: lr=2.5e-05.
===> Epoch[222](70/324): Loss: 0.7437 || Learning rate: lr=2.5e-05.
===> Epoch[222](80/324): Loss: 0.6819 || Learning rate: lr=2.5e-05.
===> Epoch[222](90/324): Loss: 0.6228 || Learning rate: lr=2.5e-05.
===> Epoch[222](100/324): Loss: 0.5466 || Learning rate: lr=2.5e-05.
===> Epoch[222](110/324): Loss: 0.6167 || Learning rate: lr=2.5e-05.
===> Epoch[222](120/324): Loss: 0.8384 || Learning rate: lr=2.5e-05.
===> Epoch[222](130/324): Loss: 0.7309 || Learning rate: lr=2.5e-05.
===> Epoch[222](140/324): Loss: 0.5476 || Learning rate: lr=2.5e-05.
===> Epoch[222](150/324): Loss: 0.7468 || Learning rate: lr=2.5e-05.
===> Epoch[222](160/324): Loss: 1.0384 || Learning rate: lr=2.5e-05.
===> Epoch[222](170/324): Loss: 0.5311 || Learning rate: lr=2.5e-05.
===> Epoch[222](180/324): Loss: 0.6079 || Learning rate: lr=2.5e-05.
===> Epoch[222](190/324): Loss: 0.5032 || Learning rate: lr=2.5e-05.
===> Epoch[222](200/324): Loss: 0.7063 || Learning rate: lr=2.5e-05.
===> Epoch[222](210/324): Loss: 0.7081 || Learning rate: lr=2.5e-05.
===> Epoch[222](220/324): Loss: 0.9794 || Learning rate: lr=2.5e-05.
===> Epoch[222](230/324): Loss: 0.7782 || Learning rate: lr=2.5e-05.
===> Epoch[222](240/324): Loss: 0.4579 || Learning rate: lr=2.5e-05.
===> Epoch[222](250/324): Loss: 0.6452 || Learning rate: lr=2.5e-05.
===> Epoch[222](260/324): Loss: 0.5675 || Learning rate: lr=2.5e-05.
===> Epoch[222](270/324): Loss: 0.5231 || Learning rate: lr=2.5e-05.
===> Epoch[222](280/324): Loss: 0.9094 || Learning rate: lr=2.5e-05.
===> Epoch[222](290/324): Loss: 0.8185 || Learning rate: lr=2.5e-05.
===> Epoch[222](300/324): Loss: 0.7967 || Learning rate: lr=2.5e-05.
===> Epoch[222](310/324): Loss: 0.7915 || Learning rate: lr=2.5e-05.
===> Epoch[222](320/324): Loss: 0.5547 || Learning rate: lr=2.5e-05.
===> Epoch[223](10/324): Loss: 0.5049 || Learning rate: lr=2.5e-05.
===> Epoch[223](20/324): Loss: 0.6608 || Learning rate: lr=2.5e-05.
===> Epoch[223](30/324): Loss: 0.9774 || Learning rate: lr=2.5e-05.
===> Epoch[223](40/324): Loss: 0.5056 || Learning rate: lr=2.5e-05.
===> Epoch[223](50/324): Loss: 0.5946 || Learning rate: lr=2.5e-05.
===> Epoch[223](60/324): Loss: 0.9147 || Learning rate: lr=2.5e-05.
===> Epoch[223](70/324): Loss: 0.4516 || Learning rate: lr=2.5e-05.
===> Epoch[223](80/324): Loss: 0.8468 || Learning rate: lr=2.5e-05.
===> Epoch[223](90/324): Loss: 0.7003 || Learning rate: lr=2.5e-05.
===> Epoch[223](100/324): Loss: 0.5556 || Learning rate: lr=2.5e-05.
===> Epoch[223](110/324): Loss: 0.6597 || Learning rate: lr=2.5e-05.
===> Epoch[223](120/324): Loss: 0.8292 || Learning rate: lr=2.5e-05.
===> Epoch[223](130/324): Loss: 0.7632 || Learning rate: lr=2.5e-05.
===> Epoch[223](140/324): Loss: 0.6050 || Learning rate: lr=2.5e-05.
===> Epoch[223](150/324): Loss: 0.5218 || Learning rate: lr=2.5e-05.
===> Epoch[223](160/324): Loss: 0.6446 || Learning rate: lr=2.5e-05.
===> Epoch[223](170/324): Loss: 1.0088 || Learning rate: lr=2.5e-05.
===> Epoch[223](180/324): Loss: 0.9319 || Learning rate: lr=2.5e-05.
===> Epoch[223](190/324): Loss: 0.5342 || Learning rate: lr=2.5e-05.
===> Epoch[223](200/324): Loss: 0.8198 || Learning rate: lr=2.5e-05.
===> Epoch[223](210/324): Loss: 0.7230 || Learning rate: lr=2.5e-05.
===> Epoch[223](220/324): Loss: 0.6855 || Learning rate: lr=2.5e-05.
===> Epoch[223](230/324): Loss: 0.4168 || Learning rate: lr=2.5e-05.
===> Epoch[223](240/324): Loss: 0.6106 || Learning rate: lr=2.5e-05.
===> Epoch[223](250/324): Loss: 0.6728 || Learning rate: lr=2.5e-05.
===> Epoch[223](260/324): Loss: 1.0016 || Learning rate: lr=2.5e-05.
===> Epoch[223](270/324): Loss: 0.9292 || Learning rate: lr=2.5e-05.
===> Epoch[223](280/324): Loss: 0.8107 || Learning rate: lr=2.5e-05.
===> Epoch[223](290/324): Loss: 0.8941 || Learning rate: lr=2.5e-05.
===> Epoch[223](300/324): Loss: 0.7398 || Learning rate: lr=2.5e-05.
===> Epoch[223](310/324): Loss: 0.4863 || Learning rate: lr=2.5e-05.
===> Epoch[223](320/324): Loss: 0.7978 || Learning rate: lr=2.5e-05.
===> Epoch[224](10/324): Loss: 0.6661 || Learning rate: lr=2.5e-05.
===> Epoch[224](20/324): Loss: 0.6736 || Learning rate: lr=2.5e-05.
===> Epoch[224](30/324): Loss: 0.7810 || Learning rate: lr=2.5e-05.
===> Epoch[224](40/324): Loss: 0.8287 || Learning rate: lr=2.5e-05.
===> Epoch[224](50/324): Loss: 0.8590 || Learning rate: lr=2.5e-05.
===> Epoch[224](60/324): Loss: 0.5545 || Learning rate: lr=2.5e-05.
===> Epoch[224](70/324): Loss: 0.6411 || Learning rate: lr=2.5e-05.
===> Epoch[224](80/324): Loss: 0.7006 || Learning rate: lr=2.5e-05.
===> Epoch[224](90/324): Loss: 0.9046 || Learning rate: lr=2.5e-05.
===> Epoch[224](100/324): Loss: 0.5018 || Learning rate: lr=2.5e-05.
===> Epoch[224](110/324): Loss: 0.4434 || Learning rate: lr=2.5e-05.
===> Epoch[224](120/324): Loss: 0.5926 || Learning rate: lr=2.5e-05.
===> Epoch[224](130/324): Loss: 0.7485 || Learning rate: lr=2.5e-05.
===> Epoch[224](140/324): Loss: 1.0418 || Learning rate: lr=2.5e-05.
===> Epoch[224](150/324): Loss: 0.7536 || Learning rate: lr=2.5e-05.
===> Epoch[224](160/324): Loss: 0.8724 || Learning rate: lr=2.5e-05.
===> Epoch[224](170/324): Loss: 0.8926 || Learning rate: lr=2.5e-05.
===> Epoch[224](180/324): Loss: 0.8817 || Learning rate: lr=2.5e-05.
===> Epoch[224](190/324): Loss: 0.5042 || Learning rate: lr=2.5e-05.
===> Epoch[224](200/324): Loss: 0.7600 || Learning rate: lr=2.5e-05.
===> Epoch[224](210/324): Loss: 0.6759 || Learning rate: lr=2.5e-05.
===> Epoch[224](220/324): Loss: 0.7738 || Learning rate: lr=2.5e-05.
===> Epoch[224](230/324): Loss: 0.8206 || Learning rate: lr=2.5e-05.
===> Epoch[224](240/324): Loss: 0.6557 || Learning rate: lr=2.5e-05.
===> Epoch[224](250/324): Loss: 0.5776 || Learning rate: lr=2.5e-05.
===> Epoch[224](260/324): Loss: 0.8723 || Learning rate: lr=2.5e-05.
===> Epoch[224](270/324): Loss: 0.5761 || Learning rate: lr=2.5e-05.
===> Epoch[224](280/324): Loss: 0.7559 || Learning rate: lr=2.5e-05.
===> Epoch[224](290/324): Loss: 0.5930 || Learning rate: lr=2.5e-05.
===> Epoch[224](300/324): Loss: 0.6002 || Learning rate: lr=2.5e-05.
===> Epoch[224](310/324): Loss: 0.6644 || Learning rate: lr=2.5e-05.
===> Epoch[224](320/324): Loss: 0.5460 || Learning rate: lr=2.5e-05.
===> Epoch[225](10/324): Loss: 0.4664 || Learning rate: lr=2.5e-05.
===> Epoch[225](20/324): Loss: 0.8046 || Learning rate: lr=2.5e-05.
===> Epoch[225](30/324): Loss: 0.7139 || Learning rate: lr=2.5e-05.
===> Epoch[225](40/324): Loss: 0.7027 || Learning rate: lr=2.5e-05.
===> Epoch[225](50/324): Loss: 0.6399 || Learning rate: lr=2.5e-05.
===> Epoch[225](60/324): Loss: 0.8396 || Learning rate: lr=2.5e-05.
===> Epoch[225](70/324): Loss: 0.6274 || Learning rate: lr=2.5e-05.
===> Epoch[225](80/324): Loss: 0.7102 || Learning rate: lr=2.5e-05.
===> Epoch[225](90/324): Loss: 0.8775 || Learning rate: lr=2.5e-05.
===> Epoch[225](100/324): Loss: 0.9352 || Learning rate: lr=2.5e-05.
===> Epoch[225](110/324): Loss: 0.6450 || Learning rate: lr=2.5e-05.
===> Epoch[225](120/324): Loss: 0.8722 || Learning rate: lr=2.5e-05.
===> Epoch[225](130/324): Loss: 0.7750 || Learning rate: lr=2.5e-05.
===> Epoch[225](140/324): Loss: 0.6585 || Learning rate: lr=2.5e-05.
===> Epoch[225](150/324): Loss: 0.5673 || Learning rate: lr=2.5e-05.
===> Epoch[225](160/324): Loss: 0.6876 || Learning rate: lr=2.5e-05.
===> Epoch[225](170/324): Loss: 0.7000 || Learning rate: lr=2.5e-05.
===> Epoch[225](180/324): Loss: 0.6340 || Learning rate: lr=2.5e-05.
===> Epoch[225](190/324): Loss: 0.6760 || Learning rate: lr=2.5e-05.
===> Epoch[225](200/324): Loss: 0.4454 || Learning rate: lr=2.5e-05.
===> Epoch[225](210/324): Loss: 0.6544 || Learning rate: lr=2.5e-05.
===> Epoch[225](220/324): Loss: 0.5097 || Learning rate: lr=2.5e-05.
===> Epoch[225](230/324): Loss: 0.5096 || Learning rate: lr=2.5e-05.
===> Epoch[225](240/324): Loss: 0.4478 || Learning rate: lr=2.5e-05.
===> Epoch[225](250/324): Loss: 0.5319 || Learning rate: lr=2.5e-05.
===> Epoch[225](260/324): Loss: 0.6726 || Learning rate: lr=2.5e-05.
===> Epoch[225](270/324): Loss: 0.6334 || Learning rate: lr=2.5e-05.
===> Epoch[225](280/324): Loss: 0.6552 || Learning rate: lr=2.5e-05.
===> Epoch[225](290/324): Loss: 0.4863 || Learning rate: lr=2.5e-05.
===> Epoch[225](300/324): Loss: 0.4848 || Learning rate: lr=2.5e-05.
===> Epoch[225](310/324): Loss: 0.7040 || Learning rate: lr=2.5e-05.
===> Epoch[225](320/324): Loss: 0.8159 || Learning rate: lr=2.5e-05.
===> Epoch[226](10/324): Loss: 0.5448 || Learning rate: lr=2.5e-05.
===> Epoch[226](20/324): Loss: 0.8289 || Learning rate: lr=2.5e-05.
===> Epoch[226](30/324): Loss: 0.8377 || Learning rate: lr=2.5e-05.
===> Epoch[226](40/324): Loss: 0.7887 || Learning rate: lr=2.5e-05.
===> Epoch[226](50/324): Loss: 1.1064 || Learning rate: lr=2.5e-05.
===> Epoch[226](60/324): Loss: 0.5948 || Learning rate: lr=2.5e-05.
===> Epoch[226](70/324): Loss: 0.6352 || Learning rate: lr=2.5e-05.
===> Epoch[226](80/324): Loss: 0.7063 || Learning rate: lr=2.5e-05.
===> Epoch[226](90/324): Loss: 0.5825 || Learning rate: lr=2.5e-05.
===> Epoch[226](100/324): Loss: 0.6105 || Learning rate: lr=2.5e-05.
===> Epoch[226](110/324): Loss: 0.6326 || Learning rate: lr=2.5e-05.
===> Epoch[226](120/324): Loss: 0.5272 || Learning rate: lr=2.5e-05.
===> Epoch[226](130/324): Loss: 0.7705 || Learning rate: lr=2.5e-05.
===> Epoch[226](140/324): Loss: 0.6550 || Learning rate: lr=2.5e-05.
===> Epoch[226](150/324): Loss: 0.4852 || Learning rate: lr=2.5e-05.
===> Epoch[226](160/324): Loss: 0.6577 || Learning rate: lr=2.5e-05.
===> Epoch[226](170/324): Loss: 0.6420 || Learning rate: lr=2.5e-05.
===> Epoch[226](180/324): Loss: 0.6596 || Learning rate: lr=2.5e-05.
===> Epoch[226](190/324): Loss: 0.8155 || Learning rate: lr=2.5e-05.
===> Epoch[226](200/324): Loss: 0.5730 || Learning rate: lr=2.5e-05.
===> Epoch[226](210/324): Loss: 0.5863 || Learning rate: lr=2.5e-05.
===> Epoch[226](220/324): Loss: 0.6567 || Learning rate: lr=2.5e-05.
===> Epoch[226](230/324): Loss: 0.6638 || Learning rate: lr=2.5e-05.
===> Epoch[226](240/324): Loss: 0.5624 || Learning rate: lr=2.5e-05.
===> Epoch[226](250/324): Loss: 0.7565 || Learning rate: lr=2.5e-05.
===> Epoch[226](260/324): Loss: 0.6106 || Learning rate: lr=2.5e-05.
===> Epoch[226](270/324): Loss: 0.5585 || Learning rate: lr=2.5e-05.
===> Epoch[226](280/324): Loss: 0.9214 || Learning rate: lr=2.5e-05.
===> Epoch[226](290/324): Loss: 0.8470 || Learning rate: lr=2.5e-05.
===> Epoch[226](300/324): Loss: 0.9106 || Learning rate: lr=2.5e-05.
===> Epoch[226](310/324): Loss: 0.5075 || Learning rate: lr=2.5e-05.
===> Epoch[226](320/324): Loss: 0.9824 || Learning rate: lr=2.5e-05.
===> Epoch[227](10/324): Loss: 0.7909 || Learning rate: lr=2.5e-05.
===> Epoch[227](20/324): Loss: 0.7652 || Learning rate: lr=2.5e-05.
===> Epoch[227](30/324): Loss: 0.5738 || Learning rate: lr=2.5e-05.
===> Epoch[227](40/324): Loss: 0.5881 || Learning rate: lr=2.5e-05.
===> Epoch[227](50/324): Loss: 0.5686 || Learning rate: lr=2.5e-05.
===> Epoch[227](60/324): Loss: 0.6549 || Learning rate: lr=2.5e-05.
===> Epoch[227](70/324): Loss: 0.5957 || Learning rate: lr=2.5e-05.
===> Epoch[227](80/324): Loss: 0.7222 || Learning rate: lr=2.5e-05.
===> Epoch[227](90/324): Loss: 0.6104 || Learning rate: lr=2.5e-05.
===> Epoch[227](100/324): Loss: 0.8763 || Learning rate: lr=2.5e-05.
===> Epoch[227](110/324): Loss: 0.5797 || Learning rate: lr=2.5e-05.
===> Epoch[227](120/324): Loss: 0.6428 || Learning rate: lr=2.5e-05.
===> Epoch[227](130/324): Loss: 0.5392 || Learning rate: lr=2.5e-05.
===> Epoch[227](140/324): Loss: 0.9051 || Learning rate: lr=2.5e-05.
===> Epoch[227](150/324): Loss: 0.7581 || Learning rate: lr=2.5e-05.
===> Epoch[227](160/324): Loss: 0.5015 || Learning rate: lr=2.5e-05.
===> Epoch[227](170/324): Loss: 0.5065 || Learning rate: lr=2.5e-05.
===> Epoch[227](180/324): Loss: 0.5506 || Learning rate: lr=2.5e-05.
===> Epoch[227](190/324): Loss: 0.5503 || Learning rate: lr=2.5e-05.
===> Epoch[227](200/324): Loss: 0.8154 || Learning rate: lr=2.5e-05.
===> Epoch[227](210/324): Loss: 0.9030 || Learning rate: lr=2.5e-05.
===> Epoch[227](220/324): Loss: 0.8432 || Learning rate: lr=2.5e-05.
===> Epoch[227](230/324): Loss: 0.7376 || Learning rate: lr=2.5e-05.
===> Epoch[227](240/324): Loss: 0.7162 || Learning rate: lr=2.5e-05.
===> Epoch[227](250/324): Loss: 0.5007 || Learning rate: lr=2.5e-05.
===> Epoch[227](260/324): Loss: 0.5213 || Learning rate: lr=2.5e-05.
===> Epoch[227](270/324): Loss: 0.6492 || Learning rate: lr=2.5e-05.
===> Epoch[227](280/324): Loss: 0.8127 || Learning rate: lr=2.5e-05.
===> Epoch[227](290/324): Loss: 0.6536 || Learning rate: lr=2.5e-05.
===> Epoch[227](300/324): Loss: 1.0124 || Learning rate: lr=2.5e-05.
===> Epoch[227](310/324): Loss: 0.7143 || Learning rate: lr=2.5e-05.
===> Epoch[227](320/324): Loss: 0.7178 || Learning rate: lr=2.5e-05.
===> Epoch[228](10/324): Loss: 0.5780 || Learning rate: lr=2.5e-05.
===> Epoch[228](20/324): Loss: 0.4412 || Learning rate: lr=2.5e-05.
===> Epoch[228](30/324): Loss: 0.6908 || Learning rate: lr=2.5e-05.
===> Epoch[228](40/324): Loss: 0.7257 || Learning rate: lr=2.5e-05.
===> Epoch[228](50/324): Loss: 1.0636 || Learning rate: lr=2.5e-05.
===> Epoch[228](60/324): Loss: 0.5632 || Learning rate: lr=2.5e-05.
===> Epoch[228](70/324): Loss: 0.6065 || Learning rate: lr=2.5e-05.
===> Epoch[228](80/324): Loss: 0.8681 || Learning rate: lr=2.5e-05.
===> Epoch[228](90/324): Loss: 0.5249 || Learning rate: lr=2.5e-05.
===> Epoch[228](100/324): Loss: 0.7101 || Learning rate: lr=2.5e-05.
===> Epoch[228](110/324): Loss: 0.9068 || Learning rate: lr=2.5e-05.
===> Epoch[228](120/324): Loss: 0.4587 || Learning rate: lr=2.5e-05.
===> Epoch[228](130/324): Loss: 0.5912 || Learning rate: lr=2.5e-05.
===> Epoch[228](140/324): Loss: 0.6504 || Learning rate: lr=2.5e-05.
===> Epoch[228](150/324): Loss: 0.5559 || Learning rate: lr=2.5e-05.
===> Epoch[228](160/324): Loss: 0.7044 || Learning rate: lr=2.5e-05.
===> Epoch[228](170/324): Loss: 0.6392 || Learning rate: lr=2.5e-05.
===> Epoch[228](180/324): Loss: 0.7856 || Learning rate: lr=2.5e-05.
===> Epoch[228](190/324): Loss: 0.6171 || Learning rate: lr=2.5e-05.
===> Epoch[228](200/324): Loss: 0.8789 || Learning rate: lr=2.5e-05.
===> Epoch[228](210/324): Loss: 0.5645 || Learning rate: lr=2.5e-05.
===> Epoch[228](220/324): Loss: 0.9223 || Learning rate: lr=2.5e-05.
===> Epoch[228](230/324): Loss: 0.6860 || Learning rate: lr=2.5e-05.
===> Epoch[228](240/324): Loss: 0.5649 || Learning rate: lr=2.5e-05.
===> Epoch[228](250/324): Loss: 0.7104 || Learning rate: lr=2.5e-05.
===> Epoch[228](260/324): Loss: 0.6772 || Learning rate: lr=2.5e-05.
===> Epoch[228](270/324): Loss: 0.6456 || Learning rate: lr=2.5e-05.
===> Epoch[228](280/324): Loss: 0.6305 || Learning rate: lr=2.5e-05.
===> Epoch[228](290/324): Loss: 0.7873 || Learning rate: lr=2.5e-05.
===> Epoch[228](300/324): Loss: 0.9914 || Learning rate: lr=2.5e-05.
===> Epoch[228](310/324): Loss: 0.4989 || Learning rate: lr=2.5e-05.
===> Epoch[228](320/324): Loss: 0.5750 || Learning rate: lr=2.5e-05.
===> Epoch[229](10/324): Loss: 0.5123 || Learning rate: lr=2.5e-05.
===> Epoch[229](20/324): Loss: 0.9286 || Learning rate: lr=2.5e-05.
===> Epoch[229](30/324): Loss: 0.7114 || Learning rate: lr=2.5e-05.
===> Epoch[229](40/324): Loss: 0.6281 || Learning rate: lr=2.5e-05.
===> Epoch[229](50/324): Loss: 0.7338 || Learning rate: lr=2.5e-05.
===> Epoch[229](60/324): Loss: 0.6233 || Learning rate: lr=2.5e-05.
===> Epoch[229](70/324): Loss: 0.9977 || Learning rate: lr=2.5e-05.
===> Epoch[229](80/324): Loss: 0.5962 || Learning rate: lr=2.5e-05.
===> Epoch[229](90/324): Loss: 0.6314 || Learning rate: lr=2.5e-05.
===> Epoch[229](100/324): Loss: 0.7274 || Learning rate: lr=2.5e-05.
===> Epoch[229](110/324): Loss: 0.5019 || Learning rate: lr=2.5e-05.
===> Epoch[229](120/324): Loss: 0.4375 || Learning rate: lr=2.5e-05.
===> Epoch[229](130/324): Loss: 0.6611 || Learning rate: lr=2.5e-05.
===> Epoch[229](140/324): Loss: 0.4664 || Learning rate: lr=2.5e-05.
===> Epoch[229](150/324): Loss: 0.5026 || Learning rate: lr=2.5e-05.
===> Epoch[229](160/324): Loss: 0.8236 || Learning rate: lr=2.5e-05.
===> Epoch[229](170/324): Loss: 0.7953 || Learning rate: lr=2.5e-05.
===> Epoch[229](180/324): Loss: 0.4786 || Learning rate: lr=2.5e-05.
===> Epoch[229](190/324): Loss: 0.6534 || Learning rate: lr=2.5e-05.
===> Epoch[229](200/324): Loss: 0.9325 || Learning rate: lr=2.5e-05.
===> Epoch[229](210/324): Loss: 0.3763 || Learning rate: lr=2.5e-05.
===> Epoch[229](220/324): Loss: 0.6483 || Learning rate: lr=2.5e-05.
===> Epoch[229](230/324): Loss: 0.5044 || Learning rate: lr=2.5e-05.
===> Epoch[229](240/324): Loss: 0.7115 || Learning rate: lr=2.5e-05.
===> Epoch[229](250/324): Loss: 0.7301 || Learning rate: lr=2.5e-05.
===> Epoch[229](260/324): Loss: 0.8820 || Learning rate: lr=2.5e-05.
===> Epoch[229](270/324): Loss: 0.6972 || Learning rate: lr=2.5e-05.
===> Epoch[229](280/324): Loss: 0.9654 || Learning rate: lr=2.5e-05.
===> Epoch[229](290/324): Loss: 0.6956 || Learning rate: lr=2.5e-05.
===> Epoch[229](300/324): Loss: 0.6934 || Learning rate: lr=2.5e-05.
===> Epoch[229](310/324): Loss: 0.4762 || Learning rate: lr=2.5e-05.
===> Epoch[229](320/324): Loss: 0.8020 || Learning rate: lr=2.5e-05.
===> Epoch[230](10/324): Loss: 0.7158 || Learning rate: lr=2.5e-05.
===> Epoch[230](20/324): Loss: 0.6234 || Learning rate: lr=2.5e-05.
===> Epoch[230](30/324): Loss: 0.7190 || Learning rate: lr=2.5e-05.
===> Epoch[230](40/324): Loss: 0.5967 || Learning rate: lr=2.5e-05.
===> Epoch[230](50/324): Loss: 0.5077 || Learning rate: lr=2.5e-05.
===> Epoch[230](60/324): Loss: 0.5683 || Learning rate: lr=2.5e-05.
===> Epoch[230](70/324): Loss: 0.5858 || Learning rate: lr=2.5e-05.
===> Epoch[230](80/324): Loss: 0.7331 || Learning rate: lr=2.5e-05.
===> Epoch[230](90/324): Loss: 0.5304 || Learning rate: lr=2.5e-05.
===> Epoch[230](100/324): Loss: 0.8368 || Learning rate: lr=2.5e-05.
===> Epoch[230](110/324): Loss: 0.7040 || Learning rate: lr=2.5e-05.
===> Epoch[230](120/324): Loss: 0.8022 || Learning rate: lr=2.5e-05.
===> Epoch[230](130/324): Loss: 0.7791 || Learning rate: lr=2.5e-05.
===> Epoch[230](140/324): Loss: 0.8746 || Learning rate: lr=2.5e-05.
===> Epoch[230](150/324): Loss: 0.6613 || Learning rate: lr=2.5e-05.
===> Epoch[230](160/324): Loss: 0.5289 || Learning rate: lr=2.5e-05.
===> Epoch[230](170/324): Loss: 0.7833 || Learning rate: lr=2.5e-05.
===> Epoch[230](180/324): Loss: 0.6562 || Learning rate: lr=2.5e-05.
===> Epoch[230](190/324): Loss: 0.8072 || Learning rate: lr=2.5e-05.
===> Epoch[230](200/324): Loss: 0.6711 || Learning rate: lr=2.5e-05.
===> Epoch[230](210/324): Loss: 0.9358 || Learning rate: lr=2.5e-05.
===> Epoch[230](220/324): Loss: 0.5059 || Learning rate: lr=2.5e-05.
===> Epoch[230](230/324): Loss: 0.7336 || Learning rate: lr=2.5e-05.
===> Epoch[230](240/324): Loss: 0.5044 || Learning rate: lr=2.5e-05.
===> Epoch[230](250/324): Loss: 0.5930 || Learning rate: lr=2.5e-05.
===> Epoch[230](260/324): Loss: 0.6127 || Learning rate: lr=2.5e-05.
===> Epoch[230](270/324): Loss: 0.6402 || Learning rate: lr=2.5e-05.
===> Epoch[230](280/324): Loss: 0.6141 || Learning rate: lr=2.5e-05.
===> Epoch[230](290/324): Loss: 0.6385 || Learning rate: lr=2.5e-05.
===> Epoch[230](300/324): Loss: 0.5435 || Learning rate: lr=2.5e-05.
===> Epoch[230](310/324): Loss: 0.8592 || Learning rate: lr=2.5e-05.
===> Epoch[230](320/324): Loss: 0.8316 || Learning rate: lr=2.5e-05.
===> Epoch[231](10/324): Loss: 0.7322 || Learning rate: lr=2.5e-05.
===> Epoch[231](20/324): Loss: 0.3922 || Learning rate: lr=2.5e-05.
===> Epoch[231](30/324): Loss: 0.6900 || Learning rate: lr=2.5e-05.
===> Epoch[231](40/324): Loss: 0.7955 || Learning rate: lr=2.5e-05.
===> Epoch[231](50/324): Loss: 0.6414 || Learning rate: lr=2.5e-05.
===> Epoch[231](60/324): Loss: 0.6238 || Learning rate: lr=2.5e-05.
===> Epoch[231](70/324): Loss: 0.7198 || Learning rate: lr=2.5e-05.
===> Epoch[231](80/324): Loss: 0.7562 || Learning rate: lr=2.5e-05.
===> Epoch[231](90/324): Loss: 0.6317 || Learning rate: lr=2.5e-05.
===> Epoch[231](100/324): Loss: 0.7010 || Learning rate: lr=2.5e-05.
===> Epoch[231](110/324): Loss: 0.7024 || Learning rate: lr=2.5e-05.
===> Epoch[231](120/324): Loss: 0.7150 || Learning rate: lr=2.5e-05.
===> Epoch[231](130/324): Loss: 0.5657 || Learning rate: lr=2.5e-05.
===> Epoch[231](140/324): Loss: 0.6059 || Learning rate: lr=2.5e-05.
===> Epoch[231](150/324): Loss: 0.6968 || Learning rate: lr=2.5e-05.
===> Epoch[231](160/324): Loss: 0.8284 || Learning rate: lr=2.5e-05.
===> Epoch[231](170/324): Loss: 0.8116 || Learning rate: lr=2.5e-05.
===> Epoch[231](180/324): Loss: 0.3849 || Learning rate: lr=2.5e-05.
===> Epoch[231](190/324): Loss: 0.4589 || Learning rate: lr=2.5e-05.
===> Epoch[231](200/324): Loss: 0.6331 || Learning rate: lr=2.5e-05.
===> Epoch[231](210/324): Loss: 0.6473 || Learning rate: lr=2.5e-05.
===> Epoch[231](220/324): Loss: 0.7806 || Learning rate: lr=2.5e-05.
===> Epoch[231](230/324): Loss: 0.6770 || Learning rate: lr=2.5e-05.
===> Epoch[231](240/324): Loss: 0.5644 || Learning rate: lr=2.5e-05.
===> Epoch[231](250/324): Loss: 1.0721 || Learning rate: lr=2.5e-05.
===> Epoch[231](260/324): Loss: 0.5864 || Learning rate: lr=2.5e-05.
===> Epoch[231](270/324): Loss: 0.5444 || Learning rate: lr=2.5e-05.
===> Epoch[231](280/324): Loss: 0.8423 || Learning rate: lr=2.5e-05.
===> Epoch[231](290/324): Loss: 0.7050 || Learning rate: lr=2.5e-05.
===> Epoch[231](300/324): Loss: 0.4907 || Learning rate: lr=2.5e-05.
===> Epoch[231](310/324): Loss: 0.7943 || Learning rate: lr=2.5e-05.
===> Epoch[231](320/324): Loss: 0.8746 || Learning rate: lr=2.5e-05.
===> Epoch[232](10/324): Loss: 0.7350 || Learning rate: lr=2.5e-05.
===> Epoch[232](20/324): Loss: 0.8255 || Learning rate: lr=2.5e-05.
===> Epoch[232](30/324): Loss: 0.5638 || Learning rate: lr=2.5e-05.
===> Epoch[232](40/324): Loss: 0.8643 || Learning rate: lr=2.5e-05.
===> Epoch[232](50/324): Loss: 0.7109 || Learning rate: lr=2.5e-05.
===> Epoch[232](60/324): Loss: 0.7085 || Learning rate: lr=2.5e-05.
===> Epoch[232](70/324): Loss: 0.5913 || Learning rate: lr=2.5e-05.
===> Epoch[232](80/324): Loss: 0.7523 || Learning rate: lr=2.5e-05.
===> Epoch[232](90/324): Loss: 0.6662 || Learning rate: lr=2.5e-05.
===> Epoch[232](100/324): Loss: 0.7097 || Learning rate: lr=2.5e-05.
===> Epoch[232](110/324): Loss: 0.6958 || Learning rate: lr=2.5e-05.
===> Epoch[232](120/324): Loss: 0.7519 || Learning rate: lr=2.5e-05.
===> Epoch[232](130/324): Loss: 0.7963 || Learning rate: lr=2.5e-05.
===> Epoch[232](140/324): Loss: 0.5128 || Learning rate: lr=2.5e-05.
===> Epoch[232](150/324): Loss: 0.7088 || Learning rate: lr=2.5e-05.
===> Epoch[232](160/324): Loss: 0.5384 || Learning rate: lr=2.5e-05.
===> Epoch[232](170/324): Loss: 0.8052 || Learning rate: lr=2.5e-05.
===> Epoch[232](180/324): Loss: 0.6975 || Learning rate: lr=2.5e-05.
===> Epoch[232](190/324): Loss: 1.0379 || Learning rate: lr=2.5e-05.
===> Epoch[232](200/324): Loss: 0.8867 || Learning rate: lr=2.5e-05.
===> Epoch[232](210/324): Loss: 0.6775 || Learning rate: lr=2.5e-05.
===> Epoch[232](220/324): Loss: 0.4612 || Learning rate: lr=2.5e-05.
===> Epoch[232](230/324): Loss: 0.6595 || Learning rate: lr=2.5e-05.
===> Epoch[232](240/324): Loss: 0.6317 || Learning rate: lr=2.5e-05.
===> Epoch[232](250/324): Loss: 0.7911 || Learning rate: lr=2.5e-05.
===> Epoch[232](260/324): Loss: 0.5815 || Learning rate: lr=2.5e-05.
===> Epoch[232](270/324): Loss: 0.8645 || Learning rate: lr=2.5e-05.
===> Epoch[232](280/324): Loss: 0.7357 || Learning rate: lr=2.5e-05.
===> Epoch[232](290/324): Loss: 0.6080 || Learning rate: lr=2.5e-05.
===> Epoch[232](300/324): Loss: 0.5874 || Learning rate: lr=2.5e-05.
===> Epoch[232](310/324): Loss: 0.6277 || Learning rate: lr=2.5e-05.
===> Epoch[232](320/324): Loss: 0.7799 || Learning rate: lr=2.5e-05.
===> Epoch[233](10/324): Loss: 0.6432 || Learning rate: lr=2.5e-05.
===> Epoch[233](20/324): Loss: 0.8627 || Learning rate: lr=2.5e-05.
===> Epoch[233](30/324): Loss: 0.8040 || Learning rate: lr=2.5e-05.
===> Epoch[233](40/324): Loss: 1.0052 || Learning rate: lr=2.5e-05.
===> Epoch[233](50/324): Loss: 0.6803 || Learning rate: lr=2.5e-05.
===> Epoch[233](60/324): Loss: 0.9361 || Learning rate: lr=2.5e-05.
===> Epoch[233](70/324): Loss: 0.4956 || Learning rate: lr=2.5e-05.
===> Epoch[233](80/324): Loss: 0.6414 || Learning rate: lr=2.5e-05.
===> Epoch[233](90/324): Loss: 0.6932 || Learning rate: lr=2.5e-05.
===> Epoch[233](100/324): Loss: 0.6555 || Learning rate: lr=2.5e-05.
===> Epoch[233](110/324): Loss: 0.8114 || Learning rate: lr=2.5e-05.
===> Epoch[233](120/324): Loss: 1.0606 || Learning rate: lr=2.5e-05.
===> Epoch[233](130/324): Loss: 0.6292 || Learning rate: lr=2.5e-05.
===> Epoch[233](140/324): Loss: 0.5334 || Learning rate: lr=2.5e-05.
===> Epoch[233](150/324): Loss: 0.8316 || Learning rate: lr=2.5e-05.
===> Epoch[233](160/324): Loss: 0.6286 || Learning rate: lr=2.5e-05.
===> Epoch[233](170/324): Loss: 0.7170 || Learning rate: lr=2.5e-05.
===> Epoch[233](180/324): Loss: 0.4807 || Learning rate: lr=2.5e-05.
===> Epoch[233](190/324): Loss: 0.6774 || Learning rate: lr=2.5e-05.
===> Epoch[233](200/324): Loss: 0.6537 || Learning rate: lr=2.5e-05.
===> Epoch[233](210/324): Loss: 0.9437 || Learning rate: lr=2.5e-05.
===> Epoch[233](220/324): Loss: 0.8722 || Learning rate: lr=2.5e-05.
===> Epoch[233](230/324): Loss: 0.7834 || Learning rate: lr=2.5e-05.
===> Epoch[233](240/324): Loss: 0.6545 || Learning rate: lr=2.5e-05.
===> Epoch[233](250/324): Loss: 0.7107 || Learning rate: lr=2.5e-05.
===> Epoch[233](260/324): Loss: 0.7490 || Learning rate: lr=2.5e-05.
===> Epoch[233](270/324): Loss: 0.5485 || Learning rate: lr=2.5e-05.
===> Epoch[233](280/324): Loss: 0.7672 || Learning rate: lr=2.5e-05.
===> Epoch[233](290/324): Loss: 0.8362 || Learning rate: lr=2.5e-05.
===> Epoch[233](300/324): Loss: 0.4300 || Learning rate: lr=2.5e-05.
===> Epoch[233](310/324): Loss: 0.7647 || Learning rate: lr=2.5e-05.
===> Epoch[233](320/324): Loss: 0.4620 || Learning rate: lr=2.5e-05.
===> Epoch[234](10/324): Loss: 0.8610 || Learning rate: lr=2.5e-05.
===> Epoch[234](20/324): Loss: 0.6586 || Learning rate: lr=2.5e-05.
===> Epoch[234](30/324): Loss: 0.8842 || Learning rate: lr=2.5e-05.
===> Epoch[234](40/324): Loss: 0.5210 || Learning rate: lr=2.5e-05.
===> Epoch[234](50/324): Loss: 0.8971 || Learning rate: lr=2.5e-05.
===> Epoch[234](60/324): Loss: 0.8195 || Learning rate: lr=2.5e-05.
===> Epoch[234](70/324): Loss: 0.8687 || Learning rate: lr=2.5e-05.
===> Epoch[234](80/324): Loss: 0.3240 || Learning rate: lr=2.5e-05.
===> Epoch[234](90/324): Loss: 0.3429 || Learning rate: lr=2.5e-05.
===> Epoch[234](100/324): Loss: 0.6329 || Learning rate: lr=2.5e-05.
===> Epoch[234](110/324): Loss: 0.5253 || Learning rate: lr=2.5e-05.
===> Epoch[234](120/324): Loss: 0.6897 || Learning rate: lr=2.5e-05.
===> Epoch[234](130/324): Loss: 0.9259 || Learning rate: lr=2.5e-05.
===> Epoch[234](140/324): Loss: 0.6717 || Learning rate: lr=2.5e-05.
===> Epoch[234](150/324): Loss: 0.6895 || Learning rate: lr=2.5e-05.
===> Epoch[234](160/324): Loss: 0.7996 || Learning rate: lr=2.5e-05.
===> Epoch[234](170/324): Loss: 0.6001 || Learning rate: lr=2.5e-05.
===> Epoch[234](180/324): Loss: 0.7692 || Learning rate: lr=2.5e-05.
===> Epoch[234](190/324): Loss: 0.5955 || Learning rate: lr=2.5e-05.
===> Epoch[234](200/324): Loss: 0.5844 || Learning rate: lr=2.5e-05.
===> Epoch[234](210/324): Loss: 0.7602 || Learning rate: lr=2.5e-05.
===> Epoch[234](220/324): Loss: 0.5998 || Learning rate: lr=2.5e-05.
===> Epoch[234](230/324): Loss: 0.7568 || Learning rate: lr=2.5e-05.
===> Epoch[234](240/324): Loss: 0.6769 || Learning rate: lr=2.5e-05.
===> Epoch[234](250/324): Loss: 0.5944 || Learning rate: lr=2.5e-05.
===> Epoch[234](260/324): Loss: 0.7793 || Learning rate: lr=2.5e-05.
===> Epoch[234](270/324): Loss: 0.6176 || Learning rate: lr=2.5e-05.
===> Epoch[234](280/324): Loss: 0.6386 || Learning rate: lr=2.5e-05.
===> Epoch[234](290/324): Loss: 0.8846 || Learning rate: lr=2.5e-05.
===> Epoch[234](300/324): Loss: 0.5508 || Learning rate: lr=2.5e-05.
===> Epoch[234](310/324): Loss: 0.7968 || Learning rate: lr=2.5e-05.
===> Epoch[234](320/324): Loss: 0.5501 || Learning rate: lr=2.5e-05.
===> Epoch[235](10/324): Loss: 0.6073 || Learning rate: lr=2.5e-05.
===> Epoch[235](20/324): Loss: 0.4254 || Learning rate: lr=2.5e-05.
===> Epoch[235](30/324): Loss: 0.7476 || Learning rate: lr=2.5e-05.
===> Epoch[235](40/324): Loss: 0.9682 || Learning rate: lr=2.5e-05.
===> Epoch[235](50/324): Loss: 0.7923 || Learning rate: lr=2.5e-05.
===> Epoch[235](60/324): Loss: 0.7925 || Learning rate: lr=2.5e-05.
===> Epoch[235](70/324): Loss: 0.7061 || Learning rate: lr=2.5e-05.
===> Epoch[235](80/324): Loss: 0.6218 || Learning rate: lr=2.5e-05.
===> Epoch[235](90/324): Loss: 0.6396 || Learning rate: lr=2.5e-05.
===> Epoch[235](100/324): Loss: 1.0698 || Learning rate: lr=2.5e-05.
===> Epoch[235](110/324): Loss: 0.5246 || Learning rate: lr=2.5e-05.
===> Epoch[235](120/324): Loss: 0.6787 || Learning rate: lr=2.5e-05.
===> Epoch[235](130/324): Loss: 0.5253 || Learning rate: lr=2.5e-05.
===> Epoch[235](140/324): Loss: 0.5079 || Learning rate: lr=2.5e-05.
===> Epoch[235](150/324): Loss: 0.7893 || Learning rate: lr=2.5e-05.
===> Epoch[235](160/324): Loss: 0.6565 || Learning rate: lr=2.5e-05.
===> Epoch[235](170/324): Loss: 0.8115 || Learning rate: lr=2.5e-05.
===> Epoch[235](180/324): Loss: 0.8275 || Learning rate: lr=2.5e-05.
===> Epoch[235](190/324): Loss: 0.7460 || Learning rate: lr=2.5e-05.
===> Epoch[235](200/324): Loss: 1.0982 || Learning rate: lr=2.5e-05.
===> Epoch[235](210/324): Loss: 0.6768 || Learning rate: lr=2.5e-05.
===> Epoch[235](220/324): Loss: 0.7291 || Learning rate: lr=2.5e-05.
===> Epoch[235](230/324): Loss: 0.8201 || Learning rate: lr=2.5e-05.
===> Epoch[235](240/324): Loss: 0.7263 || Learning rate: lr=2.5e-05.
===> Epoch[235](250/324): Loss: 0.7021 || Learning rate: lr=2.5e-05.
===> Epoch[235](260/324): Loss: 0.5511 || Learning rate: lr=2.5e-05.
===> Epoch[235](270/324): Loss: 0.7394 || Learning rate: lr=2.5e-05.
===> Epoch[235](280/324): Loss: 0.6854 || Learning rate: lr=2.5e-05.
===> Epoch[235](290/324): Loss: 0.5424 || Learning rate: lr=2.5e-05.
===> Epoch[235](300/324): Loss: 0.5948 || Learning rate: lr=2.5e-05.
===> Epoch[235](310/324): Loss: 0.9507 || Learning rate: lr=2.5e-05.
===> Epoch[235](320/324): Loss: 0.5968 || Learning rate: lr=2.5e-05.
===> Epoch[236](10/324): Loss: 0.5301 || Learning rate: lr=2.5e-05.
===> Epoch[236](20/324): Loss: 1.1254 || Learning rate: lr=2.5e-05.
===> Epoch[236](30/324): Loss: 0.5276 || Learning rate: lr=2.5e-05.
===> Epoch[236](40/324): Loss: 0.7592 || Learning rate: lr=2.5e-05.
===> Epoch[236](50/324): Loss: 0.4739 || Learning rate: lr=2.5e-05.
===> Epoch[236](60/324): Loss: 0.7800 || Learning rate: lr=2.5e-05.
===> Epoch[236](70/324): Loss: 0.8636 || Learning rate: lr=2.5e-05.
===> Epoch[236](80/324): Loss: 0.3543 || Learning rate: lr=2.5e-05.
===> Epoch[236](90/324): Loss: 0.6081 || Learning rate: lr=2.5e-05.
===> Epoch[236](100/324): Loss: 0.7308 || Learning rate: lr=2.5e-05.
===> Epoch[236](110/324): Loss: 0.4912 || Learning rate: lr=2.5e-05.
===> Epoch[236](120/324): Loss: 0.9093 || Learning rate: lr=2.5e-05.
===> Epoch[236](130/324): Loss: 0.8505 || Learning rate: lr=2.5e-05.
===> Epoch[236](140/324): Loss: 0.7279 || Learning rate: lr=2.5e-05.
===> Epoch[236](150/324): Loss: 0.4621 || Learning rate: lr=2.5e-05.
===> Epoch[236](160/324): Loss: 0.5956 || Learning rate: lr=2.5e-05.
===> Epoch[236](170/324): Loss: 0.6472 || Learning rate: lr=2.5e-05.
===> Epoch[236](180/324): Loss: 0.7094 || Learning rate: lr=2.5e-05.
===> Epoch[236](190/324): Loss: 0.6494 || Learning rate: lr=2.5e-05.
===> Epoch[236](200/324): Loss: 0.7916 || Learning rate: lr=2.5e-05.
===> Epoch[236](210/324): Loss: 1.0430 || Learning rate: lr=2.5e-05.
===> Epoch[236](220/324): Loss: 0.8797 || Learning rate: lr=2.5e-05.
===> Epoch[236](230/324): Loss: 0.7447 || Learning rate: lr=2.5e-05.
===> Epoch[236](240/324): Loss: 0.5125 || Learning rate: lr=2.5e-05.
===> Epoch[236](250/324): Loss: 0.6995 || Learning rate: lr=2.5e-05.
===> Epoch[236](260/324): Loss: 0.4840 || Learning rate: lr=2.5e-05.
===> Epoch[236](270/324): Loss: 0.5240 || Learning rate: lr=2.5e-05.
===> Epoch[236](280/324): Loss: 0.6538 || Learning rate: lr=2.5e-05.
===> Epoch[236](290/324): Loss: 0.5847 || Learning rate: lr=2.5e-05.
===> Epoch[236](300/324): Loss: 0.6818 || Learning rate: lr=2.5e-05.
===> Epoch[236](310/324): Loss: 0.6890 || Learning rate: lr=2.5e-05.
===> Epoch[236](320/324): Loss: 0.7003 || Learning rate: lr=2.5e-05.
===> Epoch[237](10/324): Loss: 1.0424 || Learning rate: lr=2.5e-05.
===> Epoch[237](20/324): Loss: 0.9268 || Learning rate: lr=2.5e-05.
===> Epoch[237](30/324): Loss: 0.7381 || Learning rate: lr=2.5e-05.
===> Epoch[237](40/324): Loss: 0.5456 || Learning rate: lr=2.5e-05.
===> Epoch[237](50/324): Loss: 0.7145 || Learning rate: lr=2.5e-05.
===> Epoch[237](60/324): Loss: 0.7336 || Learning rate: lr=2.5e-05.
===> Epoch[237](70/324): Loss: 0.6766 || Learning rate: lr=2.5e-05.
===> Epoch[237](80/324): Loss: 0.5498 || Learning rate: lr=2.5e-05.
===> Epoch[237](90/324): Loss: 0.6048 || Learning rate: lr=2.5e-05.
===> Epoch[237](100/324): Loss: 0.7352 || Learning rate: lr=2.5e-05.
===> Epoch[237](110/324): Loss: 0.5790 || Learning rate: lr=2.5e-05.
===> Epoch[237](120/324): Loss: 0.8621 || Learning rate: lr=2.5e-05.
===> Epoch[237](130/324): Loss: 0.6720 || Learning rate: lr=2.5e-05.
===> Epoch[237](140/324): Loss: 0.5454 || Learning rate: lr=2.5e-05.
===> Epoch[237](150/324): Loss: 0.5878 || Learning rate: lr=2.5e-05.
===> Epoch[237](160/324): Loss: 0.7341 || Learning rate: lr=2.5e-05.
===> Epoch[237](170/324): Loss: 0.8733 || Learning rate: lr=2.5e-05.
===> Epoch[237](180/324): Loss: 0.4890 || Learning rate: lr=2.5e-05.
===> Epoch[237](190/324): Loss: 0.6707 || Learning rate: lr=2.5e-05.
===> Epoch[237](200/324): Loss: 0.8050 || Learning rate: lr=2.5e-05.
===> Epoch[237](210/324): Loss: 0.4559 || Learning rate: lr=2.5e-05.
===> Epoch[237](220/324): Loss: 0.6488 || Learning rate: lr=2.5e-05.
===> Epoch[237](230/324): Loss: 0.7308 || Learning rate: lr=2.5e-05.
===> Epoch[237](240/324): Loss: 0.8832 || Learning rate: lr=2.5e-05.
===> Epoch[237](250/324): Loss: 0.4321 || Learning rate: lr=2.5e-05.
===> Epoch[237](260/324): Loss: 0.9203 || Learning rate: lr=2.5e-05.
===> Epoch[237](270/324): Loss: 0.9955 || Learning rate: lr=2.5e-05.
===> Epoch[237](280/324): Loss: 0.8844 || Learning rate: lr=2.5e-05.
===> Epoch[237](290/324): Loss: 0.8184 || Learning rate: lr=2.5e-05.
===> Epoch[237](300/324): Loss: 0.9038 || Learning rate: lr=2.5e-05.
===> Epoch[237](310/324): Loss: 0.8451 || Learning rate: lr=2.5e-05.
===> Epoch[237](320/324): Loss: 0.6918 || Learning rate: lr=2.5e-05.
===> Epoch[238](10/324): Loss: 0.4460 || Learning rate: lr=2.5e-05.
===> Epoch[238](20/324): Loss: 0.7478 || Learning rate: lr=2.5e-05.
===> Epoch[238](30/324): Loss: 0.6341 || Learning rate: lr=2.5e-05.
===> Epoch[238](40/324): Loss: 0.6209 || Learning rate: lr=2.5e-05.
===> Epoch[238](50/324): Loss: 0.7764 || Learning rate: lr=2.5e-05.
===> Epoch[238](60/324): Loss: 0.5818 || Learning rate: lr=2.5e-05.
===> Epoch[238](70/324): Loss: 0.4699 || Learning rate: lr=2.5e-05.
===> Epoch[238](80/324): Loss: 0.6740 || Learning rate: lr=2.5e-05.
===> Epoch[238](90/324): Loss: 0.7517 || Learning rate: lr=2.5e-05.
===> Epoch[238](100/324): Loss: 0.8648 || Learning rate: lr=2.5e-05.
===> Epoch[238](110/324): Loss: 0.5334 || Learning rate: lr=2.5e-05.
===> Epoch[238](120/324): Loss: 0.6473 || Learning rate: lr=2.5e-05.
===> Epoch[238](130/324): Loss: 0.6045 || Learning rate: lr=2.5e-05.
===> Epoch[238](140/324): Loss: 0.4462 || Learning rate: lr=2.5e-05.
===> Epoch[238](150/324): Loss: 0.6183 || Learning rate: lr=2.5e-05.
===> Epoch[238](160/324): Loss: 0.5851 || Learning rate: lr=2.5e-05.
===> Epoch[238](170/324): Loss: 0.7018 || Learning rate: lr=2.5e-05.
===> Epoch[238](180/324): Loss: 0.8065 || Learning rate: lr=2.5e-05.
===> Epoch[238](190/324): Loss: 0.8292 || Learning rate: lr=2.5e-05.
===> Epoch[238](200/324): Loss: 0.5704 || Learning rate: lr=2.5e-05.
===> Epoch[238](210/324): Loss: 0.8369 || Learning rate: lr=2.5e-05.
===> Epoch[238](220/324): Loss: 0.8645 || Learning rate: lr=2.5e-05.
===> Epoch[238](230/324): Loss: 0.6303 || Learning rate: lr=2.5e-05.
===> Epoch[238](240/324): Loss: 0.5609 || Learning rate: lr=2.5e-05.
===> Epoch[238](250/324): Loss: 0.6790 || Learning rate: lr=2.5e-05.
===> Epoch[238](260/324): Loss: 0.6192 || Learning rate: lr=2.5e-05.
===> Epoch[238](270/324): Loss: 0.6535 || Learning rate: lr=2.5e-05.
===> Epoch[238](280/324): Loss: 0.9260 || Learning rate: lr=2.5e-05.
===> Epoch[238](290/324): Loss: 0.6413 || Learning rate: lr=2.5e-05.
===> Epoch[238](300/324): Loss: 0.7526 || Learning rate: lr=2.5e-05.
===> Epoch[238](310/324): Loss: 0.4423 || Learning rate: lr=2.5e-05.
===> Epoch[238](320/324): Loss: 0.6164 || Learning rate: lr=2.5e-05.
===> Epoch[239](10/324): Loss: 0.9528 || Learning rate: lr=2.5e-05.
===> Epoch[239](20/324): Loss: 0.4520 || Learning rate: lr=2.5e-05.
===> Epoch[239](30/324): Loss: 0.4756 || Learning rate: lr=2.5e-05.
===> Epoch[239](40/324): Loss: 0.5545 || Learning rate: lr=2.5e-05.
===> Epoch[239](50/324): Loss: 0.7790 || Learning rate: lr=2.5e-05.
===> Epoch[239](60/324): Loss: 0.5510 || Learning rate: lr=2.5e-05.
===> Epoch[239](70/324): Loss: 0.6044 || Learning rate: lr=2.5e-05.
===> Epoch[239](80/324): Loss: 0.7280 || Learning rate: lr=2.5e-05.
===> Epoch[239](90/324): Loss: 0.7786 || Learning rate: lr=2.5e-05.
===> Epoch[239](100/324): Loss: 0.8772 || Learning rate: lr=2.5e-05.
===> Epoch[239](110/324): Loss: 1.0917 || Learning rate: lr=2.5e-05.
===> Epoch[239](120/324): Loss: 0.4774 || Learning rate: lr=2.5e-05.
===> Epoch[239](130/324): Loss: 0.7293 || Learning rate: lr=2.5e-05.
===> Epoch[239](140/324): Loss: 0.4887 || Learning rate: lr=2.5e-05.
===> Epoch[239](150/324): Loss: 0.4242 || Learning rate: lr=2.5e-05.
===> Epoch[239](160/324): Loss: 0.7289 || Learning rate: lr=2.5e-05.
===> Epoch[239](170/324): Loss: 0.7451 || Learning rate: lr=2.5e-05.
===> Epoch[239](180/324): Loss: 0.6805 || Learning rate: lr=2.5e-05.
===> Epoch[239](190/324): Loss: 0.6292 || Learning rate: lr=2.5e-05.
===> Epoch[239](200/324): Loss: 0.7581 || Learning rate: lr=2.5e-05.
===> Epoch[239](210/324): Loss: 0.6450 || Learning rate: lr=2.5e-05.
===> Epoch[239](220/324): Loss: 0.4450 || Learning rate: lr=2.5e-05.
===> Epoch[239](230/324): Loss: 0.4977 || Learning rate: lr=2.5e-05.
===> Epoch[239](240/324): Loss: 1.1186 || Learning rate: lr=2.5e-05.
===> Epoch[239](250/324): Loss: 0.7385 || Learning rate: lr=2.5e-05.
===> Epoch[239](260/324): Loss: 0.5738 || Learning rate: lr=2.5e-05.
===> Epoch[239](270/324): Loss: 0.4637 || Learning rate: lr=2.5e-05.
===> Epoch[239](280/324): Loss: 0.6453 || Learning rate: lr=2.5e-05.
===> Epoch[239](290/324): Loss: 0.5493 || Learning rate: lr=2.5e-05.
===> Epoch[239](300/324): Loss: 0.7119 || Learning rate: lr=2.5e-05.
===> Epoch[239](310/324): Loss: 0.7925 || Learning rate: lr=2.5e-05.
===> Epoch[239](320/324): Loss: 0.7261 || Learning rate: lr=2.5e-05.
===> Epoch[240](10/324): Loss: 0.5058 || Learning rate: lr=2.5e-05.
===> Epoch[240](20/324): Loss: 0.8489 || Learning rate: lr=2.5e-05.
===> Epoch[240](30/324): Loss: 0.8232 || Learning rate: lr=2.5e-05.
===> Epoch[240](40/324): Loss: 0.6925 || Learning rate: lr=2.5e-05.
===> Epoch[240](50/324): Loss: 0.3917 || Learning rate: lr=2.5e-05.
===> Epoch[240](60/324): Loss: 0.8962 || Learning rate: lr=2.5e-05.
===> Epoch[240](70/324): Loss: 0.6688 || Learning rate: lr=2.5e-05.
===> Epoch[240](80/324): Loss: 0.8112 || Learning rate: lr=2.5e-05.
===> Epoch[240](90/324): Loss: 0.6906 || Learning rate: lr=2.5e-05.
===> Epoch[240](100/324): Loss: 0.5684 || Learning rate: lr=2.5e-05.
===> Epoch[240](110/324): Loss: 0.5847 || Learning rate: lr=2.5e-05.
===> Epoch[240](120/324): Loss: 0.7084 || Learning rate: lr=2.5e-05.
===> Epoch[240](130/324): Loss: 0.7687 || Learning rate: lr=2.5e-05.
===> Epoch[240](140/324): Loss: 0.6320 || Learning rate: lr=2.5e-05.
===> Epoch[240](150/324): Loss: 0.7065 || Learning rate: lr=2.5e-05.
===> Epoch[240](160/324): Loss: 0.6574 || Learning rate: lr=2.5e-05.
===> Epoch[240](170/324): Loss: 0.6803 || Learning rate: lr=2.5e-05.
===> Epoch[240](180/324): Loss: 0.6162 || Learning rate: lr=2.5e-05.
===> Epoch[240](190/324): Loss: 0.6050 || Learning rate: lr=2.5e-05.
===> Epoch[240](200/324): Loss: 0.8036 || Learning rate: lr=2.5e-05.
===> Epoch[240](210/324): Loss: 1.0823 || Learning rate: lr=2.5e-05.
===> Epoch[240](220/324): Loss: 0.6512 || Learning rate: lr=2.5e-05.
===> Epoch[240](230/324): Loss: 0.7553 || Learning rate: lr=2.5e-05.
===> Epoch[240](240/324): Loss: 0.6831 || Learning rate: lr=2.5e-05.
===> Epoch[240](250/324): Loss: 0.8286 || Learning rate: lr=2.5e-05.
===> Epoch[240](260/324): Loss: 0.7006 || Learning rate: lr=2.5e-05.
===> Epoch[240](270/324): Loss: 0.6352 || Learning rate: lr=2.5e-05.
===> Epoch[240](280/324): Loss: 0.5586 || Learning rate: lr=2.5e-05.
===> Epoch[240](290/324): Loss: 0.7440 || Learning rate: lr=2.5e-05.
===> Epoch[240](300/324): Loss: 0.6413 || Learning rate: lr=2.5e-05.
===> Epoch[240](310/324): Loss: 0.4057 || Learning rate: lr=2.5e-05.
===> Epoch[240](320/324): Loss: 0.6804 || Learning rate: lr=2.5e-05.
Checkpoint saved to weights/epoch_v2_240.pth
===> Epoch[241](10/324): Loss: 0.5397 || Learning rate: lr=2.5e-05.
===> Epoch[241](20/324): Loss: 0.7898 || Learning rate: lr=2.5e-05.
===> Epoch[241](30/324): Loss: 1.0045 || Learning rate: lr=2.5e-05.
===> Epoch[241](40/324): Loss: 0.5955 || Learning rate: lr=2.5e-05.
===> Epoch[241](50/324): Loss: 0.5224 || Learning rate: lr=2.5e-05.
===> Epoch[241](60/324): Loss: 0.7510 || Learning rate: lr=2.5e-05.
===> Epoch[241](70/324): Loss: 0.7082 || Learning rate: lr=2.5e-05.
===> Epoch[241](80/324): Loss: 0.8333 || Learning rate: lr=2.5e-05.
===> Epoch[241](90/324): Loss: 0.6851 || Learning rate: lr=2.5e-05.
===> Epoch[241](100/324): Loss: 0.6693 || Learning rate: lr=2.5e-05.
===> Epoch[241](110/324): Loss: 0.7422 || Learning rate: lr=2.5e-05.
===> Epoch[241](120/324): Loss: 0.5221 || Learning rate: lr=2.5e-05.
===> Epoch[241](130/324): Loss: 0.9200 || Learning rate: lr=2.5e-05.
===> Epoch[241](140/324): Loss: 0.5260 || Learning rate: lr=2.5e-05.
===> Epoch[241](150/324): Loss: 0.5847 || Learning rate: lr=2.5e-05.
===> Epoch[241](160/324): Loss: 0.5880 || Learning rate: lr=2.5e-05.
===> Epoch[241](170/324): Loss: 0.6347 || Learning rate: lr=2.5e-05.
===> Epoch[241](180/324): Loss: 0.6657 || Learning rate: lr=2.5e-05.
===> Epoch[241](190/324): Loss: 0.6047 || Learning rate: lr=2.5e-05.
===> Epoch[241](200/324): Loss: 0.7439 || Learning rate: lr=2.5e-05.
===> Epoch[241](210/324): Loss: 0.7960 || Learning rate: lr=2.5e-05.
===> Epoch[241](220/324): Loss: 0.8392 || Learning rate: lr=2.5e-05.
===> Epoch[241](230/324): Loss: 0.5954 || Learning rate: lr=2.5e-05.
===> Epoch[241](240/324): Loss: 0.6586 || Learning rate: lr=2.5e-05.
===> Epoch[241](250/324): Loss: 0.6034 || Learning rate: lr=2.5e-05.
===> Epoch[241](260/324): Loss: 0.6205 || Learning rate: lr=2.5e-05.
===> Epoch[241](270/324): Loss: 0.4263 || Learning rate: lr=2.5e-05.
===> Epoch[241](280/324): Loss: 0.7758 || Learning rate: lr=2.5e-05.
===> Epoch[241](290/324): Loss: 0.6571 || Learning rate: lr=2.5e-05.
===> Epoch[241](300/324): Loss: 0.6350 || Learning rate: lr=2.5e-05.
===> Epoch[241](310/324): Loss: 0.5084 || Learning rate: lr=2.5e-05.
===> Epoch[241](320/324): Loss: 1.0802 || Learning rate: lr=2.5e-05.
===> Epoch[242](10/324): Loss: 0.7168 || Learning rate: lr=2.5e-05.
===> Epoch[242](20/324): Loss: 0.5647 || Learning rate: lr=2.5e-05.
===> Epoch[242](30/324): Loss: 0.7206 || Learning rate: lr=2.5e-05.
===> Epoch[242](40/324): Loss: 0.6695 || Learning rate: lr=2.5e-05.
===> Epoch[242](50/324): Loss: 0.6371 || Learning rate: lr=2.5e-05.
===> Epoch[242](60/324): Loss: 0.6635 || Learning rate: lr=2.5e-05.
===> Epoch[242](70/324): Loss: 0.6685 || Learning rate: lr=2.5e-05.
===> Epoch[242](80/324): Loss: 0.7191 || Learning rate: lr=2.5e-05.
===> Epoch[242](90/324): Loss: 0.6121 || Learning rate: lr=2.5e-05.
===> Epoch[242](100/324): Loss: 0.7441 || Learning rate: lr=2.5e-05.
===> Epoch[242](110/324): Loss: 0.5160 || Learning rate: lr=2.5e-05.
===> Epoch[242](120/324): Loss: 0.5019 || Learning rate: lr=2.5e-05.
===> Epoch[242](130/324): Loss: 0.4743 || Learning rate: lr=2.5e-05.
===> Epoch[242](140/324): Loss: 0.7342 || Learning rate: lr=2.5e-05.
===> Epoch[242](150/324): Loss: 0.5181 || Learning rate: lr=2.5e-05.
===> Epoch[242](160/324): Loss: 0.6881 || Learning rate: lr=2.5e-05.
===> Epoch[242](170/324): Loss: 0.9512 || Learning rate: lr=2.5e-05.
===> Epoch[242](180/324): Loss: 1.1580 || Learning rate: lr=2.5e-05.
===> Epoch[242](190/324): Loss: 0.6327 || Learning rate: lr=2.5e-05.
===> Epoch[242](200/324): Loss: 0.7610 || Learning rate: lr=2.5e-05.
===> Epoch[242](210/324): Loss: 0.8742 || Learning rate: lr=2.5e-05.
===> Epoch[242](220/324): Loss: 0.4958 || Learning rate: lr=2.5e-05.
===> Epoch[242](230/324): Loss: 0.8134 || Learning rate: lr=2.5e-05.
===> Epoch[242](240/324): Loss: 0.7249 || Learning rate: lr=2.5e-05.
===> Epoch[242](250/324): Loss: 1.0501 || Learning rate: lr=2.5e-05.
===> Epoch[242](260/324): Loss: 0.6804 || Learning rate: lr=2.5e-05.
===> Epoch[242](270/324): Loss: 0.5532 || Learning rate: lr=2.5e-05.
===> Epoch[242](280/324): Loss: 0.7343 || Learning rate: lr=2.5e-05.
===> Epoch[242](290/324): Loss: 0.8064 || Learning rate: lr=2.5e-05.
===> Epoch[242](300/324): Loss: 0.7020 || Learning rate: lr=2.5e-05.
===> Epoch[242](310/324): Loss: 0.4644 || Learning rate: lr=2.5e-05.
===> Epoch[242](320/324): Loss: 0.6221 || Learning rate: lr=2.5e-05.
===> Epoch[243](10/324): Loss: 0.7032 || Learning rate: lr=2.5e-05.
===> Epoch[243](20/324): Loss: 0.8518 || Learning rate: lr=2.5e-05.
===> Epoch[243](30/324): Loss: 0.5005 || Learning rate: lr=2.5e-05.
===> Epoch[243](40/324): Loss: 0.5996 || Learning rate: lr=2.5e-05.
===> Epoch[243](50/324): Loss: 0.7860 || Learning rate: lr=2.5e-05.
===> Epoch[243](60/324): Loss: 0.6061 || Learning rate: lr=2.5e-05.
===> Epoch[243](70/324): Loss: 0.5578 || Learning rate: lr=2.5e-05.
===> Epoch[243](80/324): Loss: 0.5038 || Learning rate: lr=2.5e-05.
===> Epoch[243](90/324): Loss: 0.6699 || Learning rate: lr=2.5e-05.
===> Epoch[243](100/324): Loss: 0.7998 || Learning rate: lr=2.5e-05.
===> Epoch[243](110/324): Loss: 1.3195 || Learning rate: lr=2.5e-05.
===> Epoch[243](120/324): Loss: 0.8041 || Learning rate: lr=2.5e-05.
===> Epoch[243](130/324): Loss: 0.7979 || Learning rate: lr=2.5e-05.
===> Epoch[243](140/324): Loss: 0.6816 || Learning rate: lr=2.5e-05.
===> Epoch[243](150/324): Loss: 0.7455 || Learning rate: lr=2.5e-05.
===> Epoch[243](160/324): Loss: 0.5261 || Learning rate: lr=2.5e-05.
===> Epoch[243](170/324): Loss: 0.8695 || Learning rate: lr=2.5e-05.
===> Epoch[243](180/324): Loss: 0.4997 || Learning rate: lr=2.5e-05.
===> Epoch[243](190/324): Loss: 0.8671 || Learning rate: lr=2.5e-05.
===> Epoch[243](200/324): Loss: 0.6136 || Learning rate: lr=2.5e-05.
===> Epoch[243](210/324): Loss: 0.6371 || Learning rate: lr=2.5e-05.
===> Epoch[243](220/324): Loss: 0.6061 || Learning rate: lr=2.5e-05.
===> Epoch[243](230/324): Loss: 0.8325 || Learning rate: lr=2.5e-05.
===> Epoch[243](240/324): Loss: 0.6458 || Learning rate: lr=2.5e-05.
===> Epoch[243](250/324): Loss: 0.5610 || Learning rate: lr=2.5e-05.
===> Epoch[243](260/324): Loss: 0.5135 || Learning rate: lr=2.5e-05.
===> Epoch[243](270/324): Loss: 0.5121 || Learning rate: lr=2.5e-05.
===> Epoch[243](280/324): Loss: 0.4599 || Learning rate: lr=2.5e-05.
===> Epoch[243](290/324): Loss: 0.8230 || Learning rate: lr=2.5e-05.
===> Epoch[243](300/324): Loss: 0.8982 || Learning rate: lr=2.5e-05.
===> Epoch[243](310/324): Loss: 0.9095 || Learning rate: lr=2.5e-05.
===> Epoch[243](320/324): Loss: 0.7008 || Learning rate: lr=2.5e-05.
===> Epoch[244](10/324): Loss: 0.4694 || Learning rate: lr=2.5e-05.
===> Epoch[244](20/324): Loss: 0.5572 || Learning rate: lr=2.5e-05.
===> Epoch[244](30/324): Loss: 0.7546 || Learning rate: lr=2.5e-05.
===> Epoch[244](40/324): Loss: 0.6902 || Learning rate: lr=2.5e-05.
===> Epoch[244](50/324): Loss: 0.6314 || Learning rate: lr=2.5e-05.
===> Epoch[244](60/324): Loss: 0.5248 || Learning rate: lr=2.5e-05.
===> Epoch[244](70/324): Loss: 0.8428 || Learning rate: lr=2.5e-05.
===> Epoch[244](80/324): Loss: 0.6548 || Learning rate: lr=2.5e-05.
===> Epoch[244](90/324): Loss: 0.7183 || Learning rate: lr=2.5e-05.
===> Epoch[244](100/324): Loss: 0.7252 || Learning rate: lr=2.5e-05.
===> Epoch[244](110/324): Loss: 0.9061 || Learning rate: lr=2.5e-05.
===> Epoch[244](120/324): Loss: 0.7799 || Learning rate: lr=2.5e-05.
===> Epoch[244](130/324): Loss: 0.6226 || Learning rate: lr=2.5e-05.
===> Epoch[244](140/324): Loss: 0.5783 || Learning rate: lr=2.5e-05.
===> Epoch[244](150/324): Loss: 0.6978 || Learning rate: lr=2.5e-05.
===> Epoch[244](160/324): Loss: 0.5416 || Learning rate: lr=2.5e-05.
===> Epoch[244](170/324): Loss: 0.9972 || Learning rate: lr=2.5e-05.
===> Epoch[244](180/324): Loss: 0.7002 || Learning rate: lr=2.5e-05.
===> Epoch[244](190/324): Loss: 0.5924 || Learning rate: lr=2.5e-05.
===> Epoch[244](200/324): Loss: 0.5870 || Learning rate: lr=2.5e-05.
===> Epoch[244](210/324): Loss: 0.5652 || Learning rate: lr=2.5e-05.
===> Epoch[244](220/324): Loss: 0.5199 || Learning rate: lr=2.5e-05.
===> Epoch[244](230/324): Loss: 0.5973 || Learning rate: lr=2.5e-05.
===> Epoch[244](240/324): Loss: 0.8886 || Learning rate: lr=2.5e-05.
===> Epoch[244](250/324): Loss: 0.5524 || Learning rate: lr=2.5e-05.
===> Epoch[244](260/324): Loss: 0.6574 || Learning rate: lr=2.5e-05.
===> Epoch[244](270/324): Loss: 0.9154 || Learning rate: lr=2.5e-05.
===> Epoch[244](280/324): Loss: 0.5801 || Learning rate: lr=2.5e-05.
===> Epoch[244](290/324): Loss: 0.7252 || Learning rate: lr=2.5e-05.
===> Epoch[244](300/324): Loss: 0.5956 || Learning rate: lr=2.5e-05.
===> Epoch[244](310/324): Loss: 0.5458 || Learning rate: lr=2.5e-05.
===> Epoch[244](320/324): Loss: 0.9779 || Learning rate: lr=2.5e-05.
===> Epoch[245](10/324): Loss: 0.7330 || Learning rate: lr=2.5e-05.
===> Epoch[245](20/324): Loss: 0.9416 || Learning rate: lr=2.5e-05.
===> Epoch[245](30/324): Loss: 0.5139 || Learning rate: lr=2.5e-05.
===> Epoch[245](40/324): Loss: 0.8519 || Learning rate: lr=2.5e-05.
===> Epoch[245](50/324): Loss: 0.8634 || Learning rate: lr=2.5e-05.
===> Epoch[245](60/324): Loss: 0.8052 || Learning rate: lr=2.5e-05.
===> Epoch[245](70/324): Loss: 0.6627 || Learning rate: lr=2.5e-05.
===> Epoch[245](80/324): Loss: 0.6534 || Learning rate: lr=2.5e-05.
===> Epoch[245](90/324): Loss: 0.6426 || Learning rate: lr=2.5e-05.
===> Epoch[245](100/324): Loss: 0.5284 || Learning rate: lr=2.5e-05.
===> Epoch[245](110/324): Loss: 0.4684 || Learning rate: lr=2.5e-05.
===> Epoch[245](120/324): Loss: 0.7541 || Learning rate: lr=2.5e-05.
===> Epoch[245](130/324): Loss: 0.6688 || Learning rate: lr=2.5e-05.
===> Epoch[245](140/324): Loss: 0.7827 || Learning rate: lr=2.5e-05.
===> Epoch[245](150/324): Loss: 0.7000 || Learning rate: lr=2.5e-05.
===> Epoch[245](160/324): Loss: 0.5701 || Learning rate: lr=2.5e-05.
===> Epoch[245](170/324): Loss: 0.5231 || Learning rate: lr=2.5e-05.
===> Epoch[245](180/324): Loss: 0.5740 || Learning rate: lr=2.5e-05.
===> Epoch[245](190/324): Loss: 0.6133 || Learning rate: lr=2.5e-05.
===> Epoch[245](200/324): Loss: 0.6426 || Learning rate: lr=2.5e-05.
===> Epoch[245](210/324): Loss: 0.6252 || Learning rate: lr=2.5e-05.
===> Epoch[245](220/324): Loss: 0.5725 || Learning rate: lr=2.5e-05.
===> Epoch[245](230/324): Loss: 0.6017 || Learning rate: lr=2.5e-05.
===> Epoch[245](240/324): Loss: 0.4922 || Learning rate: lr=2.5e-05.
===> Epoch[245](250/324): Loss: 0.5331 || Learning rate: lr=2.5e-05.
===> Epoch[245](260/324): Loss: 0.6878 || Learning rate: lr=2.5e-05.
===> Epoch[245](270/324): Loss: 0.6302 || Learning rate: lr=2.5e-05.
===> Epoch[245](280/324): Loss: 0.9501 || Learning rate: lr=2.5e-05.
===> Epoch[245](290/324): Loss: 0.8083 || Learning rate: lr=2.5e-05.
===> Epoch[245](300/324): Loss: 0.8019 || Learning rate: lr=2.5e-05.
===> Epoch[245](310/324): Loss: 1.1179 || Learning rate: lr=2.5e-05.
===> Epoch[245](320/324): Loss: 0.8487 || Learning rate: lr=2.5e-05.
===> Epoch[246](10/324): Loss: 0.8968 || Learning rate: lr=2.5e-05.
===> Epoch[246](20/324): Loss: 0.7838 || Learning rate: lr=2.5e-05.
===> Epoch[246](30/324): Loss: 0.7243 || Learning rate: lr=2.5e-05.
===> Epoch[246](40/324): Loss: 0.4967 || Learning rate: lr=2.5e-05.
===> Epoch[246](50/324): Loss: 0.8985 || Learning rate: lr=2.5e-05.
===> Epoch[246](60/324): Loss: 0.5291 || Learning rate: lr=2.5e-05.
===> Epoch[246](70/324): Loss: 0.6317 || Learning rate: lr=2.5e-05.
===> Epoch[246](80/324): Loss: 0.5238 || Learning rate: lr=2.5e-05.
===> Epoch[246](90/324): Loss: 0.8234 || Learning rate: lr=2.5e-05.
===> Epoch[246](100/324): Loss: 0.5414 || Learning rate: lr=2.5e-05.
===> Epoch[246](110/324): Loss: 0.6516 || Learning rate: lr=2.5e-05.
===> Epoch[246](120/324): Loss: 0.6038 || Learning rate: lr=2.5e-05.
===> Epoch[246](130/324): Loss: 0.7227 || Learning rate: lr=2.5e-05.
===> Epoch[246](140/324): Loss: 0.6705 || Learning rate: lr=2.5e-05.
===> Epoch[246](150/324): Loss: 0.7498 || Learning rate: lr=2.5e-05.
===> Epoch[246](160/324): Loss: 0.4963 || Learning rate: lr=2.5e-05.
===> Epoch[246](170/324): Loss: 0.3926 || Learning rate: lr=2.5e-05.
===> Epoch[246](180/324): Loss: 0.4999 || Learning rate: lr=2.5e-05.
===> Epoch[246](190/324): Loss: 0.7969 || Learning rate: lr=2.5e-05.
===> Epoch[246](200/324): Loss: 0.7705 || Learning rate: lr=2.5e-05.
===> Epoch[246](210/324): Loss: 0.8198 || Learning rate: lr=2.5e-05.
===> Epoch[246](220/324): Loss: 0.5939 || Learning rate: lr=2.5e-05.
===> Epoch[246](230/324): Loss: 0.5530 || Learning rate: lr=2.5e-05.
===> Epoch[246](240/324): Loss: 0.8937 || Learning rate: lr=2.5e-05.
===> Epoch[246](250/324): Loss: 0.4839 || Learning rate: lr=2.5e-05.
===> Epoch[246](260/324): Loss: 0.9987 || Learning rate: lr=2.5e-05.
===> Epoch[246](270/324): Loss: 0.6379 || Learning rate: lr=2.5e-05.
===> Epoch[246](280/324): Loss: 0.6552 || Learning rate: lr=2.5e-05.
===> Epoch[246](290/324): Loss: 0.5779 || Learning rate: lr=2.5e-05.
===> Epoch[246](300/324): Loss: 0.8857 || Learning rate: lr=2.5e-05.
===> Epoch[246](310/324): Loss: 0.6287 || Learning rate: lr=2.5e-05.
===> Epoch[246](320/324): Loss: 0.6147 || Learning rate: lr=2.5e-05.
===> Epoch[247](10/324): Loss: 0.6000 || Learning rate: lr=2.5e-05.
===> Epoch[247](20/324): Loss: 0.6882 || Learning rate: lr=2.5e-05.
===> Epoch[247](30/324): Loss: 0.5523 || Learning rate: lr=2.5e-05.
===> Epoch[247](40/324): Loss: 0.5157 || Learning rate: lr=2.5e-05.
===> Epoch[247](50/324): Loss: 0.7367 || Learning rate: lr=2.5e-05.
===> Epoch[247](60/324): Loss: 0.6699 || Learning rate: lr=2.5e-05.
===> Epoch[247](70/324): Loss: 1.1099 || Learning rate: lr=2.5e-05.
===> Epoch[247](80/324): Loss: 0.8636 || Learning rate: lr=2.5e-05.
===> Epoch[247](90/324): Loss: 0.8494 || Learning rate: lr=2.5e-05.
===> Epoch[247](100/324): Loss: 0.7775 || Learning rate: lr=2.5e-05.
===> Epoch[247](110/324): Loss: 0.6523 || Learning rate: lr=2.5e-05.
===> Epoch[247](120/324): Loss: 0.7674 || Learning rate: lr=2.5e-05.
===> Epoch[247](130/324): Loss: 0.8597 || Learning rate: lr=2.5e-05.
===> Epoch[247](140/324): Loss: 0.6720 || Learning rate: lr=2.5e-05.
===> Epoch[247](150/324): Loss: 0.7518 || Learning rate: lr=2.5e-05.
===> Epoch[247](160/324): Loss: 0.9439 || Learning rate: lr=2.5e-05.
===> Epoch[247](170/324): Loss: 0.4808 || Learning rate: lr=2.5e-05.
===> Epoch[247](180/324): Loss: 0.6893 || Learning rate: lr=2.5e-05.
===> Epoch[247](190/324): Loss: 0.6072 || Learning rate: lr=2.5e-05.
===> Epoch[247](200/324): Loss: 0.7438 || Learning rate: lr=2.5e-05.
===> Epoch[247](210/324): Loss: 0.7834 || Learning rate: lr=2.5e-05.
===> Epoch[247](220/324): Loss: 0.7120 || Learning rate: lr=2.5e-05.
===> Epoch[247](230/324): Loss: 0.7863 || Learning rate: lr=2.5e-05.
===> Epoch[247](240/324): Loss: 0.6355 || Learning rate: lr=2.5e-05.
===> Epoch[247](250/324): Loss: 0.6206 || Learning rate: lr=2.5e-05.
===> Epoch[247](260/324): Loss: 0.5555 || Learning rate: lr=2.5e-05.
===> Epoch[247](270/324): Loss: 0.4479 || Learning rate: lr=2.5e-05.
===> Epoch[247](280/324): Loss: 0.6558 || Learning rate: lr=2.5e-05.
===> Epoch[247](290/324): Loss: 0.5924 || Learning rate: lr=2.5e-05.
===> Epoch[247](300/324): Loss: 0.4386 || Learning rate: lr=2.5e-05.
===> Epoch[247](310/324): Loss: 0.6131 || Learning rate: lr=2.5e-05.
===> Epoch[247](320/324): Loss: 0.7312 || Learning rate: lr=2.5e-05.
===> Epoch[248](10/324): Loss: 0.6251 || Learning rate: lr=2.5e-05.
===> Epoch[248](20/324): Loss: 0.4752 || Learning rate: lr=2.5e-05.
===> Epoch[248](30/324): Loss: 0.7450 || Learning rate: lr=2.5e-05.
===> Epoch[248](40/324): Loss: 0.6732 || Learning rate: lr=2.5e-05.
===> Epoch[248](50/324): Loss: 0.5392 || Learning rate: lr=2.5e-05.
===> Epoch[248](60/324): Loss: 0.5279 || Learning rate: lr=2.5e-05.
===> Epoch[248](70/324): Loss: 0.7320 || Learning rate: lr=2.5e-05.
===> Epoch[248](80/324): Loss: 0.8141 || Learning rate: lr=2.5e-05.
===> Epoch[248](90/324): Loss: 0.8546 || Learning rate: lr=2.5e-05.
===> Epoch[248](100/324): Loss: 0.5894 || Learning rate: lr=2.5e-05.
===> Epoch[248](110/324): Loss: 0.6773 || Learning rate: lr=2.5e-05.
===> Epoch[248](120/324): Loss: 0.5564 || Learning rate: lr=2.5e-05.
===> Epoch[248](130/324): Loss: 0.8493 || Learning rate: lr=2.5e-05.
===> Epoch[248](140/324): Loss: 0.6834 || Learning rate: lr=2.5e-05.
===> Epoch[248](150/324): Loss: 0.7310 || Learning rate: lr=2.5e-05.
===> Epoch[248](160/324): Loss: 0.6781 || Learning rate: lr=2.5e-05.
===> Epoch[248](170/324): Loss: 1.0318 || Learning rate: lr=2.5e-05.
===> Epoch[248](180/324): Loss: 0.9598 || Learning rate: lr=2.5e-05.
===> Epoch[248](190/324): Loss: 0.6606 || Learning rate: lr=2.5e-05.
===> Epoch[248](200/324): Loss: 0.9678 || Learning rate: lr=2.5e-05.
===> Epoch[248](210/324): Loss: 0.6085 || Learning rate: lr=2.5e-05.
===> Epoch[248](220/324): Loss: 0.6436 || Learning rate: lr=2.5e-05.
===> Epoch[248](230/324): Loss: 0.7097 || Learning rate: lr=2.5e-05.
===> Epoch[248](240/324): Loss: 0.7601 || Learning rate: lr=2.5e-05.
===> Epoch[248](250/324): Loss: 0.6228 || Learning rate: lr=2.5e-05.
===> Epoch[248](260/324): Loss: 0.5773 || Learning rate: lr=2.5e-05.
===> Epoch[248](270/324): Loss: 0.8595 || Learning rate: lr=2.5e-05.
===> Epoch[248](280/324): Loss: 0.7614 || Learning rate: lr=2.5e-05.
===> Epoch[248](290/324): Loss: 0.6026 || Learning rate: lr=2.5e-05.
===> Epoch[248](300/324): Loss: 0.8070 || Learning rate: lr=2.5e-05.
===> Epoch[248](310/324): Loss: 0.4426 || Learning rate: lr=2.5e-05.
===> Epoch[248](320/324): Loss: 0.5416 || Learning rate: lr=2.5e-05.
===> Epoch[249](10/324): Loss: 0.6389 || Learning rate: lr=2.5e-05.
===> Epoch[249](20/324): Loss: 0.5118 || Learning rate: lr=2.5e-05.
===> Epoch[249](30/324): Loss: 0.7133 || Learning rate: lr=2.5e-05.
===> Epoch[249](40/324): Loss: 0.7031 || Learning rate: lr=2.5e-05.
===> Epoch[249](50/324): Loss: 0.3162 || Learning rate: lr=2.5e-05.
===> Epoch[249](60/324): Loss: 0.9221 || Learning rate: lr=2.5e-05.
===> Epoch[249](70/324): Loss: 0.4681 || Learning rate: lr=2.5e-05.
===> Epoch[249](80/324): Loss: 0.6417 || Learning rate: lr=2.5e-05.
===> Epoch[249](90/324): Loss: 0.7313 || Learning rate: lr=2.5e-05.
===> Epoch[249](100/324): Loss: 0.7905 || Learning rate: lr=2.5e-05.
===> Epoch[249](110/324): Loss: 0.7361 || Learning rate: lr=2.5e-05.
===> Epoch[249](120/324): Loss: 0.9798 || Learning rate: lr=2.5e-05.
===> Epoch[249](130/324): Loss: 0.9211 || Learning rate: lr=2.5e-05.
===> Epoch[249](140/324): Loss: 1.0695 || Learning rate: lr=2.5e-05.
===> Epoch[249](150/324): Loss: 0.7712 || Learning rate: lr=2.5e-05.
===> Epoch[249](160/324): Loss: 0.6186 || Learning rate: lr=2.5e-05.
===> Epoch[249](170/324): Loss: 0.6253 || Learning rate: lr=2.5e-05.
===> Epoch[249](180/324): Loss: 0.6895 || Learning rate: lr=2.5e-05.
===> Epoch[249](190/324): Loss: 0.5897 || Learning rate: lr=2.5e-05.
===> Epoch[249](200/324): Loss: 0.8848 || Learning rate: lr=2.5e-05.
===> Epoch[249](210/324): Loss: 0.4380 || Learning rate: lr=2.5e-05.
===> Epoch[249](220/324): Loss: 0.6698 || Learning rate: lr=2.5e-05.
===> Epoch[249](230/324): Loss: 0.5071 || Learning rate: lr=2.5e-05.
===> Epoch[249](240/324): Loss: 0.5800 || Learning rate: lr=2.5e-05.
===> Epoch[249](250/324): Loss: 0.7290 || Learning rate: lr=2.5e-05.
===> Epoch[249](260/324): Loss: 0.8820 || Learning rate: lr=2.5e-05.
===> Epoch[249](270/324): Loss: 0.4707 || Learning rate: lr=2.5e-05.
===> Epoch[249](280/324): Loss: 0.6115 || Learning rate: lr=2.5e-05.
===> Epoch[249](290/324): Loss: 0.4884 || Learning rate: lr=2.5e-05.
===> Epoch[249](300/324): Loss: 0.6238 || Learning rate: lr=2.5e-05.
===> Epoch[249](310/324): Loss: 0.7349 || Learning rate: lr=2.5e-05.
===> Epoch[249](320/324): Loss: 0.5740 || Learning rate: lr=2.5e-05.
===> Epoch[250](10/324): Loss: 0.6222 || Learning rate: lr=2.5e-05.
===> Epoch[250](20/324): Loss: 0.4281 || Learning rate: lr=2.5e-05.
===> Epoch[250](30/324): Loss: 0.5556 || Learning rate: lr=2.5e-05.
===> Epoch[250](40/324): Loss: 0.5694 || Learning rate: lr=2.5e-05.
===> Epoch[250](50/324): Loss: 0.7411 || Learning rate: lr=2.5e-05.
===> Epoch[250](60/324): Loss: 0.5401 || Learning rate: lr=2.5e-05.
===> Epoch[250](70/324): Loss: 0.4495 || Learning rate: lr=2.5e-05.
===> Epoch[250](80/324): Loss: 0.5917 || Learning rate: lr=2.5e-05.
===> Epoch[250](90/324): Loss: 0.5859 || Learning rate: lr=2.5e-05.
===> Epoch[250](100/324): Loss: 0.9464 || Learning rate: lr=2.5e-05.
===> Epoch[250](110/324): Loss: 0.7133 || Learning rate: lr=2.5e-05.
===> Epoch[250](120/324): Loss: 0.8280 || Learning rate: lr=2.5e-05.
===> Epoch[250](130/324): Loss: 0.9139 || Learning rate: lr=2.5e-05.
===> Epoch[250](140/324): Loss: 0.6646 || Learning rate: lr=2.5e-05.
===> Epoch[250](150/324): Loss: 0.5776 || Learning rate: lr=2.5e-05.
===> Epoch[250](160/324): Loss: 0.4779 || Learning rate: lr=2.5e-05.
===> Epoch[250](170/324): Loss: 0.4443 || Learning rate: lr=2.5e-05.
===> Epoch[250](180/324): Loss: 0.5322 || Learning rate: lr=2.5e-05.
===> Epoch[250](190/324): Loss: 0.5653 || Learning rate: lr=2.5e-05.
===> Epoch[250](200/324): Loss: 0.7684 || Learning rate: lr=2.5e-05.
===> Epoch[250](210/324): Loss: 0.9412 || Learning rate: lr=2.5e-05.
===> Epoch[250](220/324): Loss: 0.7772 || Learning rate: lr=2.5e-05.
===> Epoch[250](230/324): Loss: 0.8432 || Learning rate: lr=2.5e-05.
===> Epoch[250](240/324): Loss: 0.5189 || Learning rate: lr=2.5e-05.
===> Epoch[250](250/324): Loss: 0.7041 || Learning rate: lr=2.5e-05.
===> Epoch[250](260/324): Loss: 0.9017 || Learning rate: lr=2.5e-05.
===> Epoch[250](270/324): Loss: 0.7301 || Learning rate: lr=2.5e-05.
===> Epoch[250](280/324): Loss: 0.4940 || Learning rate: lr=2.5e-05.
===> Epoch[250](290/324): Loss: 0.7696 || Learning rate: lr=2.5e-05.
===> Epoch[250](300/324): Loss: 1.1107 || Learning rate: lr=2.5e-05.
===> Epoch[250](310/324): Loss: 0.6227 || Learning rate: lr=2.5e-05.
===> Epoch[250](320/324): Loss: 0.4158 || Learning rate: lr=2.5e-05.
===> Epoch[251](10/324): Loss: 0.7100 || Learning rate: lr=2.5e-05.
===> Epoch[251](20/324): Loss: 0.7978 || Learning rate: lr=2.5e-05.
===> Epoch[251](30/324): Loss: 0.6659 || Learning rate: lr=2.5e-05.
===> Epoch[251](40/324): Loss: 0.4533 || Learning rate: lr=2.5e-05.
===> Epoch[251](50/324): Loss: 0.9765 || Learning rate: lr=2.5e-05.
===> Epoch[251](60/324): Loss: 0.7166 || Learning rate: lr=2.5e-05.
===> Epoch[251](70/324): Loss: 0.5588 || Learning rate: lr=2.5e-05.
===> Epoch[251](80/324): Loss: 0.7443 || Learning rate: lr=2.5e-05.
===> Epoch[251](90/324): Loss: 0.6606 || Learning rate: lr=2.5e-05.
===> Epoch[251](100/324): Loss: 0.5997 || Learning rate: lr=2.5e-05.
===> Epoch[251](110/324): Loss: 0.6306 || Learning rate: lr=2.5e-05.
===> Epoch[251](120/324): Loss: 0.5908 || Learning rate: lr=2.5e-05.
===> Epoch[251](130/324): Loss: 0.5910 || Learning rate: lr=2.5e-05.
===> Epoch[251](140/324): Loss: 0.4063 || Learning rate: lr=2.5e-05.
===> Epoch[251](150/324): Loss: 0.4791 || Learning rate: lr=2.5e-05.
===> Epoch[251](160/324): Loss: 0.5888 || Learning rate: lr=2.5e-05.
===> Epoch[251](170/324): Loss: 0.7480 || Learning rate: lr=2.5e-05.
===> Epoch[251](180/324): Loss: 0.5953 || Learning rate: lr=2.5e-05.
===> Epoch[251](190/324): Loss: 0.7387 || Learning rate: lr=2.5e-05.
===> Epoch[251](200/324): Loss: 0.5363 || Learning rate: lr=2.5e-05.
===> Epoch[251](210/324): Loss: 0.4582 || Learning rate: lr=2.5e-05.
===> Epoch[251](220/324): Loss: 0.6657 || Learning rate: lr=2.5e-05.
===> Epoch[251](230/324): Loss: 0.6384 || Learning rate: lr=2.5e-05.
===> Epoch[251](240/324): Loss: 0.7766 || Learning rate: lr=2.5e-05.
===> Epoch[251](250/324): Loss: 0.7481 || Learning rate: lr=2.5e-05.
===> Epoch[251](260/324): Loss: 0.4288 || Learning rate: lr=2.5e-05.
===> Epoch[251](270/324): Loss: 0.8091 || Learning rate: lr=2.5e-05.
===> Epoch[251](280/324): Loss: 0.6845 || Learning rate: lr=2.5e-05.
===> Epoch[251](290/324): Loss: 0.7380 || Learning rate: lr=2.5e-05.
===> Epoch[251](300/324): Loss: 0.7403 || Learning rate: lr=2.5e-05.
===> Epoch[251](310/324): Loss: 0.7662 || Learning rate: lr=2.5e-05.
===> Epoch[251](320/324): Loss: 0.5664 || Learning rate: lr=2.5e-05.
===> Epoch[252](10/324): Loss: 1.0943 || Learning rate: lr=2.5e-05.
===> Epoch[252](20/324): Loss: 0.6123 || Learning rate: lr=2.5e-05.
===> Epoch[252](30/324): Loss: 0.5285 || Learning rate: lr=2.5e-05.
===> Epoch[252](40/324): Loss: 0.5670 || Learning rate: lr=2.5e-05.
===> Epoch[252](50/324): Loss: 0.4281 || Learning rate: lr=2.5e-05.
===> Epoch[252](60/324): Loss: 0.4575 || Learning rate: lr=2.5e-05.
===> Epoch[252](70/324): Loss: 0.6022 || Learning rate: lr=2.5e-05.
===> Epoch[252](80/324): Loss: 0.3043 || Learning rate: lr=2.5e-05.
===> Epoch[252](90/324): Loss: 0.8036 || Learning rate: lr=2.5e-05.
===> Epoch[252](100/324): Loss: 0.7689 || Learning rate: lr=2.5e-05.
===> Epoch[252](110/324): Loss: 0.7022 || Learning rate: lr=2.5e-05.
===> Epoch[252](120/324): Loss: 0.7071 || Learning rate: lr=2.5e-05.
===> Epoch[252](130/324): Loss: 0.8052 || Learning rate: lr=2.5e-05.
===> Epoch[252](140/324): Loss: 0.7240 || Learning rate: lr=2.5e-05.
===> Epoch[252](150/324): Loss: 0.5180 || Learning rate: lr=2.5e-05.
===> Epoch[252](160/324): Loss: 0.5523 || Learning rate: lr=2.5e-05.
===> Epoch[252](170/324): Loss: 0.4987 || Learning rate: lr=2.5e-05.
===> Epoch[252](180/324): Loss: 0.9048 || Learning rate: lr=2.5e-05.
===> Epoch[252](190/324): Loss: 0.8844 || Learning rate: lr=2.5e-05.
===> Epoch[252](200/324): Loss: 0.6977 || Learning rate: lr=2.5e-05.
===> Epoch[252](210/324): Loss: 0.5777 || Learning rate: lr=2.5e-05.
===> Epoch[252](220/324): Loss: 0.8365 || Learning rate: lr=2.5e-05.
===> Epoch[252](230/324): Loss: 0.6919 || Learning rate: lr=2.5e-05.
===> Epoch[252](240/324): Loss: 0.5578 || Learning rate: lr=2.5e-05.
===> Epoch[252](250/324): Loss: 0.8016 || Learning rate: lr=2.5e-05.
===> Epoch[252](260/324): Loss: 0.6865 || Learning rate: lr=2.5e-05.
===> Epoch[252](270/324): Loss: 0.9488 || Learning rate: lr=2.5e-05.
===> Epoch[252](280/324): Loss: 0.5100 || Learning rate: lr=2.5e-05.
===> Epoch[252](290/324): Loss: 0.6331 || Learning rate: lr=2.5e-05.
===> Epoch[252](300/324): Loss: 1.1991 || Learning rate: lr=2.5e-05.
===> Epoch[252](310/324): Loss: 0.5752 || Learning rate: lr=2.5e-05.
===> Epoch[252](320/324): Loss: 0.6695 || Learning rate: lr=2.5e-05.
===> Epoch[253](10/324): Loss: 0.7820 || Learning rate: lr=2.5e-05.
===> Epoch[253](20/324): Loss: 0.4814 || Learning rate: lr=2.5e-05.
===> Epoch[253](30/324): Loss: 0.4489 || Learning rate: lr=2.5e-05.
===> Epoch[253](40/324): Loss: 0.7199 || Learning rate: lr=2.5e-05.
===> Epoch[253](50/324): Loss: 0.8284 || Learning rate: lr=2.5e-05.
===> Epoch[253](60/324): Loss: 1.0449 || Learning rate: lr=2.5e-05.
===> Epoch[253](70/324): Loss: 0.7745 || Learning rate: lr=2.5e-05.
===> Epoch[253](80/324): Loss: 0.7273 || Learning rate: lr=2.5e-05.
===> Epoch[253](90/324): Loss: 0.6167 || Learning rate: lr=2.5e-05.
===> Epoch[253](100/324): Loss: 0.5140 || Learning rate: lr=2.5e-05.
===> Epoch[253](110/324): Loss: 0.6125 || Learning rate: lr=2.5e-05.
===> Epoch[253](120/324): Loss: 0.7101 || Learning rate: lr=2.5e-05.
===> Epoch[253](130/324): Loss: 0.3624 || Learning rate: lr=2.5e-05.
===> Epoch[253](140/324): Loss: 0.7445 || Learning rate: lr=2.5e-05.
===> Epoch[253](150/324): Loss: 0.6685 || Learning rate: lr=2.5e-05.
===> Epoch[253](160/324): Loss: 0.8288 || Learning rate: lr=2.5e-05.
===> Epoch[253](170/324): Loss: 0.5257 || Learning rate: lr=2.5e-05.
===> Epoch[253](180/324): Loss: 0.9125 || Learning rate: lr=2.5e-05.
===> Epoch[253](190/324): Loss: 0.8129 || Learning rate: lr=2.5e-05.
===> Epoch[253](200/324): Loss: 0.7219 || Learning rate: lr=2.5e-05.
===> Epoch[253](210/324): Loss: 0.6221 || Learning rate: lr=2.5e-05.
===> Epoch[253](220/324): Loss: 0.6944 || Learning rate: lr=2.5e-05.
===> Epoch[253](230/324): Loss: 0.4927 || Learning rate: lr=2.5e-05.
===> Epoch[253](240/324): Loss: 0.3680 || Learning rate: lr=2.5e-05.
===> Epoch[253](250/324): Loss: 1.0518 || Learning rate: lr=2.5e-05.
===> Epoch[253](260/324): Loss: 0.9172 || Learning rate: lr=2.5e-05.
===> Epoch[253](270/324): Loss: 0.4589 || Learning rate: lr=2.5e-05.
===> Epoch[253](280/324): Loss: 0.5806 || Learning rate: lr=2.5e-05.
===> Epoch[253](290/324): Loss: 0.8006 || Learning rate: lr=2.5e-05.
===> Epoch[253](300/324): Loss: 0.7687 || Learning rate: lr=2.5e-05.
===> Epoch[253](310/324): Loss: 0.6406 || Learning rate: lr=2.5e-05.
===> Epoch[253](320/324): Loss: 0.7057 || Learning rate: lr=2.5e-05.
===> Epoch[254](10/324): Loss: 0.7597 || Learning rate: lr=2.5e-05.
===> Epoch[254](20/324): Loss: 0.6191 || Learning rate: lr=2.5e-05.
===> Epoch[254](30/324): Loss: 0.9495 || Learning rate: lr=2.5e-05.
===> Epoch[254](40/324): Loss: 0.6972 || Learning rate: lr=2.5e-05.
===> Epoch[254](50/324): Loss: 0.5498 || Learning rate: lr=2.5e-05.
===> Epoch[254](60/324): Loss: 0.6045 || Learning rate: lr=2.5e-05.
===> Epoch[254](70/324): Loss: 0.5558 || Learning rate: lr=2.5e-05.
===> Epoch[254](80/324): Loss: 0.4664 || Learning rate: lr=2.5e-05.
===> Epoch[254](90/324): Loss: 0.8683 || Learning rate: lr=2.5e-05.
===> Epoch[254](100/324): Loss: 0.8826 || Learning rate: lr=2.5e-05.
===> Epoch[254](110/324): Loss: 0.6422 || Learning rate: lr=2.5e-05.
===> Epoch[254](120/324): Loss: 0.5001 || Learning rate: lr=2.5e-05.
===> Epoch[254](130/324): Loss: 0.5486 || Learning rate: lr=2.5e-05.
===> Epoch[254](140/324): Loss: 0.7642 || Learning rate: lr=2.5e-05.
===> Epoch[254](150/324): Loss: 0.7950 || Learning rate: lr=2.5e-05.
===> Epoch[254](160/324): Loss: 0.6510 || Learning rate: lr=2.5e-05.
===> Epoch[254](170/324): Loss: 1.0447 || Learning rate: lr=2.5e-05.
===> Epoch[254](180/324): Loss: 0.6670 || Learning rate: lr=2.5e-05.
===> Epoch[254](190/324): Loss: 0.5851 || Learning rate: lr=2.5e-05.
===> Epoch[254](200/324): Loss: 0.7931 || Learning rate: lr=2.5e-05.
===> Epoch[254](210/324): Loss: 0.7410 || Learning rate: lr=2.5e-05.
===> Epoch[254](220/324): Loss: 0.5476 || Learning rate: lr=2.5e-05.
===> Epoch[254](230/324): Loss: 0.4801 || Learning rate: lr=2.5e-05.
===> Epoch[254](240/324): Loss: 0.7673 || Learning rate: lr=2.5e-05.
===> Epoch[254](250/324): Loss: 1.0159 || Learning rate: lr=2.5e-05.
===> Epoch[254](260/324): Loss: 0.7931 || Learning rate: lr=2.5e-05.
===> Epoch[254](270/324): Loss: 0.5070 || Learning rate: lr=2.5e-05.
===> Epoch[254](280/324): Loss: 0.5747 || Learning rate: lr=2.5e-05.
===> Epoch[254](290/324): Loss: 0.7336 || Learning rate: lr=2.5e-05.
===> Epoch[254](300/324): Loss: 0.6250 || Learning rate: lr=2.5e-05.
===> Epoch[254](310/324): Loss: 0.8299 || Learning rate: lr=2.5e-05.
===> Epoch[254](320/324): Loss: 0.7645 || Learning rate: lr=2.5e-05.
===> Epoch[255](10/324): Loss: 0.7253 || Learning rate: lr=2.5e-05.
===> Epoch[255](20/324): Loss: 0.7353 || Learning rate: lr=2.5e-05.
===> Epoch[255](30/324): Loss: 0.8392 || Learning rate: lr=2.5e-05.
===> Epoch[255](40/324): Loss: 0.7654 || Learning rate: lr=2.5e-05.
===> Epoch[255](50/324): Loss: 0.7756 || Learning rate: lr=2.5e-05.
===> Epoch[255](60/324): Loss: 0.8624 || Learning rate: lr=2.5e-05.
===> Epoch[255](70/324): Loss: 0.7184 || Learning rate: lr=2.5e-05.
===> Epoch[255](80/324): Loss: 0.6470 || Learning rate: lr=2.5e-05.
===> Epoch[255](90/324): Loss: 0.5203 || Learning rate: lr=2.5e-05.
===> Epoch[255](100/324): Loss: 0.7095 || Learning rate: lr=2.5e-05.
===> Epoch[255](110/324): Loss: 0.3900 || Learning rate: lr=2.5e-05.
===> Epoch[255](120/324): Loss: 0.6870 || Learning rate: lr=2.5e-05.
===> Epoch[255](130/324): Loss: 0.7144 || Learning rate: lr=2.5e-05.
===> Epoch[255](140/324): Loss: 0.7466 || Learning rate: lr=2.5e-05.
===> Epoch[255](150/324): Loss: 0.7247 || Learning rate: lr=2.5e-05.
===> Epoch[255](160/324): Loss: 0.5857 || Learning rate: lr=2.5e-05.
===> Epoch[255](170/324): Loss: 0.7919 || Learning rate: lr=2.5e-05.
===> Epoch[255](180/324): Loss: 0.5962 || Learning rate: lr=2.5e-05.
===> Epoch[255](190/324): Loss: 0.5212 || Learning rate: lr=2.5e-05.
===> Epoch[255](200/324): Loss: 0.7717 || Learning rate: lr=2.5e-05.
===> Epoch[255](210/324): Loss: 0.6338 || Learning rate: lr=2.5e-05.
===> Epoch[255](220/324): Loss: 0.6884 || Learning rate: lr=2.5e-05.
===> Epoch[255](230/324): Loss: 0.6296 || Learning rate: lr=2.5e-05.
===> Epoch[255](240/324): Loss: 0.4399 || Learning rate: lr=2.5e-05.
===> Epoch[255](250/324): Loss: 0.7117 || Learning rate: lr=2.5e-05.
===> Epoch[255](260/324): Loss: 0.4979 || Learning rate: lr=2.5e-05.
===> Epoch[255](270/324): Loss: 0.8097 || Learning rate: lr=2.5e-05.
===> Epoch[255](280/324): Loss: 0.4943 || Learning rate: lr=2.5e-05.
===> Epoch[255](290/324): Loss: 0.5913 || Learning rate: lr=2.5e-05.
===> Epoch[255](300/324): Loss: 0.7323 || Learning rate: lr=2.5e-05.
===> Epoch[255](310/324): Loss: 0.5951 || Learning rate: lr=2.5e-05.
===> Epoch[255](320/324): Loss: 0.7724 || Learning rate: lr=2.5e-05.
===> Epoch[256](10/324): Loss: 0.6844 || Learning rate: lr=2.5e-05.
===> Epoch[256](20/324): Loss: 0.7865 || Learning rate: lr=2.5e-05.
===> Epoch[256](30/324): Loss: 0.7310 || Learning rate: lr=2.5e-05.
===> Epoch[256](40/324): Loss: 0.8041 || Learning rate: lr=2.5e-05.
===> Epoch[256](50/324): Loss: 0.6316 || Learning rate: lr=2.5e-05.
===> Epoch[256](60/324): Loss: 0.5261 || Learning rate: lr=2.5e-05.
===> Epoch[256](70/324): Loss: 0.6712 || Learning rate: lr=2.5e-05.
===> Epoch[256](80/324): Loss: 0.4189 || Learning rate: lr=2.5e-05.
===> Epoch[256](90/324): Loss: 0.9988 || Learning rate: lr=2.5e-05.
===> Epoch[256](100/324): Loss: 0.5469 || Learning rate: lr=2.5e-05.
===> Epoch[256](110/324): Loss: 0.5720 || Learning rate: lr=2.5e-05.
===> Epoch[256](120/324): Loss: 0.7809 || Learning rate: lr=2.5e-05.
===> Epoch[256](130/324): Loss: 0.5429 || Learning rate: lr=2.5e-05.
===> Epoch[256](140/324): Loss: 0.6171 || Learning rate: lr=2.5e-05.
===> Epoch[256](150/324): Loss: 0.9425 || Learning rate: lr=2.5e-05.
===> Epoch[256](160/324): Loss: 0.5162 || Learning rate: lr=2.5e-05.
===> Epoch[256](170/324): Loss: 0.9253 || Learning rate: lr=2.5e-05.
===> Epoch[256](180/324): Loss: 0.5774 || Learning rate: lr=2.5e-05.
===> Epoch[256](190/324): Loss: 0.7443 || Learning rate: lr=2.5e-05.
===> Epoch[256](200/324): Loss: 0.5460 || Learning rate: lr=2.5e-05.
===> Epoch[256](210/324): Loss: 1.0168 || Learning rate: lr=2.5e-05.
===> Epoch[256](220/324): Loss: 0.6053 || Learning rate: lr=2.5e-05.
===> Epoch[256](230/324): Loss: 0.5263 || Learning rate: lr=2.5e-05.
===> Epoch[256](240/324): Loss: 0.7972 || Learning rate: lr=2.5e-05.
===> Epoch[256](250/324): Loss: 0.5374 || Learning rate: lr=2.5e-05.
===> Epoch[256](260/324): Loss: 0.8114 || Learning rate: lr=2.5e-05.
===> Epoch[256](270/324): Loss: 0.5909 || Learning rate: lr=2.5e-05.
===> Epoch[256](280/324): Loss: 1.1018 || Learning rate: lr=2.5e-05.
===> Epoch[256](290/324): Loss: 0.6841 || Learning rate: lr=2.5e-05.
===> Epoch[256](300/324): Loss: 0.6531 || Learning rate: lr=2.5e-05.
===> Epoch[256](310/324): Loss: 0.6774 || Learning rate: lr=2.5e-05.
===> Epoch[256](320/324): Loss: 0.7528 || Learning rate: lr=2.5e-05.
===> Epoch[257](10/324): Loss: 0.7609 || Learning rate: lr=2.5e-05.
===> Epoch[257](20/324): Loss: 0.6121 || Learning rate: lr=2.5e-05.
===> Epoch[257](30/324): Loss: 0.7016 || Learning rate: lr=2.5e-05.
===> Epoch[257](40/324): Loss: 0.7618 || Learning rate: lr=2.5e-05.
===> Epoch[257](50/324): Loss: 1.0230 || Learning rate: lr=2.5e-05.
===> Epoch[257](60/324): Loss: 0.4408 || Learning rate: lr=2.5e-05.
===> Epoch[257](70/324): Loss: 0.4912 || Learning rate: lr=2.5e-05.
===> Epoch[257](80/324): Loss: 0.6871 || Learning rate: lr=2.5e-05.
===> Epoch[257](90/324): Loss: 0.7231 || Learning rate: lr=2.5e-05.
===> Epoch[257](100/324): Loss: 0.4948 || Learning rate: lr=2.5e-05.
===> Epoch[257](110/324): Loss: 0.7652 || Learning rate: lr=2.5e-05.
===> Epoch[257](120/324): Loss: 0.6082 || Learning rate: lr=2.5e-05.
===> Epoch[257](130/324): Loss: 0.7291 || Learning rate: lr=2.5e-05.
===> Epoch[257](140/324): Loss: 0.7092 || Learning rate: lr=2.5e-05.
===> Epoch[257](150/324): Loss: 0.5414 || Learning rate: lr=2.5e-05.
===> Epoch[257](160/324): Loss: 0.7693 || Learning rate: lr=2.5e-05.
===> Epoch[257](170/324): Loss: 0.8940 || Learning rate: lr=2.5e-05.
===> Epoch[257](180/324): Loss: 0.4922 || Learning rate: lr=2.5e-05.
===> Epoch[257](190/324): Loss: 0.7717 || Learning rate: lr=2.5e-05.
===> Epoch[257](200/324): Loss: 0.7020 || Learning rate: lr=2.5e-05.
===> Epoch[257](210/324): Loss: 0.6172 || Learning rate: lr=2.5e-05.
===> Epoch[257](220/324): Loss: 0.6207 || Learning rate: lr=2.5e-05.
===> Epoch[257](230/324): Loss: 0.7172 || Learning rate: lr=2.5e-05.
===> Epoch[257](240/324): Loss: 0.4871 || Learning rate: lr=2.5e-05.
===> Epoch[257](250/324): Loss: 0.4723 || Learning rate: lr=2.5e-05.
===> Epoch[257](260/324): Loss: 0.4827 || Learning rate: lr=2.5e-05.
===> Epoch[257](270/324): Loss: 0.5293 || Learning rate: lr=2.5e-05.
===> Epoch[257](280/324): Loss: 0.5475 || Learning rate: lr=2.5e-05.
===> Epoch[257](290/324): Loss: 0.5730 || Learning rate: lr=2.5e-05.
===> Epoch[257](300/324): Loss: 0.8928 || Learning rate: lr=2.5e-05.
===> Epoch[257](310/324): Loss: 0.6334 || Learning rate: lr=2.5e-05.
===> Epoch[257](320/324): Loss: 0.8046 || Learning rate: lr=2.5e-05.
===> Epoch[258](10/324): Loss: 0.7387 || Learning rate: lr=2.5e-05.
===> Epoch[258](20/324): Loss: 0.8263 || Learning rate: lr=2.5e-05.
===> Epoch[258](30/324): Loss: 0.6845 || Learning rate: lr=2.5e-05.
===> Epoch[258](40/324): Loss: 0.6099 || Learning rate: lr=2.5e-05.
===> Epoch[258](50/324): Loss: 0.5101 || Learning rate: lr=2.5e-05.
===> Epoch[258](60/324): Loss: 0.5025 || Learning rate: lr=2.5e-05.
===> Epoch[258](70/324): Loss: 1.2514 || Learning rate: lr=2.5e-05.
===> Epoch[258](80/324): Loss: 0.6987 || Learning rate: lr=2.5e-05.
===> Epoch[258](90/324): Loss: 0.4634 || Learning rate: lr=2.5e-05.
===> Epoch[258](100/324): Loss: 0.6807 || Learning rate: lr=2.5e-05.
===> Epoch[258](110/324): Loss: 0.8405 || Learning rate: lr=2.5e-05.
===> Epoch[258](120/324): Loss: 0.6008 || Learning rate: lr=2.5e-05.
===> Epoch[258](130/324): Loss: 0.7293 || Learning rate: lr=2.5e-05.
===> Epoch[258](140/324): Loss: 0.6224 || Learning rate: lr=2.5e-05.
===> Epoch[258](150/324): Loss: 0.6278 || Learning rate: lr=2.5e-05.
===> Epoch[258](160/324): Loss: 0.6967 || Learning rate: lr=2.5e-05.
===> Epoch[258](170/324): Loss: 0.8903 || Learning rate: lr=2.5e-05.
===> Epoch[258](180/324): Loss: 0.6922 || Learning rate: lr=2.5e-05.
===> Epoch[258](190/324): Loss: 0.9503 || Learning rate: lr=2.5e-05.
===> Epoch[258](200/324): Loss: 0.5516 || Learning rate: lr=2.5e-05.
===> Epoch[258](210/324): Loss: 0.6532 || Learning rate: lr=2.5e-05.
===> Epoch[258](220/324): Loss: 0.5585 || Learning rate: lr=2.5e-05.
===> Epoch[258](230/324): Loss: 0.5192 || Learning rate: lr=2.5e-05.
===> Epoch[258](240/324): Loss: 0.5397 || Learning rate: lr=2.5e-05.
===> Epoch[258](250/324): Loss: 1.0734 || Learning rate: lr=2.5e-05.
===> Epoch[258](260/324): Loss: 0.4973 || Learning rate: lr=2.5e-05.
===> Epoch[258](270/324): Loss: 0.6433 || Learning rate: lr=2.5e-05.
===> Epoch[258](280/324): Loss: 0.7277 || Learning rate: lr=2.5e-05.
===> Epoch[258](290/324): Loss: 0.8402 || Learning rate: lr=2.5e-05.
===> Epoch[258](300/324): Loss: 0.6337 || Learning rate: lr=2.5e-05.
===> Epoch[258](310/324): Loss: 0.7762 || Learning rate: lr=2.5e-05.
===> Epoch[258](320/324): Loss: 0.7005 || Learning rate: lr=2.5e-05.
===> Epoch[259](10/324): Loss: 0.5654 || Learning rate: lr=2.5e-05.
===> Epoch[259](20/324): Loss: 0.6971 || Learning rate: lr=2.5e-05.
===> Epoch[259](30/324): Loss: 0.6505 || Learning rate: lr=2.5e-05.
===> Epoch[259](40/324): Loss: 0.4130 || Learning rate: lr=2.5e-05.
===> Epoch[259](50/324): Loss: 0.7865 || Learning rate: lr=2.5e-05.
===> Epoch[259](60/324): Loss: 0.5203 || Learning rate: lr=2.5e-05.
===> Epoch[259](70/324): Loss: 0.5935 || Learning rate: lr=2.5e-05.
===> Epoch[259](80/324): Loss: 0.9975 || Learning rate: lr=2.5e-05.
===> Epoch[259](90/324): Loss: 0.6827 || Learning rate: lr=2.5e-05.
===> Epoch[259](100/324): Loss: 0.6717 || Learning rate: lr=2.5e-05.
===> Epoch[259](110/324): Loss: 0.7259 || Learning rate: lr=2.5e-05.
===> Epoch[259](120/324): Loss: 0.5697 || Learning rate: lr=2.5e-05.
===> Epoch[259](130/324): Loss: 0.5769 || Learning rate: lr=2.5e-05.
===> Epoch[259](140/324): Loss: 1.0946 || Learning rate: lr=2.5e-05.
===> Epoch[259](150/324): Loss: 0.7139 || Learning rate: lr=2.5e-05.
===> Epoch[259](160/324): Loss: 0.7717 || Learning rate: lr=2.5e-05.
===> Epoch[259](170/324): Loss: 0.6588 || Learning rate: lr=2.5e-05.
===> Epoch[259](180/324): Loss: 0.8914 || Learning rate: lr=2.5e-05.
===> Epoch[259](190/324): Loss: 0.6738 || Learning rate: lr=2.5e-05.
===> Epoch[259](200/324): Loss: 0.5717 || Learning rate: lr=2.5e-05.
===> Epoch[259](210/324): Loss: 0.6250 || Learning rate: lr=2.5e-05.
===> Epoch[259](220/324): Loss: 0.6022 || Learning rate: lr=2.5e-05.
===> Epoch[259](230/324): Loss: 0.8137 || Learning rate: lr=2.5e-05.
===> Epoch[259](240/324): Loss: 0.8978 || Learning rate: lr=2.5e-05.
===> Epoch[259](250/324): Loss: 0.9690 || Learning rate: lr=2.5e-05.
===> Epoch[259](260/324): Loss: 0.6803 || Learning rate: lr=2.5e-05.
===> Epoch[259](270/324): Loss: 0.4789 || Learning rate: lr=2.5e-05.
===> Epoch[259](280/324): Loss: 0.7490 || Learning rate: lr=2.5e-05.
===> Epoch[259](290/324): Loss: 0.5671 || Learning rate: lr=2.5e-05.
===> Epoch[259](300/324): Loss: 0.8433 || Learning rate: lr=2.5e-05.
===> Epoch[259](310/324): Loss: 0.5856 || Learning rate: lr=2.5e-05.
===> Epoch[259](320/324): Loss: 0.9643 || Learning rate: lr=2.5e-05.
===> Epoch[260](10/324): Loss: 0.6046 || Learning rate: lr=2.5e-05.
===> Epoch[260](20/324): Loss: 0.7608 || Learning rate: lr=2.5e-05.
===> Epoch[260](30/324): Loss: 0.5946 || Learning rate: lr=2.5e-05.
===> Epoch[260](40/324): Loss: 0.7180 || Learning rate: lr=2.5e-05.
===> Epoch[260](50/324): Loss: 0.6010 || Learning rate: lr=2.5e-05.
===> Epoch[260](60/324): Loss: 0.4187 || Learning rate: lr=2.5e-05.
===> Epoch[260](70/324): Loss: 0.7677 || Learning rate: lr=2.5e-05.
===> Epoch[260](80/324): Loss: 0.4680 || Learning rate: lr=2.5e-05.
===> Epoch[260](90/324): Loss: 0.8056 || Learning rate: lr=2.5e-05.
===> Epoch[260](100/324): Loss: 0.4632 || Learning rate: lr=2.5e-05.
===> Epoch[260](110/324): Loss: 0.7123 || Learning rate: lr=2.5e-05.
===> Epoch[260](120/324): Loss: 0.8146 || Learning rate: lr=2.5e-05.
===> Epoch[260](130/324): Loss: 0.6452 || Learning rate: lr=2.5e-05.
===> Epoch[260](140/324): Loss: 0.8140 || Learning rate: lr=2.5e-05.
===> Epoch[260](150/324): Loss: 0.7736 || Learning rate: lr=2.5e-05.
===> Epoch[260](160/324): Loss: 0.7638 || Learning rate: lr=2.5e-05.
===> Epoch[260](170/324): Loss: 0.6948 || Learning rate: lr=2.5e-05.
===> Epoch[260](180/324): Loss: 0.5008 || Learning rate: lr=2.5e-05.
===> Epoch[260](190/324): Loss: 0.5785 || Learning rate: lr=2.5e-05.
===> Epoch[260](200/324): Loss: 0.6939 || Learning rate: lr=2.5e-05.
===> Epoch[260](210/324): Loss: 0.7651 || Learning rate: lr=2.5e-05.
===> Epoch[260](220/324): Loss: 0.7533 || Learning rate: lr=2.5e-05.
===> Epoch[260](230/324): Loss: 0.5822 || Learning rate: lr=2.5e-05.
===> Epoch[260](240/324): Loss: 0.8292 || Learning rate: lr=2.5e-05.
===> Epoch[260](250/324): Loss: 0.5979 || Learning rate: lr=2.5e-05.
===> Epoch[260](260/324): Loss: 0.6960 || Learning rate: lr=2.5e-05.
===> Epoch[260](270/324): Loss: 0.6792 || Learning rate: lr=2.5e-05.
===> Epoch[260](280/324): Loss: 0.6098 || Learning rate: lr=2.5e-05.
===> Epoch[260](290/324): Loss: 0.5890 || Learning rate: lr=2.5e-05.
===> Epoch[260](300/324): Loss: 0.5856 || Learning rate: lr=2.5e-05.
===> Epoch[260](310/324): Loss: 0.6671 || Learning rate: lr=2.5e-05.
===> Epoch[260](320/324): Loss: 0.4402 || Learning rate: lr=2.5e-05.
Checkpoint saved to weights/epoch_v2_260.pth
===> Epoch[261](10/324): Loss: 0.6539 || Learning rate: lr=2.5e-05.
===> Epoch[261](20/324): Loss: 0.5729 || Learning rate: lr=2.5e-05.
===> Epoch[261](30/324): Loss: 0.5096 || Learning rate: lr=2.5e-05.
===> Epoch[261](40/324): Loss: 0.8302 || Learning rate: lr=2.5e-05.
===> Epoch[261](50/324): Loss: 0.6929 || Learning rate: lr=2.5e-05.
===> Epoch[261](60/324): Loss: 0.8451 || Learning rate: lr=2.5e-05.
===> Epoch[261](70/324): Loss: 0.5998 || Learning rate: lr=2.5e-05.
===> Epoch[261](80/324): Loss: 0.9554 || Learning rate: lr=2.5e-05.
===> Epoch[261](90/324): Loss: 0.5121 || Learning rate: lr=2.5e-05.
===> Epoch[261](100/324): Loss: 0.6128 || Learning rate: lr=2.5e-05.
===> Epoch[261](110/324): Loss: 0.7034 || Learning rate: lr=2.5e-05.
===> Epoch[261](120/324): Loss: 0.6847 || Learning rate: lr=2.5e-05.
===> Epoch[261](130/324): Loss: 1.0102 || Learning rate: lr=2.5e-05.
===> Epoch[261](140/324): Loss: 0.8328 || Learning rate: lr=2.5e-05.
===> Epoch[261](150/324): Loss: 0.6360 || Learning rate: lr=2.5e-05.
===> Epoch[261](160/324): Loss: 0.5323 || Learning rate: lr=2.5e-05.
===> Epoch[261](170/324): Loss: 0.8405 || Learning rate: lr=2.5e-05.
===> Epoch[261](180/324): Loss: 0.5152 || Learning rate: lr=2.5e-05.
===> Epoch[261](190/324): Loss: 0.5827 || Learning rate: lr=2.5e-05.
===> Epoch[261](200/324): Loss: 0.8034 || Learning rate: lr=2.5e-05.
===> Epoch[261](210/324): Loss: 0.9484 || Learning rate: lr=2.5e-05.
===> Epoch[261](220/324): Loss: 0.7701 || Learning rate: lr=2.5e-05.
===> Epoch[261](230/324): Loss: 0.5981 || Learning rate: lr=2.5e-05.
===> Epoch[261](240/324): Loss: 0.5984 || Learning rate: lr=2.5e-05.
===> Epoch[261](250/324): Loss: 0.6258 || Learning rate: lr=2.5e-05.
===> Epoch[261](260/324): Loss: 0.6407 || Learning rate: lr=2.5e-05.
===> Epoch[261](270/324): Loss: 0.6578 || Learning rate: lr=2.5e-05.
===> Epoch[261](280/324): Loss: 0.6297 || Learning rate: lr=2.5e-05.
===> Epoch[261](290/324): Loss: 1.0400 || Learning rate: lr=2.5e-05.
===> Epoch[261](300/324): Loss: 0.5944 || Learning rate: lr=2.5e-05.
===> Epoch[261](310/324): Loss: 0.8920 || Learning rate: lr=2.5e-05.
===> Epoch[261](320/324): Loss: 0.7304 || Learning rate: lr=2.5e-05.
===> Epoch[262](10/324): Loss: 0.7407 || Learning rate: lr=2.5e-05.
===> Epoch[262](20/324): Loss: 0.7264 || Learning rate: lr=2.5e-05.
===> Epoch[262](30/324): Loss: 0.7435 || Learning rate: lr=2.5e-05.
===> Epoch[262](40/324): Loss: 0.6732 || Learning rate: lr=2.5e-05.
===> Epoch[262](50/324): Loss: 0.8391 || Learning rate: lr=2.5e-05.
===> Epoch[262](60/324): Loss: 0.5972 || Learning rate: lr=2.5e-05.
===> Epoch[262](70/324): Loss: 0.8034 || Learning rate: lr=2.5e-05.
===> Epoch[262](80/324): Loss: 0.5355 || Learning rate: lr=2.5e-05.
===> Epoch[262](90/324): Loss: 0.6484 || Learning rate: lr=2.5e-05.
===> Epoch[262](100/324): Loss: 0.9133 || Learning rate: lr=2.5e-05.
===> Epoch[262](110/324): Loss: 0.8302 || Learning rate: lr=2.5e-05.
===> Epoch[262](120/324): Loss: 0.6155 || Learning rate: lr=2.5e-05.
===> Epoch[262](130/324): Loss: 0.6482 || Learning rate: lr=2.5e-05.
===> Epoch[262](140/324): Loss: 0.5048 || Learning rate: lr=2.5e-05.
===> Epoch[262](150/324): Loss: 0.6328 || Learning rate: lr=2.5e-05.
===> Epoch[262](160/324): Loss: 0.4895 || Learning rate: lr=2.5e-05.
===> Epoch[262](170/324): Loss: 0.7664 || Learning rate: lr=2.5e-05.
===> Epoch[262](180/324): Loss: 0.7601 || Learning rate: lr=2.5e-05.
===> Epoch[262](190/324): Loss: 0.8800 || Learning rate: lr=2.5e-05.
===> Epoch[262](200/324): Loss: 0.7756 || Learning rate: lr=2.5e-05.
===> Epoch[262](210/324): Loss: 0.5551 || Learning rate: lr=2.5e-05.
===> Epoch[262](220/324): Loss: 0.7104 || Learning rate: lr=2.5e-05.
===> Epoch[262](230/324): Loss: 0.6781 || Learning rate: lr=2.5e-05.
===> Epoch[262](240/324): Loss: 0.4187 || Learning rate: lr=2.5e-05.
===> Epoch[262](250/324): Loss: 0.5452 || Learning rate: lr=2.5e-05.
===> Epoch[262](260/324): Loss: 0.5569 || Learning rate: lr=2.5e-05.
===> Epoch[262](270/324): Loss: 0.7007 || Learning rate: lr=2.5e-05.
===> Epoch[262](280/324): Loss: 0.5192 || Learning rate: lr=2.5e-05.
===> Epoch[262](290/324): Loss: 0.7558 || Learning rate: lr=2.5e-05.
===> Epoch[262](300/324): Loss: 0.7575 || Learning rate: lr=2.5e-05.
===> Epoch[262](310/324): Loss: 0.5732 || Learning rate: lr=2.5e-05.
===> Epoch[262](320/324): Loss: 0.6555 || Learning rate: lr=2.5e-05.
===> Epoch[263](10/324): Loss: 0.9519 || Learning rate: lr=2.5e-05.
===> Epoch[263](20/324): Loss: 0.9661 || Learning rate: lr=2.5e-05.
===> Epoch[263](30/324): Loss: 0.7489 || Learning rate: lr=2.5e-05.
===> Epoch[263](40/324): Loss: 0.6246 || Learning rate: lr=2.5e-05.
===> Epoch[263](50/324): Loss: 0.4955 || Learning rate: lr=2.5e-05.
===> Epoch[263](60/324): Loss: 0.4549 || Learning rate: lr=2.5e-05.
===> Epoch[263](70/324): Loss: 0.9893 || Learning rate: lr=2.5e-05.
===> Epoch[263](80/324): Loss: 0.7096 || Learning rate: lr=2.5e-05.
===> Epoch[263](90/324): Loss: 0.5760 || Learning rate: lr=2.5e-05.
===> Epoch[263](100/324): Loss: 0.9981 || Learning rate: lr=2.5e-05.
===> Epoch[263](110/324): Loss: 0.6023 || Learning rate: lr=2.5e-05.
===> Epoch[263](120/324): Loss: 0.8130 || Learning rate: lr=2.5e-05.
===> Epoch[263](130/324): Loss: 0.6608 || Learning rate: lr=2.5e-05.
===> Epoch[263](140/324): Loss: 0.8438 || Learning rate: lr=2.5e-05.
===> Epoch[263](150/324): Loss: 0.4880 || Learning rate: lr=2.5e-05.
===> Epoch[263](160/324): Loss: 0.6450 || Learning rate: lr=2.5e-05.
===> Epoch[263](170/324): Loss: 0.6059 || Learning rate: lr=2.5e-05.
===> Epoch[263](180/324): Loss: 0.7889 || Learning rate: lr=2.5e-05.
===> Epoch[263](190/324): Loss: 0.5533 || Learning rate: lr=2.5e-05.
===> Epoch[263](200/324): Loss: 0.4367 || Learning rate: lr=2.5e-05.
===> Epoch[263](210/324): Loss: 0.7691 || Learning rate: lr=2.5e-05.
===> Epoch[263](220/324): Loss: 1.0474 || Learning rate: lr=2.5e-05.
===> Epoch[263](230/324): Loss: 0.9524 || Learning rate: lr=2.5e-05.
===> Epoch[263](240/324): Loss: 0.5662 || Learning rate: lr=2.5e-05.
===> Epoch[263](250/324): Loss: 0.4985 || Learning rate: lr=2.5e-05.
===> Epoch[263](260/324): Loss: 0.8874 || Learning rate: lr=2.5e-05.
===> Epoch[263](270/324): Loss: 0.8151 || Learning rate: lr=2.5e-05.
===> Epoch[263](280/324): Loss: 0.5692 || Learning rate: lr=2.5e-05.
===> Epoch[263](290/324): Loss: 0.7412 || Learning rate: lr=2.5e-05.
===> Epoch[263](300/324): Loss: 0.5896 || Learning rate: lr=2.5e-05.
===> Epoch[263](310/324): Loss: 0.6195 || Learning rate: lr=2.5e-05.
===> Epoch[263](320/324): Loss: 0.5474 || Learning rate: lr=2.5e-05.
===> Epoch[264](10/324): Loss: 0.7339 || Learning rate: lr=2.5e-05.
===> Epoch[264](20/324): Loss: 0.7371 || Learning rate: lr=2.5e-05.
===> Epoch[264](30/324): Loss: 0.9985 || Learning rate: lr=2.5e-05.
===> Epoch[264](40/324): Loss: 0.7962 || Learning rate: lr=2.5e-05.
===> Epoch[264](50/324): Loss: 0.6777 || Learning rate: lr=2.5e-05.
===> Epoch[264](60/324): Loss: 0.4883 || Learning rate: lr=2.5e-05.
===> Epoch[264](70/324): Loss: 0.8084 || Learning rate: lr=2.5e-05.
===> Epoch[264](80/324): Loss: 1.0103 || Learning rate: lr=2.5e-05.
===> Epoch[264](90/324): Loss: 0.6081 || Learning rate: lr=2.5e-05.
===> Epoch[264](100/324): Loss: 0.8868 || Learning rate: lr=2.5e-05.
===> Epoch[264](110/324): Loss: 0.7487 || Learning rate: lr=2.5e-05.
===> Epoch[264](120/324): Loss: 0.7435 || Learning rate: lr=2.5e-05.
===> Epoch[264](130/324): Loss: 0.7440 || Learning rate: lr=2.5e-05.
===> Epoch[264](140/324): Loss: 0.5838 || Learning rate: lr=2.5e-05.
===> Epoch[264](150/324): Loss: 0.6023 || Learning rate: lr=2.5e-05.
===> Epoch[264](160/324): Loss: 0.5513 || Learning rate: lr=2.5e-05.
===> Epoch[264](170/324): Loss: 0.7024 || Learning rate: lr=2.5e-05.
===> Epoch[264](180/324): Loss: 0.4323 || Learning rate: lr=2.5e-05.
===> Epoch[264](190/324): Loss: 0.5545 || Learning rate: lr=2.5e-05.
===> Epoch[264](200/324): Loss: 0.8706 || Learning rate: lr=2.5e-05.
===> Epoch[264](210/324): Loss: 0.8580 || Learning rate: lr=2.5e-05.
===> Epoch[264](220/324): Loss: 0.7810 || Learning rate: lr=2.5e-05.
===> Epoch[264](230/324): Loss: 0.6826 || Learning rate: lr=2.5e-05.
===> Epoch[264](240/324): Loss: 0.4061 || Learning rate: lr=2.5e-05.
===> Epoch[264](250/324): Loss: 0.5893 || Learning rate: lr=2.5e-05.
===> Epoch[264](260/324): Loss: 0.6489 || Learning rate: lr=2.5e-05.
===> Epoch[264](270/324): Loss: 0.5073 || Learning rate: lr=2.5e-05.
===> Epoch[264](280/324): Loss: 0.5811 || Learning rate: lr=2.5e-05.
===> Epoch[264](290/324): Loss: 0.8069 || Learning rate: lr=2.5e-05.
===> Epoch[264](300/324): Loss: 0.9490 || Learning rate: lr=2.5e-05.
===> Epoch[264](310/324): Loss: 0.6601 || Learning rate: lr=2.5e-05.
===> Epoch[264](320/324): Loss: 0.6246 || Learning rate: lr=2.5e-05.
===> Epoch[265](10/324): Loss: 0.4130 || Learning rate: lr=2.5e-05.
===> Epoch[265](20/324): Loss: 0.6051 || Learning rate: lr=2.5e-05.
===> Epoch[265](30/324): Loss: 0.8290 || Learning rate: lr=2.5e-05.
===> Epoch[265](40/324): Loss: 0.7383 || Learning rate: lr=2.5e-05.
===> Epoch[265](50/324): Loss: 0.7280 || Learning rate: lr=2.5e-05.
===> Epoch[265](60/324): Loss: 0.7039 || Learning rate: lr=2.5e-05.
===> Epoch[265](70/324): Loss: 0.6027 || Learning rate: lr=2.5e-05.
===> Epoch[265](80/324): Loss: 0.6609 || Learning rate: lr=2.5e-05.
===> Epoch[265](90/324): Loss: 0.5954 || Learning rate: lr=2.5e-05.
===> Epoch[265](100/324): Loss: 0.7901 || Learning rate: lr=2.5e-05.
===> Epoch[265](110/324): Loss: 0.5303 || Learning rate: lr=2.5e-05.
===> Epoch[265](120/324): Loss: 0.9184 || Learning rate: lr=2.5e-05.
===> Epoch[265](130/324): Loss: 0.9301 || Learning rate: lr=2.5e-05.
===> Epoch[265](140/324): Loss: 0.6492 || Learning rate: lr=2.5e-05.
===> Epoch[265](150/324): Loss: 0.6567 || Learning rate: lr=2.5e-05.
===> Epoch[265](160/324): Loss: 0.6950 || Learning rate: lr=2.5e-05.
===> Epoch[265](170/324): Loss: 0.6799 || Learning rate: lr=2.5e-05.
===> Epoch[265](180/324): Loss: 0.3858 || Learning rate: lr=2.5e-05.
===> Epoch[265](190/324): Loss: 0.8232 || Learning rate: lr=2.5e-05.
===> Epoch[265](200/324): Loss: 0.4594 || Learning rate: lr=2.5e-05.
===> Epoch[265](210/324): Loss: 0.5944 || Learning rate: lr=2.5e-05.
===> Epoch[265](220/324): Loss: 0.8227 || Learning rate: lr=2.5e-05.
===> Epoch[265](230/324): Loss: 0.7062 || Learning rate: lr=2.5e-05.
===> Epoch[265](240/324): Loss: 0.9792 || Learning rate: lr=2.5e-05.
===> Epoch[265](250/324): Loss: 0.5657 || Learning rate: lr=2.5e-05.
===> Epoch[265](260/324): Loss: 0.6123 || Learning rate: lr=2.5e-05.
===> Epoch[265](270/324): Loss: 0.5649 || Learning rate: lr=2.5e-05.
===> Epoch[265](280/324): Loss: 0.6324 || Learning rate: lr=2.5e-05.
===> Epoch[265](290/324): Loss: 0.7890 || Learning rate: lr=2.5e-05.
===> Epoch[265](300/324): Loss: 0.7577 || Learning rate: lr=2.5e-05.
===> Epoch[265](310/324): Loss: 0.5639 || Learning rate: lr=2.5e-05.
===> Epoch[265](320/324): Loss: 0.6742 || Learning rate: lr=2.5e-05.
===> Epoch[266](10/324): Loss: 0.8018 || Learning rate: lr=2.5e-05.
===> Epoch[266](20/324): Loss: 0.5234 || Learning rate: lr=2.5e-05.
===> Epoch[266](30/324): Loss: 0.6974 || Learning rate: lr=2.5e-05.
===> Epoch[266](40/324): Loss: 0.5997 || Learning rate: lr=2.5e-05.
===> Epoch[266](50/324): Loss: 0.7390 || Learning rate: lr=2.5e-05.
===> Epoch[266](60/324): Loss: 0.8158 || Learning rate: lr=2.5e-05.
===> Epoch[266](70/324): Loss: 0.6493 || Learning rate: lr=2.5e-05.
===> Epoch[266](80/324): Loss: 0.6203 || Learning rate: lr=2.5e-05.
===> Epoch[266](90/324): Loss: 0.4585 || Learning rate: lr=2.5e-05.
===> Epoch[266](100/324): Loss: 0.6974 || Learning rate: lr=2.5e-05.
===> Epoch[266](110/324): Loss: 0.6244 || Learning rate: lr=2.5e-05.
===> Epoch[266](120/324): Loss: 0.5889 || Learning rate: lr=2.5e-05.
===> Epoch[266](130/324): Loss: 0.5980 || Learning rate: lr=2.5e-05.
===> Epoch[266](140/324): Loss: 0.7677 || Learning rate: lr=2.5e-05.
===> Epoch[266](150/324): Loss: 0.7788 || Learning rate: lr=2.5e-05.
===> Epoch[266](160/324): Loss: 0.7263 || Learning rate: lr=2.5e-05.
===> Epoch[266](170/324): Loss: 0.7307 || Learning rate: lr=2.5e-05.
===> Epoch[266](180/324): Loss: 0.7556 || Learning rate: lr=2.5e-05.
===> Epoch[266](190/324): Loss: 0.6142 || Learning rate: lr=2.5e-05.
===> Epoch[266](200/324): Loss: 0.8856 || Learning rate: lr=2.5e-05.
===> Epoch[266](210/324): Loss: 0.6969 || Learning rate: lr=2.5e-05.
===> Epoch[266](220/324): Loss: 0.8860 || Learning rate: lr=2.5e-05.
===> Epoch[266](230/324): Loss: 0.8223 || Learning rate: lr=2.5e-05.
===> Epoch[266](240/324): Loss: 0.7312 || Learning rate: lr=2.5e-05.
===> Epoch[266](250/324): Loss: 0.7585 || Learning rate: lr=2.5e-05.
===> Epoch[266](260/324): Loss: 0.7022 || Learning rate: lr=2.5e-05.
===> Epoch[266](270/324): Loss: 0.9181 || Learning rate: lr=2.5e-05.
===> Epoch[266](280/324): Loss: 0.7035 || Learning rate: lr=2.5e-05.
===> Epoch[266](290/324): Loss: 0.5974 || Learning rate: lr=2.5e-05.
===> Epoch[266](300/324): Loss: 0.6483 || Learning rate: lr=2.5e-05.
===> Epoch[266](310/324): Loss: 0.8170 || Learning rate: lr=2.5e-05.
===> Epoch[266](320/324): Loss: 0.9057 || Learning rate: lr=2.5e-05.
===> Epoch[267](10/324): Loss: 1.0408 || Learning rate: lr=2.5e-05.
===> Epoch[267](20/324): Loss: 0.4377 || Learning rate: lr=2.5e-05.
===> Epoch[267](30/324): Loss: 0.7608 || Learning rate: lr=2.5e-05.
===> Epoch[267](40/324): Loss: 0.4954 || Learning rate: lr=2.5e-05.
===> Epoch[267](50/324): Loss: 0.6425 || Learning rate: lr=2.5e-05.
===> Epoch[267](60/324): Loss: 0.8490 || Learning rate: lr=2.5e-05.
===> Epoch[267](70/324): Loss: 0.5610 || Learning rate: lr=2.5e-05.
===> Epoch[267](80/324): Loss: 0.5928 || Learning rate: lr=2.5e-05.
===> Epoch[267](90/324): Loss: 0.4907 || Learning rate: lr=2.5e-05.
===> Epoch[267](100/324): Loss: 0.8961 || Learning rate: lr=2.5e-05.
===> Epoch[267](110/324): Loss: 0.4219 || Learning rate: lr=2.5e-05.
===> Epoch[267](120/324): Loss: 1.0509 || Learning rate: lr=2.5e-05.
===> Epoch[267](130/324): Loss: 1.0358 || Learning rate: lr=2.5e-05.
===> Epoch[267](140/324): Loss: 0.6489 || Learning rate: lr=2.5e-05.
===> Epoch[267](150/324): Loss: 0.9609 || Learning rate: lr=2.5e-05.
===> Epoch[267](160/324): Loss: 0.5039 || Learning rate: lr=2.5e-05.
===> Epoch[267](170/324): Loss: 0.5434 || Learning rate: lr=2.5e-05.
===> Epoch[267](180/324): Loss: 0.6074 || Learning rate: lr=2.5e-05.
===> Epoch[267](190/324): Loss: 0.6834 || Learning rate: lr=2.5e-05.
===> Epoch[267](200/324): Loss: 0.4567 || Learning rate: lr=2.5e-05.
===> Epoch[267](210/324): Loss: 0.6389 || Learning rate: lr=2.5e-05.
===> Epoch[267](220/324): Loss: 0.8602 || Learning rate: lr=2.5e-05.
===> Epoch[267](230/324): Loss: 0.9810 || Learning rate: lr=2.5e-05.
===> Epoch[267](240/324): Loss: 0.6769 || Learning rate: lr=2.5e-05.
===> Epoch[267](250/324): Loss: 0.5264 || Learning rate: lr=2.5e-05.
===> Epoch[267](260/324): Loss: 0.4561 || Learning rate: lr=2.5e-05.
===> Epoch[267](270/324): Loss: 0.6229 || Learning rate: lr=2.5e-05.
===> Epoch[267](280/324): Loss: 0.5287 || Learning rate: lr=2.5e-05.
===> Epoch[267](290/324): Loss: 0.6098 || Learning rate: lr=2.5e-05.
===> Epoch[267](300/324): Loss: 0.6545 || Learning rate: lr=2.5e-05.
===> Epoch[267](310/324): Loss: 0.4478 || Learning rate: lr=2.5e-05.
===> Epoch[267](320/324): Loss: 0.6849 || Learning rate: lr=2.5e-05.
===> Epoch[268](10/324): Loss: 0.6653 || Learning rate: lr=2.5e-05.
===> Epoch[268](20/324): Loss: 0.6109 || Learning rate: lr=2.5e-05.
===> Epoch[268](30/324): Loss: 0.7061 || Learning rate: lr=2.5e-05.
===> Epoch[268](40/324): Loss: 0.9758 || Learning rate: lr=2.5e-05.
===> Epoch[268](50/324): Loss: 0.8886 || Learning rate: lr=2.5e-05.
===> Epoch[268](60/324): Loss: 0.7557 || Learning rate: lr=2.5e-05.
===> Epoch[268](70/324): Loss: 0.6979 || Learning rate: lr=2.5e-05.
===> Epoch[268](80/324): Loss: 0.6451 || Learning rate: lr=2.5e-05.
===> Epoch[268](90/324): Loss: 0.8342 || Learning rate: lr=2.5e-05.
===> Epoch[268](100/324): Loss: 0.8164 || Learning rate: lr=2.5e-05.
===> Epoch[268](110/324): Loss: 0.5758 || Learning rate: lr=2.5e-05.
===> Epoch[268](120/324): Loss: 0.9278 || Learning rate: lr=2.5e-05.
===> Epoch[268](130/324): Loss: 0.5457 || Learning rate: lr=2.5e-05.
===> Epoch[268](140/324): Loss: 0.6578 || Learning rate: lr=2.5e-05.
===> Epoch[268](150/324): Loss: 1.2033 || Learning rate: lr=2.5e-05.
===> Epoch[268](160/324): Loss: 0.6742 || Learning rate: lr=2.5e-05.
===> Epoch[268](170/324): Loss: 0.7069 || Learning rate: lr=2.5e-05.
===> Epoch[268](180/324): Loss: 0.5155 || Learning rate: lr=2.5e-05.
===> Epoch[268](190/324): Loss: 0.3839 || Learning rate: lr=2.5e-05.
===> Epoch[268](200/324): Loss: 0.8137 || Learning rate: lr=2.5e-05.
===> Epoch[268](210/324): Loss: 0.9204 || Learning rate: lr=2.5e-05.
===> Epoch[268](220/324): Loss: 0.8594 || Learning rate: lr=2.5e-05.
===> Epoch[268](230/324): Loss: 0.8890 || Learning rate: lr=2.5e-05.
===> Epoch[268](240/324): Loss: 0.4848 || Learning rate: lr=2.5e-05.
===> Epoch[268](250/324): Loss: 0.5851 || Learning rate: lr=2.5e-05.
===> Epoch[268](260/324): Loss: 0.6349 || Learning rate: lr=2.5e-05.
===> Epoch[268](270/324): Loss: 0.7329 || Learning rate: lr=2.5e-05.
===> Epoch[268](280/324): Loss: 0.4362 || Learning rate: lr=2.5e-05.
===> Epoch[268](290/324): Loss: 0.8339 || Learning rate: lr=2.5e-05.
===> Epoch[268](300/324): Loss: 0.5131 || Learning rate: lr=2.5e-05.
===> Epoch[268](310/324): Loss: 0.6243 || Learning rate: lr=2.5e-05.
===> Epoch[268](320/324): Loss: 0.6989 || Learning rate: lr=2.5e-05.
===> Epoch[269](10/324): Loss: 0.3585 || Learning rate: lr=2.5e-05.
===> Epoch[269](20/324): Loss: 0.6384 || Learning rate: lr=2.5e-05.
===> Epoch[269](30/324): Loss: 0.8031 || Learning rate: lr=2.5e-05.
===> Epoch[269](40/324): Loss: 0.7921 || Learning rate: lr=2.5e-05.
===> Epoch[269](50/324): Loss: 0.5540 || Learning rate: lr=2.5e-05.
===> Epoch[269](60/324): Loss: 0.5191 || Learning rate: lr=2.5e-05.
===> Epoch[269](70/324): Loss: 0.7095 || Learning rate: lr=2.5e-05.
===> Epoch[269](80/324): Loss: 0.5566 || Learning rate: lr=2.5e-05.
===> Epoch[269](90/324): Loss: 0.9559 || Learning rate: lr=2.5e-05.
===> Epoch[269](100/324): Loss: 0.5898 || Learning rate: lr=2.5e-05.
===> Epoch[269](110/324): Loss: 0.9228 || Learning rate: lr=2.5e-05.
===> Epoch[269](120/324): Loss: 0.7916 || Learning rate: lr=2.5e-05.
===> Epoch[269](130/324): Loss: 0.5423 || Learning rate: lr=2.5e-05.
===> Epoch[269](140/324): Loss: 0.5265 || Learning rate: lr=2.5e-05.
===> Epoch[269](150/324): Loss: 0.9264 || Learning rate: lr=2.5e-05.
===> Epoch[269](160/324): Loss: 0.6774 || Learning rate: lr=2.5e-05.
===> Epoch[269](170/324): Loss: 0.6053 || Learning rate: lr=2.5e-05.
===> Epoch[269](180/324): Loss: 0.6322 || Learning rate: lr=2.5e-05.
===> Epoch[269](190/324): Loss: 0.8852 || Learning rate: lr=2.5e-05.
===> Epoch[269](200/324): Loss: 0.8165 || Learning rate: lr=2.5e-05.
===> Epoch[269](210/324): Loss: 0.9832 || Learning rate: lr=2.5e-05.
===> Epoch[269](220/324): Loss: 0.7999 || Learning rate: lr=2.5e-05.
===> Epoch[269](230/324): Loss: 0.4257 || Learning rate: lr=2.5e-05.
===> Epoch[269](240/324): Loss: 0.7257 || Learning rate: lr=2.5e-05.
===> Epoch[269](250/324): Loss: 0.5836 || Learning rate: lr=2.5e-05.
===> Epoch[269](260/324): Loss: 0.8578 || Learning rate: lr=2.5e-05.
===> Epoch[269](270/324): Loss: 0.7038 || Learning rate: lr=2.5e-05.
===> Epoch[269](280/324): Loss: 0.5548 || Learning rate: lr=2.5e-05.
===> Epoch[269](290/324): Loss: 0.5701 || Learning rate: lr=2.5e-05.
===> Epoch[269](300/324): Loss: 0.9115 || Learning rate: lr=2.5e-05.
===> Epoch[269](310/324): Loss: 0.6301 || Learning rate: lr=2.5e-05.
===> Epoch[269](320/324): Loss: 0.7984 || Learning rate: lr=2.5e-05.
===> Epoch[270](10/324): Loss: 0.8258 || Learning rate: lr=2.5e-05.
===> Epoch[270](20/324): Loss: 0.7292 || Learning rate: lr=2.5e-05.
===> Epoch[270](30/324): Loss: 0.6216 || Learning rate: lr=2.5e-05.
===> Epoch[270](40/324): Loss: 0.5461 || Learning rate: lr=2.5e-05.
===> Epoch[270](50/324): Loss: 0.5947 || Learning rate: lr=2.5e-05.
===> Epoch[270](60/324): Loss: 0.6394 || Learning rate: lr=2.5e-05.
===> Epoch[270](70/324): Loss: 0.6050 || Learning rate: lr=2.5e-05.
===> Epoch[270](80/324): Loss: 0.7594 || Learning rate: lr=2.5e-05.
===> Epoch[270](90/324): Loss: 0.8247 || Learning rate: lr=2.5e-05.
===> Epoch[270](100/324): Loss: 0.5113 || Learning rate: lr=2.5e-05.
===> Epoch[270](110/324): Loss: 0.5464 || Learning rate: lr=2.5e-05.
===> Epoch[270](120/324): Loss: 0.6765 || Learning rate: lr=2.5e-05.
===> Epoch[270](130/324): Loss: 0.6852 || Learning rate: lr=2.5e-05.
===> Epoch[270](140/324): Loss: 0.4668 || Learning rate: lr=2.5e-05.
===> Epoch[270](150/324): Loss: 0.8218 || Learning rate: lr=2.5e-05.
===> Epoch[270](160/324): Loss: 0.6147 || Learning rate: lr=2.5e-05.
===> Epoch[270](170/324): Loss: 0.9305 || Learning rate: lr=2.5e-05.
===> Epoch[270](180/324): Loss: 0.4947 || Learning rate: lr=2.5e-05.
===> Epoch[270](190/324): Loss: 0.5292 || Learning rate: lr=2.5e-05.
===> Epoch[270](200/324): Loss: 0.5741 || Learning rate: lr=2.5e-05.
===> Epoch[270](210/324): Loss: 0.6412 || Learning rate: lr=2.5e-05.
===> Epoch[270](220/324): Loss: 0.6364 || Learning rate: lr=2.5e-05.
===> Epoch[270](230/324): Loss: 0.6106 || Learning rate: lr=2.5e-05.
===> Epoch[270](240/324): Loss: 0.7231 || Learning rate: lr=2.5e-05.
===> Epoch[270](250/324): Loss: 0.6835 || Learning rate: lr=2.5e-05.
===> Epoch[270](260/324): Loss: 0.6907 || Learning rate: lr=2.5e-05.
===> Epoch[270](270/324): Loss: 0.7689 || Learning rate: lr=2.5e-05.
===> Epoch[270](280/324): Loss: 0.7833 || Learning rate: lr=2.5e-05.
===> Epoch[270](290/324): Loss: 0.5403 || Learning rate: lr=2.5e-05.
===> Epoch[270](300/324): Loss: 0.8036 || Learning rate: lr=2.5e-05.
===> Epoch[270](310/324): Loss: 0.6483 || Learning rate: lr=2.5e-05.
===> Epoch[270](320/324): Loss: 0.8071 || Learning rate: lr=2.5e-05.
===> Epoch[271](10/324): Loss: 0.7379 || Learning rate: lr=2.5e-05.
===> Epoch[271](20/324): Loss: 0.7141 || Learning rate: lr=2.5e-05.
===> Epoch[271](30/324): Loss: 0.7947 || Learning rate: lr=2.5e-05.
===> Epoch[271](40/324): Loss: 0.7045 || Learning rate: lr=2.5e-05.
===> Epoch[271](50/324): Loss: 0.6110 || Learning rate: lr=2.5e-05.
===> Epoch[271](60/324): Loss: 0.7155 || Learning rate: lr=2.5e-05.
===> Epoch[271](70/324): Loss: 0.7382 || Learning rate: lr=2.5e-05.
===> Epoch[271](80/324): Loss: 0.5164 || Learning rate: lr=2.5e-05.
===> Epoch[271](90/324): Loss: 0.8193 || Learning rate: lr=2.5e-05.
===> Epoch[271](100/324): Loss: 0.7926 || Learning rate: lr=2.5e-05.
===> Epoch[271](110/324): Loss: 0.8425 || Learning rate: lr=2.5e-05.
===> Epoch[271](120/324): Loss: 0.8476 || Learning rate: lr=2.5e-05.
===> Epoch[271](130/324): Loss: 0.4846 || Learning rate: lr=2.5e-05.
===> Epoch[271](140/324): Loss: 0.6569 || Learning rate: lr=2.5e-05.
===> Epoch[271](150/324): Loss: 0.6473 || Learning rate: lr=2.5e-05.
===> Epoch[271](160/324): Loss: 0.5405 || Learning rate: lr=2.5e-05.
===> Epoch[271](170/324): Loss: 0.5583 || Learning rate: lr=2.5e-05.
===> Epoch[271](180/324): Loss: 0.7947 || Learning rate: lr=2.5e-05.
===> Epoch[271](190/324): Loss: 0.7304 || Learning rate: lr=2.5e-05.
===> Epoch[271](200/324): Loss: 0.7796 || Learning rate: lr=2.5e-05.
===> Epoch[271](210/324): Loss: 0.8138 || Learning rate: lr=2.5e-05.
===> Epoch[271](220/324): Loss: 0.7844 || Learning rate: lr=2.5e-05.
===> Epoch[271](230/324): Loss: 0.9056 || Learning rate: lr=2.5e-05.
===> Epoch[271](240/324): Loss: 0.5387 || Learning rate: lr=2.5e-05.
===> Epoch[271](250/324): Loss: 0.5926 || Learning rate: lr=2.5e-05.
===> Epoch[271](260/324): Loss: 0.5332 || Learning rate: lr=2.5e-05.
===> Epoch[271](270/324): Loss: 0.7350 || Learning rate: lr=2.5e-05.
===> Epoch[271](280/324): Loss: 0.7653 || Learning rate: lr=2.5e-05.
===> Epoch[271](290/324): Loss: 0.7956 || Learning rate: lr=2.5e-05.
===> Epoch[271](300/324): Loss: 0.5734 || Learning rate: lr=2.5e-05.
===> Epoch[271](310/324): Loss: 0.8519 || Learning rate: lr=2.5e-05.
===> Epoch[271](320/324): Loss: 0.5039 || Learning rate: lr=2.5e-05.
===> Epoch[272](10/324): Loss: 0.6479 || Learning rate: lr=2.5e-05.
===> Epoch[272](20/324): Loss: 0.9307 || Learning rate: lr=2.5e-05.
===> Epoch[272](30/324): Loss: 0.6813 || Learning rate: lr=2.5e-05.
===> Epoch[272](40/324): Loss: 0.7310 || Learning rate: lr=2.5e-05.
===> Epoch[272](50/324): Loss: 0.5645 || Learning rate: lr=2.5e-05.
===> Epoch[272](60/324): Loss: 0.8504 || Learning rate: lr=2.5e-05.
===> Epoch[272](70/324): Loss: 0.5912 || Learning rate: lr=2.5e-05.
===> Epoch[272](80/324): Loss: 0.7782 || Learning rate: lr=2.5e-05.
===> Epoch[272](90/324): Loss: 0.9483 || Learning rate: lr=2.5e-05.
===> Epoch[272](100/324): Loss: 0.8602 || Learning rate: lr=2.5e-05.
===> Epoch[272](110/324): Loss: 0.5927 || Learning rate: lr=2.5e-05.
===> Epoch[272](120/324): Loss: 0.5714 || Learning rate: lr=2.5e-05.
===> Epoch[272](130/324): Loss: 0.7859 || Learning rate: lr=2.5e-05.
===> Epoch[272](140/324): Loss: 0.5014 || Learning rate: lr=2.5e-05.
===> Epoch[272](150/324): Loss: 0.7606 || Learning rate: lr=2.5e-05.
===> Epoch[272](160/324): Loss: 0.4720 || Learning rate: lr=2.5e-05.
===> Epoch[272](170/324): Loss: 0.7508 || Learning rate: lr=2.5e-05.
===> Epoch[272](180/324): Loss: 0.7651 || Learning rate: lr=2.5e-05.
===> Epoch[272](190/324): Loss: 0.7373 || Learning rate: lr=2.5e-05.
===> Epoch[272](200/324): Loss: 0.6368 || Learning rate: lr=2.5e-05.
===> Epoch[272](210/324): Loss: 0.5679 || Learning rate: lr=2.5e-05.
===> Epoch[272](220/324): Loss: 0.5607 || Learning rate: lr=2.5e-05.
===> Epoch[272](230/324): Loss: 0.7692 || Learning rate: lr=2.5e-05.
===> Epoch[272](240/324): Loss: 0.7015 || Learning rate: lr=2.5e-05.
===> Epoch[272](250/324): Loss: 0.6003 || Learning rate: lr=2.5e-05.
===> Epoch[272](260/324): Loss: 0.8909 || Learning rate: lr=2.5e-05.
===> Epoch[272](270/324): Loss: 0.7091 || Learning rate: lr=2.5e-05.
===> Epoch[272](280/324): Loss: 0.6808 || Learning rate: lr=2.5e-05.
===> Epoch[272](290/324): Loss: 0.5229 || Learning rate: lr=2.5e-05.
===> Epoch[272](300/324): Loss: 0.7863 || Learning rate: lr=2.5e-05.
===> Epoch[272](310/324): Loss: 0.7422 || Learning rate: lr=2.5e-05.
===> Epoch[272](320/324): Loss: 0.7188 || Learning rate: lr=2.5e-05.
===> Epoch[273](10/324): Loss: 0.6690 || Learning rate: lr=2.5e-05.
===> Epoch[273](20/324): Loss: 0.6827 || Learning rate: lr=2.5e-05.
===> Epoch[273](30/324): Loss: 0.6317 || Learning rate: lr=2.5e-05.
===> Epoch[273](40/324): Loss: 0.5897 || Learning rate: lr=2.5e-05.
===> Epoch[273](50/324): Loss: 0.6684 || Learning rate: lr=2.5e-05.
===> Epoch[273](60/324): Loss: 0.6158 || Learning rate: lr=2.5e-05.
===> Epoch[273](70/324): Loss: 0.7991 || Learning rate: lr=2.5e-05.
===> Epoch[273](80/324): Loss: 0.7517 || Learning rate: lr=2.5e-05.
===> Epoch[273](90/324): Loss: 0.6414 || Learning rate: lr=2.5e-05.
===> Epoch[273](100/324): Loss: 0.7475 || Learning rate: lr=2.5e-05.
===> Epoch[273](110/324): Loss: 0.7629 || Learning rate: lr=2.5e-05.
===> Epoch[273](120/324): Loss: 0.6845 || Learning rate: lr=2.5e-05.
===> Epoch[273](130/324): Loss: 0.4561 || Learning rate: lr=2.5e-05.
===> Epoch[273](140/324): Loss: 0.7027 || Learning rate: lr=2.5e-05.
===> Epoch[273](150/324): Loss: 0.4773 || Learning rate: lr=2.5e-05.
===> Epoch[273](160/324): Loss: 0.5572 || Learning rate: lr=2.5e-05.
===> Epoch[273](170/324): Loss: 0.9940 || Learning rate: lr=2.5e-05.
===> Epoch[273](180/324): Loss: 0.8850 || Learning rate: lr=2.5e-05.
===> Epoch[273](190/324): Loss: 0.9579 || Learning rate: lr=2.5e-05.
===> Epoch[273](200/324): Loss: 0.6123 || Learning rate: lr=2.5e-05.
===> Epoch[273](210/324): Loss: 0.5342 || Learning rate: lr=2.5e-05.
===> Epoch[273](220/324): Loss: 0.9559 || Learning rate: lr=2.5e-05.
===> Epoch[273](230/324): Loss: 0.7394 || Learning rate: lr=2.5e-05.
===> Epoch[273](240/324): Loss: 0.7329 || Learning rate: lr=2.5e-05.
===> Epoch[273](250/324): Loss: 0.5827 || Learning rate: lr=2.5e-05.
===> Epoch[273](260/324): Loss: 0.6539 || Learning rate: lr=2.5e-05.
===> Epoch[273](270/324): Loss: 0.6153 || Learning rate: lr=2.5e-05.
===> Epoch[273](280/324): Loss: 0.5790 || Learning rate: lr=2.5e-05.
===> Epoch[273](290/324): Loss: 0.8906 || Learning rate: lr=2.5e-05.
===> Epoch[273](300/324): Loss: 0.6590 || Learning rate: lr=2.5e-05.
===> Epoch[273](310/324): Loss: 0.7329 || Learning rate: lr=2.5e-05.
===> Epoch[273](320/324): Loss: 0.7024 || Learning rate: lr=2.5e-05.
===> Epoch[274](10/324): Loss: 1.0560 || Learning rate: lr=2.5e-05.
===> Epoch[274](20/324): Loss: 0.7831 || Learning rate: lr=2.5e-05.
===> Epoch[274](30/324): Loss: 0.6269 || Learning rate: lr=2.5e-05.
===> Epoch[274](40/324): Loss: 0.9248 || Learning rate: lr=2.5e-05.
===> Epoch[274](50/324): Loss: 0.6226 || Learning rate: lr=2.5e-05.
===> Epoch[274](60/324): Loss: 0.6779 || Learning rate: lr=2.5e-05.
===> Epoch[274](70/324): Loss: 0.7268 || Learning rate: lr=2.5e-05.
===> Epoch[274](80/324): Loss: 0.6793 || Learning rate: lr=2.5e-05.
===> Epoch[274](90/324): Loss: 0.8455 || Learning rate: lr=2.5e-05.
===> Epoch[274](100/324): Loss: 0.5510 || Learning rate: lr=2.5e-05.
===> Epoch[274](110/324): Loss: 0.4771 || Learning rate: lr=2.5e-05.
===> Epoch[274](120/324): Loss: 0.6492 || Learning rate: lr=2.5e-05.
===> Epoch[274](130/324): Loss: 0.4759 || Learning rate: lr=2.5e-05.
===> Epoch[274](140/324): Loss: 0.6118 || Learning rate: lr=2.5e-05.
===> Epoch[274](150/324): Loss: 0.6055 || Learning rate: lr=2.5e-05.
===> Epoch[274](160/324): Loss: 0.7419 || Learning rate: lr=2.5e-05.
===> Epoch[274](170/324): Loss: 0.7022 || Learning rate: lr=2.5e-05.
===> Epoch[274](180/324): Loss: 0.9575 || Learning rate: lr=2.5e-05.
===> Epoch[274](190/324): Loss: 0.6937 || Learning rate: lr=2.5e-05.
===> Epoch[274](200/324): Loss: 0.8282 || Learning rate: lr=2.5e-05.
===> Epoch[274](210/324): Loss: 0.5786 || Learning rate: lr=2.5e-05.
===> Epoch[274](220/324): Loss: 0.7662 || Learning rate: lr=2.5e-05.
===> Epoch[274](230/324): Loss: 0.7286 || Learning rate: lr=2.5e-05.
===> Epoch[274](240/324): Loss: 0.8164 || Learning rate: lr=2.5e-05.
===> Epoch[274](250/324): Loss: 0.7040 || Learning rate: lr=2.5e-05.
===> Epoch[274](260/324): Loss: 0.5802 || Learning rate: lr=2.5e-05.
===> Epoch[274](270/324): Loss: 0.4859 || Learning rate: lr=2.5e-05.
===> Epoch[274](280/324): Loss: 0.6764 || Learning rate: lr=2.5e-05.
===> Epoch[274](290/324): Loss: 0.7583 || Learning rate: lr=2.5e-05.
===> Epoch[274](300/324): Loss: 0.7327 || Learning rate: lr=2.5e-05.
===> Epoch[274](310/324): Loss: 0.4759 || Learning rate: lr=2.5e-05.
===> Epoch[274](320/324): Loss: 0.6882 || Learning rate: lr=2.5e-05.
===> Epoch[275](10/324): Loss: 1.0514 || Learning rate: lr=2.5e-05.
===> Epoch[275](20/324): Loss: 0.4935 || Learning rate: lr=2.5e-05.
===> Epoch[275](30/324): Loss: 0.9570 || Learning rate: lr=2.5e-05.
===> Epoch[275](40/324): Loss: 0.7125 || Learning rate: lr=2.5e-05.
===> Epoch[275](50/324): Loss: 0.7978 || Learning rate: lr=2.5e-05.
===> Epoch[275](60/324): Loss: 0.7997 || Learning rate: lr=2.5e-05.
===> Epoch[275](70/324): Loss: 0.6862 || Learning rate: lr=2.5e-05.
===> Epoch[275](80/324): Loss: 0.7334 || Learning rate: lr=2.5e-05.
===> Epoch[275](90/324): Loss: 0.5318 || Learning rate: lr=2.5e-05.
===> Epoch[275](100/324): Loss: 0.5882 || Learning rate: lr=2.5e-05.
===> Epoch[275](110/324): Loss: 0.5584 || Learning rate: lr=2.5e-05.
===> Epoch[275](120/324): Loss: 0.5916 || Learning rate: lr=2.5e-05.
===> Epoch[275](130/324): Loss: 0.5971 || Learning rate: lr=2.5e-05.
===> Epoch[275](140/324): Loss: 0.8373 || Learning rate: lr=2.5e-05.
===> Epoch[275](150/324): Loss: 0.5855 || Learning rate: lr=2.5e-05.
===> Epoch[275](160/324): Loss: 0.6186 || Learning rate: lr=2.5e-05.
===> Epoch[275](170/324): Loss: 0.8199 || Learning rate: lr=2.5e-05.
===> Epoch[275](180/324): Loss: 0.5216 || Learning rate: lr=2.5e-05.
===> Epoch[275](190/324): Loss: 0.7747 || Learning rate: lr=2.5e-05.
===> Epoch[275](200/324): Loss: 0.8808 || Learning rate: lr=2.5e-05.
===> Epoch[275](210/324): Loss: 0.5772 || Learning rate: lr=2.5e-05.
===> Epoch[275](220/324): Loss: 0.7028 || Learning rate: lr=2.5e-05.
===> Epoch[275](230/324): Loss: 0.6534 || Learning rate: lr=2.5e-05.
===> Epoch[275](240/324): Loss: 0.5309 || Learning rate: lr=2.5e-05.
===> Epoch[275](250/324): Loss: 0.8008 || Learning rate: lr=2.5e-05.
===> Epoch[275](260/324): Loss: 0.5344 || Learning rate: lr=2.5e-05.
===> Epoch[275](270/324): Loss: 0.5150 || Learning rate: lr=2.5e-05.
===> Epoch[275](280/324): Loss: 1.0702 || Learning rate: lr=2.5e-05.
===> Epoch[275](290/324): Loss: 0.5561 || Learning rate: lr=2.5e-05.
===> Epoch[275](300/324): Loss: 0.7180 || Learning rate: lr=2.5e-05.
===> Epoch[275](310/324): Loss: 0.6562 || Learning rate: lr=2.5e-05.
===> Epoch[275](320/324): Loss: 0.7550 || Learning rate: lr=2.5e-05.
===> Epoch[276](10/324): Loss: 0.7925 || Learning rate: lr=2.5e-05.
===> Epoch[276](20/324): Loss: 0.7074 || Learning rate: lr=2.5e-05.
===> Epoch[276](30/324): Loss: 1.1956 || Learning rate: lr=2.5e-05.
===> Epoch[276](40/324): Loss: 0.6252 || Learning rate: lr=2.5e-05.
===> Epoch[276](50/324): Loss: 0.6236 || Learning rate: lr=2.5e-05.
===> Epoch[276](60/324): Loss: 0.4664 || Learning rate: lr=2.5e-05.
===> Epoch[276](70/324): Loss: 0.7323 || Learning rate: lr=2.5e-05.
===> Epoch[276](80/324): Loss: 0.9775 || Learning rate: lr=2.5e-05.
===> Epoch[276](90/324): Loss: 0.6976 || Learning rate: lr=2.5e-05.
===> Epoch[276](100/324): Loss: 0.9570 || Learning rate: lr=2.5e-05.
===> Epoch[276](110/324): Loss: 0.7231 || Learning rate: lr=2.5e-05.
===> Epoch[276](120/324): Loss: 0.7445 || Learning rate: lr=2.5e-05.
===> Epoch[276](130/324): Loss: 0.8313 || Learning rate: lr=2.5e-05.
===> Epoch[276](140/324): Loss: 0.8574 || Learning rate: lr=2.5e-05.
===> Epoch[276](150/324): Loss: 0.8569 || Learning rate: lr=2.5e-05.
===> Epoch[276](160/324): Loss: 0.6728 || Learning rate: lr=2.5e-05.
===> Epoch[276](170/324): Loss: 0.6126 || Learning rate: lr=2.5e-05.
===> Epoch[276](180/324): Loss: 0.4967 || Learning rate: lr=2.5e-05.
===> Epoch[276](190/324): Loss: 0.5385 || Learning rate: lr=2.5e-05.
===> Epoch[276](200/324): Loss: 0.7790 || Learning rate: lr=2.5e-05.
===> Epoch[276](210/324): Loss: 0.7099 || Learning rate: lr=2.5e-05.
===> Epoch[276](220/324): Loss: 0.4406 || Learning rate: lr=2.5e-05.
===> Epoch[276](230/324): Loss: 0.4922 || Learning rate: lr=2.5e-05.
===> Epoch[276](240/324): Loss: 0.6006 || Learning rate: lr=2.5e-05.
===> Epoch[276](250/324): Loss: 0.7341 || Learning rate: lr=2.5e-05.
===> Epoch[276](260/324): Loss: 0.7142 || Learning rate: lr=2.5e-05.
===> Epoch[276](270/324): Loss: 0.5846 || Learning rate: lr=2.5e-05.
===> Epoch[276](280/324): Loss: 0.5284 || Learning rate: lr=2.5e-05.
===> Epoch[276](290/324): Loss: 0.4729 || Learning rate: lr=2.5e-05.
===> Epoch[276](300/324): Loss: 0.5351 || Learning rate: lr=2.5e-05.
===> Epoch[276](310/324): Loss: 0.6659 || Learning rate: lr=2.5e-05.
===> Epoch[276](320/324): Loss: 0.7097 || Learning rate: lr=2.5e-05.
===> Epoch[277](10/324): Loss: 0.8146 || Learning rate: lr=2.5e-05.
===> Epoch[277](20/324): Loss: 0.5384 || Learning rate: lr=2.5e-05.
===> Epoch[277](30/324): Loss: 0.8072 || Learning rate: lr=2.5e-05.
===> Epoch[277](40/324): Loss: 0.7809 || Learning rate: lr=2.5e-05.
===> Epoch[277](50/324): Loss: 0.5426 || Learning rate: lr=2.5e-05.
===> Epoch[277](60/324): Loss: 0.7589 || Learning rate: lr=2.5e-05.
===> Epoch[277](70/324): Loss: 0.6915 || Learning rate: lr=2.5e-05.
===> Epoch[277](80/324): Loss: 0.4340 || Learning rate: lr=2.5e-05.
===> Epoch[277](90/324): Loss: 0.5588 || Learning rate: lr=2.5e-05.
===> Epoch[277](100/324): Loss: 0.6272 || Learning rate: lr=2.5e-05.
===> Epoch[277](110/324): Loss: 0.6369 || Learning rate: lr=2.5e-05.
===> Epoch[277](120/324): Loss: 0.7231 || Learning rate: lr=2.5e-05.
===> Epoch[277](130/324): Loss: 0.5585 || Learning rate: lr=2.5e-05.
===> Epoch[277](140/324): Loss: 0.8245 || Learning rate: lr=2.5e-05.
===> Epoch[277](150/324): Loss: 0.8800 || Learning rate: lr=2.5e-05.
===> Epoch[277](160/324): Loss: 0.9036 || Learning rate: lr=2.5e-05.
===> Epoch[277](170/324): Loss: 0.3918 || Learning rate: lr=2.5e-05.
===> Epoch[277](180/324): Loss: 0.7384 || Learning rate: lr=2.5e-05.
===> Epoch[277](190/324): Loss: 0.6774 || Learning rate: lr=2.5e-05.
===> Epoch[277](200/324): Loss: 0.9688 || Learning rate: lr=2.5e-05.
===> Epoch[277](210/324): Loss: 0.5214 || Learning rate: lr=2.5e-05.
===> Epoch[277](220/324): Loss: 0.7032 || Learning rate: lr=2.5e-05.
===> Epoch[277](230/324): Loss: 0.6859 || Learning rate: lr=2.5e-05.
===> Epoch[277](240/324): Loss: 0.5220 || Learning rate: lr=2.5e-05.
===> Epoch[277](250/324): Loss: 0.7110 || Learning rate: lr=2.5e-05.
===> Epoch[277](260/324): Loss: 0.9093 || Learning rate: lr=2.5e-05.
===> Epoch[277](270/324): Loss: 0.6939 || Learning rate: lr=2.5e-05.
===> Epoch[277](280/324): Loss: 0.5374 || Learning rate: lr=2.5e-05.
===> Epoch[277](290/324): Loss: 0.7901 || Learning rate: lr=2.5e-05.
===> Epoch[277](300/324): Loss: 0.6504 || Learning rate: lr=2.5e-05.
===> Epoch[277](310/324): Loss: 0.7512 || Learning rate: lr=2.5e-05.
===> Epoch[277](320/324): Loss: 0.6963 || Learning rate: lr=2.5e-05.
===> Epoch[278](10/324): Loss: 0.5628 || Learning rate: lr=2.5e-05.
===> Epoch[278](20/324): Loss: 0.6444 || Learning rate: lr=2.5e-05.
===> Epoch[278](30/324): Loss: 0.4914 || Learning rate: lr=2.5e-05.
===> Epoch[278](40/324): Loss: 0.7595 || Learning rate: lr=2.5e-05.
===> Epoch[278](50/324): Loss: 0.6549 || Learning rate: lr=2.5e-05.
===> Epoch[278](60/324): Loss: 0.7548 || Learning rate: lr=2.5e-05.
===> Epoch[278](70/324): Loss: 0.8371 || Learning rate: lr=2.5e-05.
===> Epoch[278](80/324): Loss: 0.8704 || Learning rate: lr=2.5e-05.
===> Epoch[278](90/324): Loss: 0.7314 || Learning rate: lr=2.5e-05.
===> Epoch[278](100/324): Loss: 0.5096 || Learning rate: lr=2.5e-05.
===> Epoch[278](110/324): Loss: 0.7790 || Learning rate: lr=2.5e-05.
===> Epoch[278](120/324): Loss: 0.3690 || Learning rate: lr=2.5e-05.
===> Epoch[278](130/324): Loss: 0.4683 || Learning rate: lr=2.5e-05.
===> Epoch[278](140/324): Loss: 0.4266 || Learning rate: lr=2.5e-05.
===> Epoch[278](150/324): Loss: 0.7471 || Learning rate: lr=2.5e-05.
===> Epoch[278](160/324): Loss: 0.5255 || Learning rate: lr=2.5e-05.
===> Epoch[278](170/324): Loss: 0.6297 || Learning rate: lr=2.5e-05.
===> Epoch[278](180/324): Loss: 0.8302 || Learning rate: lr=2.5e-05.
===> Epoch[278](190/324): Loss: 0.8303 || Learning rate: lr=2.5e-05.
===> Epoch[278](200/324): Loss: 1.0206 || Learning rate: lr=2.5e-05.
===> Epoch[278](210/324): Loss: 0.6066 || Learning rate: lr=2.5e-05.
===> Epoch[278](220/324): Loss: 0.4853 || Learning rate: lr=2.5e-05.
===> Epoch[278](230/324): Loss: 0.6688 || Learning rate: lr=2.5e-05.
===> Epoch[278](240/324): Loss: 0.6914 || Learning rate: lr=2.5e-05.
===> Epoch[278](250/324): Loss: 0.6188 || Learning rate: lr=2.5e-05.
===> Epoch[278](260/324): Loss: 0.5035 || Learning rate: lr=2.5e-05.
===> Epoch[278](270/324): Loss: 0.7860 || Learning rate: lr=2.5e-05.
===> Epoch[278](280/324): Loss: 0.5976 || Learning rate: lr=2.5e-05.
===> Epoch[278](290/324): Loss: 0.8091 || Learning rate: lr=2.5e-05.
===> Epoch[278](300/324): Loss: 0.7213 || Learning rate: lr=2.5e-05.
===> Epoch[278](310/324): Loss: 0.7470 || Learning rate: lr=2.5e-05.
===> Epoch[278](320/324): Loss: 1.0153 || Learning rate: lr=2.5e-05.
===> Epoch[279](10/324): Loss: 0.4538 || Learning rate: lr=2.5e-05.
===> Epoch[279](20/324): Loss: 0.9442 || Learning rate: lr=2.5e-05.
===> Epoch[279](30/324): Loss: 0.6178 || Learning rate: lr=2.5e-05.
===> Epoch[279](40/324): Loss: 0.8243 || Learning rate: lr=2.5e-05.
===> Epoch[279](50/324): Loss: 0.7236 || Learning rate: lr=2.5e-05.
===> Epoch[279](60/324): Loss: 1.2006 || Learning rate: lr=2.5e-05.
===> Epoch[279](70/324): Loss: 0.6353 || Learning rate: lr=2.5e-05.
===> Epoch[279](80/324): Loss: 0.8107 || Learning rate: lr=2.5e-05.
===> Epoch[279](90/324): Loss: 0.7730 || Learning rate: lr=2.5e-05.
===> Epoch[279](100/324): Loss: 0.6147 || Learning rate: lr=2.5e-05.
===> Epoch[279](110/324): Loss: 0.3958 || Learning rate: lr=2.5e-05.
===> Epoch[279](120/324): Loss: 0.4662 || Learning rate: lr=2.5e-05.
===> Epoch[279](130/324): Loss: 0.5869 || Learning rate: lr=2.5e-05.
===> Epoch[279](140/324): Loss: 0.7252 || Learning rate: lr=2.5e-05.
===> Epoch[279](150/324): Loss: 0.6051 || Learning rate: lr=2.5e-05.
===> Epoch[279](160/324): Loss: 0.3987 || Learning rate: lr=2.5e-05.
===> Epoch[279](170/324): Loss: 0.6785 || Learning rate: lr=2.5e-05.
===> Epoch[279](180/324): Loss: 0.6271 || Learning rate: lr=2.5e-05.
===> Epoch[279](190/324): Loss: 0.7027 || Learning rate: lr=2.5e-05.
===> Epoch[279](200/324): Loss: 0.7308 || Learning rate: lr=2.5e-05.
===> Epoch[279](210/324): Loss: 0.8201 || Learning rate: lr=2.5e-05.
===> Epoch[279](220/324): Loss: 0.7553 || Learning rate: lr=2.5e-05.
===> Epoch[279](230/324): Loss: 0.6807 || Learning rate: lr=2.5e-05.
===> Epoch[279](240/324): Loss: 0.5395 || Learning rate: lr=2.5e-05.
===> Epoch[279](250/324): Loss: 0.4854 || Learning rate: lr=2.5e-05.
===> Epoch[279](260/324): Loss: 0.5180 || Learning rate: lr=2.5e-05.
===> Epoch[279](270/324): Loss: 0.5305 || Learning rate: lr=2.5e-05.
===> Epoch[279](280/324): Loss: 0.5668 || Learning rate: lr=2.5e-05.
===> Epoch[279](290/324): Loss: 0.7668 || Learning rate: lr=2.5e-05.
===> Epoch[279](300/324): Loss: 1.0089 || Learning rate: lr=2.5e-05.
===> Epoch[279](310/324): Loss: 0.7803 || Learning rate: lr=2.5e-05.
===> Epoch[279](320/324): Loss: 0.4716 || Learning rate: lr=2.5e-05.
===> Epoch[280](10/324): Loss: 0.5480 || Learning rate: lr=2.5e-05.
===> Epoch[280](20/324): Loss: 0.5356 || Learning rate: lr=2.5e-05.
===> Epoch[280](30/324): Loss: 0.8728 || Learning rate: lr=2.5e-05.
===> Epoch[280](40/324): Loss: 0.8690 || Learning rate: lr=2.5e-05.
===> Epoch[280](50/324): Loss: 0.6521 || Learning rate: lr=2.5e-05.
===> Epoch[280](60/324): Loss: 0.5837 || Learning rate: lr=2.5e-05.
===> Epoch[280](70/324): Loss: 0.4666 || Learning rate: lr=2.5e-05.
===> Epoch[280](80/324): Loss: 0.4995 || Learning rate: lr=2.5e-05.
===> Epoch[280](90/324): Loss: 0.5534 || Learning rate: lr=2.5e-05.
===> Epoch[280](100/324): Loss: 0.7334 || Learning rate: lr=2.5e-05.
===> Epoch[280](110/324): Loss: 0.6406 || Learning rate: lr=2.5e-05.
===> Epoch[280](120/324): Loss: 0.7508 || Learning rate: lr=2.5e-05.
===> Epoch[280](130/324): Loss: 0.7213 || Learning rate: lr=2.5e-05.
===> Epoch[280](140/324): Loss: 0.6398 || Learning rate: lr=2.5e-05.
===> Epoch[280](150/324): Loss: 0.5024 || Learning rate: lr=2.5e-05.
===> Epoch[280](160/324): Loss: 0.6711 || Learning rate: lr=2.5e-05.
===> Epoch[280](170/324): Loss: 0.5463 || Learning rate: lr=2.5e-05.
===> Epoch[280](180/324): Loss: 0.8185 || Learning rate: lr=2.5e-05.
===> Epoch[280](190/324): Loss: 0.8202 || Learning rate: lr=2.5e-05.
===> Epoch[280](200/324): Loss: 0.7195 || Learning rate: lr=2.5e-05.
===> Epoch[280](210/324): Loss: 0.4201 || Learning rate: lr=2.5e-05.
===> Epoch[280](220/324): Loss: 0.8338 || Learning rate: lr=2.5e-05.
===> Epoch[280](230/324): Loss: 0.6678 || Learning rate: lr=2.5e-05.
===> Epoch[280](240/324): Loss: 0.3511 || Learning rate: lr=2.5e-05.
===> Epoch[280](250/324): Loss: 0.7090 || Learning rate: lr=2.5e-05.
===> Epoch[280](260/324): Loss: 0.5747 || Learning rate: lr=2.5e-05.
===> Epoch[280](270/324): Loss: 0.6644 || Learning rate: lr=2.5e-05.
===> Epoch[280](280/324): Loss: 0.8940 || Learning rate: lr=2.5e-05.
===> Epoch[280](290/324): Loss: 0.8850 || Learning rate: lr=2.5e-05.
===> Epoch[280](300/324): Loss: 0.7836 || Learning rate: lr=2.5e-05.
===> Epoch[280](310/324): Loss: 0.6508 || Learning rate: lr=2.5e-05.
===> Epoch[280](320/324): Loss: 0.6905 || Learning rate: lr=2.5e-05.
Checkpoint saved to weights/epoch_v2_280.pth
===> Epoch[281](10/324): Loss: 0.5036 || Learning rate: lr=2.5e-05.
===> Epoch[281](20/324): Loss: 0.4409 || Learning rate: lr=2.5e-05.
===> Epoch[281](30/324): Loss: 0.6591 || Learning rate: lr=2.5e-05.
===> Epoch[281](40/324): Loss: 0.7937 || Learning rate: lr=2.5e-05.
===> Epoch[281](50/324): Loss: 1.0205 || Learning rate: lr=2.5e-05.
===> Epoch[281](60/324): Loss: 0.5985 || Learning rate: lr=2.5e-05.
===> Epoch[281](70/324): Loss: 0.4341 || Learning rate: lr=2.5e-05.
===> Epoch[281](80/324): Loss: 0.5771 || Learning rate: lr=2.5e-05.
===> Epoch[281](90/324): Loss: 0.5970 || Learning rate: lr=2.5e-05.
===> Epoch[281](100/324): Loss: 0.6170 || Learning rate: lr=2.5e-05.
===> Epoch[281](110/324): Loss: 0.6646 || Learning rate: lr=2.5e-05.
===> Epoch[281](120/324): Loss: 0.6299 || Learning rate: lr=2.5e-05.
===> Epoch[281](130/324): Loss: 0.7101 || Learning rate: lr=2.5e-05.
===> Epoch[281](140/324): Loss: 0.6302 || Learning rate: lr=2.5e-05.
===> Epoch[281](150/324): Loss: 0.8220 || Learning rate: lr=2.5e-05.
===> Epoch[281](160/324): Loss: 0.5120 || Learning rate: lr=2.5e-05.
===> Epoch[281](170/324): Loss: 0.7217 || Learning rate: lr=2.5e-05.
===> Epoch[281](180/324): Loss: 0.6893 || Learning rate: lr=2.5e-05.
===> Epoch[281](190/324): Loss: 0.6085 || Learning rate: lr=2.5e-05.
===> Epoch[281](200/324): Loss: 0.5502 || Learning rate: lr=2.5e-05.
===> Epoch[281](210/324): Loss: 0.6640 || Learning rate: lr=2.5e-05.
===> Epoch[281](220/324): Loss: 0.5792 || Learning rate: lr=2.5e-05.
===> Epoch[281](230/324): Loss: 0.7351 || Learning rate: lr=2.5e-05.
===> Epoch[281](240/324): Loss: 0.9062 || Learning rate: lr=2.5e-05.
===> Epoch[281](250/324): Loss: 0.5343 || Learning rate: lr=2.5e-05.
===> Epoch[281](260/324): Loss: 0.8320 || Learning rate: lr=2.5e-05.
===> Epoch[281](270/324): Loss: 0.7915 || Learning rate: lr=2.5e-05.
===> Epoch[281](280/324): Loss: 0.7084 || Learning rate: lr=2.5e-05.
===> Epoch[281](290/324): Loss: 0.6248 || Learning rate: lr=2.5e-05.
===> Epoch[281](300/324): Loss: 0.8793 || Learning rate: lr=2.5e-05.
===> Epoch[281](310/324): Loss: 0.5294 || Learning rate: lr=2.5e-05.
===> Epoch[281](320/324): Loss: 0.6045 || Learning rate: lr=2.5e-05.
===> Epoch[282](10/324): Loss: 0.5930 || Learning rate: lr=2.5e-05.
===> Epoch[282](20/324): Loss: 0.8039 || Learning rate: lr=2.5e-05.
===> Epoch[282](30/324): Loss: 0.6652 || Learning rate: lr=2.5e-05.
===> Epoch[282](40/324): Loss: 0.5043 || Learning rate: lr=2.5e-05.
===> Epoch[282](50/324): Loss: 0.8843 || Learning rate: lr=2.5e-05.
===> Epoch[282](60/324): Loss: 0.4367 || Learning rate: lr=2.5e-05.
===> Epoch[282](70/324): Loss: 0.8280 || Learning rate: lr=2.5e-05.
===> Epoch[282](80/324): Loss: 0.7376 || Learning rate: lr=2.5e-05.
===> Epoch[282](90/324): Loss: 0.5622 || Learning rate: lr=2.5e-05.
===> Epoch[282](100/324): Loss: 0.5930 || Learning rate: lr=2.5e-05.
===> Epoch[282](110/324): Loss: 0.6706 || Learning rate: lr=2.5e-05.
===> Epoch[282](120/324): Loss: 0.7579 || Learning rate: lr=2.5e-05.
===> Epoch[282](130/324): Loss: 0.7102 || Learning rate: lr=2.5e-05.
===> Epoch[282](140/324): Loss: 0.6728 || Learning rate: lr=2.5e-05.
===> Epoch[282](150/324): Loss: 0.7720 || Learning rate: lr=2.5e-05.
===> Epoch[282](160/324): Loss: 0.8598 || Learning rate: lr=2.5e-05.
===> Epoch[282](170/324): Loss: 0.6252 || Learning rate: lr=2.5e-05.
===> Epoch[282](180/324): Loss: 0.4082 || Learning rate: lr=2.5e-05.
===> Epoch[282](190/324): Loss: 0.6804 || Learning rate: lr=2.5e-05.
===> Epoch[282](200/324): Loss: 0.3430 || Learning rate: lr=2.5e-05.
===> Epoch[282](210/324): Loss: 0.8854 || Learning rate: lr=2.5e-05.
===> Epoch[282](220/324): Loss: 0.5578 || Learning rate: lr=2.5e-05.
===> Epoch[282](230/324): Loss: 0.6856 || Learning rate: lr=2.5e-05.
===> Epoch[282](240/324): Loss: 0.5911 || Learning rate: lr=2.5e-05.
===> Epoch[282](250/324): Loss: 0.6757 || Learning rate: lr=2.5e-05.
===> Epoch[282](260/324): Loss: 0.6676 || Learning rate: lr=2.5e-05.
===> Epoch[282](270/324): Loss: 0.7648 || Learning rate: lr=2.5e-05.
===> Epoch[282](280/324): Loss: 0.5597 || Learning rate: lr=2.5e-05.
===> Epoch[282](290/324): Loss: 0.8845 || Learning rate: lr=2.5e-05.
===> Epoch[282](300/324): Loss: 0.5841 || Learning rate: lr=2.5e-05.
===> Epoch[282](310/324): Loss: 0.5396 || Learning rate: lr=2.5e-05.
===> Epoch[282](320/324): Loss: 0.9019 || Learning rate: lr=2.5e-05.
===> Epoch[283](10/324): Loss: 0.7282 || Learning rate: lr=2.5e-05.
===> Epoch[283](20/324): Loss: 0.6773 || Learning rate: lr=2.5e-05.
===> Epoch[283](30/324): Loss: 0.7032 || Learning rate: lr=2.5e-05.
===> Epoch[283](40/324): Loss: 0.5158 || Learning rate: lr=2.5e-05.
===> Epoch[283](50/324): Loss: 0.7426 || Learning rate: lr=2.5e-05.
===> Epoch[283](60/324): Loss: 0.5777 || Learning rate: lr=2.5e-05.
===> Epoch[283](70/324): Loss: 0.6192 || Learning rate: lr=2.5e-05.
===> Epoch[283](80/324): Loss: 1.0363 || Learning rate: lr=2.5e-05.
===> Epoch[283](90/324): Loss: 0.7381 || Learning rate: lr=2.5e-05.
===> Epoch[283](100/324): Loss: 0.5922 || Learning rate: lr=2.5e-05.
===> Epoch[283](110/324): Loss: 0.7201 || Learning rate: lr=2.5e-05.
===> Epoch[283](120/324): Loss: 0.5689 || Learning rate: lr=2.5e-05.
===> Epoch[283](130/324): Loss: 0.5315 || Learning rate: lr=2.5e-05.
===> Epoch[283](140/324): Loss: 0.4635 || Learning rate: lr=2.5e-05.
===> Epoch[283](150/324): Loss: 0.6932 || Learning rate: lr=2.5e-05.
===> Epoch[283](160/324): Loss: 0.4301 || Learning rate: lr=2.5e-05.
===> Epoch[283](170/324): Loss: 0.6707 || Learning rate: lr=2.5e-05.
===> Epoch[283](180/324): Loss: 0.4379 || Learning rate: lr=2.5e-05.
===> Epoch[283](190/324): Loss: 0.6199 || Learning rate: lr=2.5e-05.
===> Epoch[283](200/324): Loss: 0.7450 || Learning rate: lr=2.5e-05.
===> Epoch[283](210/324): Loss: 0.7498 || Learning rate: lr=2.5e-05.
===> Epoch[283](220/324): Loss: 0.6814 || Learning rate: lr=2.5e-05.
===> Epoch[283](230/324): Loss: 0.9501 || Learning rate: lr=2.5e-05.
===> Epoch[283](240/324): Loss: 0.7024 || Learning rate: lr=2.5e-05.
===> Epoch[283](250/324): Loss: 0.9063 || Learning rate: lr=2.5e-05.
===> Epoch[283](260/324): Loss: 1.0193 || Learning rate: lr=2.5e-05.
===> Epoch[283](270/324): Loss: 0.7876 || Learning rate: lr=2.5e-05.
===> Epoch[283](280/324): Loss: 0.7312 || Learning rate: lr=2.5e-05.
===> Epoch[283](290/324): Loss: 0.7351 || Learning rate: lr=2.5e-05.
===> Epoch[283](300/324): Loss: 0.7565 || Learning rate: lr=2.5e-05.
===> Epoch[283](310/324): Loss: 0.5891 || Learning rate: lr=2.5e-05.
===> Epoch[283](320/324): Loss: 0.7603 || Learning rate: lr=2.5e-05.
===> Epoch[284](10/324): Loss: 0.7298 || Learning rate: lr=2.5e-05.
===> Epoch[284](20/324): Loss: 0.9107 || Learning rate: lr=2.5e-05.
===> Epoch[284](30/324): Loss: 0.5963 || Learning rate: lr=2.5e-05.
===> Epoch[284](40/324): Loss: 0.6157 || Learning rate: lr=2.5e-05.
===> Epoch[284](50/324): Loss: 0.5149 || Learning rate: lr=2.5e-05.
===> Epoch[284](60/324): Loss: 0.5508 || Learning rate: lr=2.5e-05.
===> Epoch[284](70/324): Loss: 0.6856 || Learning rate: lr=2.5e-05.
===> Epoch[284](80/324): Loss: 0.6547 || Learning rate: lr=2.5e-05.
===> Epoch[284](90/324): Loss: 0.7199 || Learning rate: lr=2.5e-05.
===> Epoch[284](100/324): Loss: 0.6026 || Learning rate: lr=2.5e-05.
===> Epoch[284](110/324): Loss: 0.7591 || Learning rate: lr=2.5e-05.
===> Epoch[284](120/324): Loss: 0.6087 || Learning rate: lr=2.5e-05.
===> Epoch[284](130/324): Loss: 0.7438 || Learning rate: lr=2.5e-05.
===> Epoch[284](140/324): Loss: 0.6913 || Learning rate: lr=2.5e-05.
===> Epoch[284](150/324): Loss: 0.5776 || Learning rate: lr=2.5e-05.
===> Epoch[284](160/324): Loss: 0.7180 || Learning rate: lr=2.5e-05.
===> Epoch[284](170/324): Loss: 0.4994 || Learning rate: lr=2.5e-05.
===> Epoch[284](180/324): Loss: 0.5450 || Learning rate: lr=2.5e-05.
===> Epoch[284](190/324): Loss: 0.6420 || Learning rate: lr=2.5e-05.
===> Epoch[284](200/324): Loss: 0.5099 || Learning rate: lr=2.5e-05.
===> Epoch[284](210/324): Loss: 0.6924 || Learning rate: lr=2.5e-05.
===> Epoch[284](220/324): Loss: 0.7828 || Learning rate: lr=2.5e-05.
===> Epoch[284](230/324): Loss: 0.5783 || Learning rate: lr=2.5e-05.
===> Epoch[284](240/324): Loss: 0.6077 || Learning rate: lr=2.5e-05.
===> Epoch[284](250/324): Loss: 0.7045 || Learning rate: lr=2.5e-05.
===> Epoch[284](260/324): Loss: 0.6172 || Learning rate: lr=2.5e-05.
===> Epoch[284](270/324): Loss: 0.5584 || Learning rate: lr=2.5e-05.
===> Epoch[284](280/324): Loss: 0.8040 || Learning rate: lr=2.5e-05.
===> Epoch[284](290/324): Loss: 0.3693 || Learning rate: lr=2.5e-05.
===> Epoch[284](300/324): Loss: 0.5599 || Learning rate: lr=2.5e-05.
===> Epoch[284](310/324): Loss: 1.1103 || Learning rate: lr=2.5e-05.
===> Epoch[284](320/324): Loss: 0.7201 || Learning rate: lr=2.5e-05.
===> Epoch[285](10/324): Loss: 0.7679 || Learning rate: lr=2.5e-05.
===> Epoch[285](20/324): Loss: 0.5345 || Learning rate: lr=2.5e-05.
===> Epoch[285](30/324): Loss: 0.5669 || Learning rate: lr=2.5e-05.
===> Epoch[285](40/324): Loss: 0.8500 || Learning rate: lr=2.5e-05.
===> Epoch[285](50/324): Loss: 0.6078 || Learning rate: lr=2.5e-05.
===> Epoch[285](60/324): Loss: 0.5675 || Learning rate: lr=2.5e-05.
===> Epoch[285](70/324): Loss: 0.6357 || Learning rate: lr=2.5e-05.
===> Epoch[285](80/324): Loss: 0.6111 || Learning rate: lr=2.5e-05.
===> Epoch[285](90/324): Loss: 0.7126 || Learning rate: lr=2.5e-05.
===> Epoch[285](100/324): Loss: 0.5126 || Learning rate: lr=2.5e-05.
===> Epoch[285](110/324): Loss: 0.6824 || Learning rate: lr=2.5e-05.
===> Epoch[285](120/324): Loss: 0.8382 || Learning rate: lr=2.5e-05.
===> Epoch[285](130/324): Loss: 0.7105 || Learning rate: lr=2.5e-05.
===> Epoch[285](140/324): Loss: 1.1885 || Learning rate: lr=2.5e-05.
===> Epoch[285](150/324): Loss: 0.5282 || Learning rate: lr=2.5e-05.
===> Epoch[285](160/324): Loss: 1.1093 || Learning rate: lr=2.5e-05.
===> Epoch[285](170/324): Loss: 1.0759 || Learning rate: lr=2.5e-05.
===> Epoch[285](180/324): Loss: 0.6425 || Learning rate: lr=2.5e-05.
===> Epoch[285](190/324): Loss: 0.7190 || Learning rate: lr=2.5e-05.
===> Epoch[285](200/324): Loss: 0.7836 || Learning rate: lr=2.5e-05.
===> Epoch[285](210/324): Loss: 0.6978 || Learning rate: lr=2.5e-05.
===> Epoch[285](220/324): Loss: 0.7946 || Learning rate: lr=2.5e-05.
===> Epoch[285](230/324): Loss: 0.5628 || Learning rate: lr=2.5e-05.
===> Epoch[285](240/324): Loss: 0.6721 || Learning rate: lr=2.5e-05.
===> Epoch[285](250/324): Loss: 0.7704 || Learning rate: lr=2.5e-05.
===> Epoch[285](260/324): Loss: 0.8996 || Learning rate: lr=2.5e-05.
===> Epoch[285](270/324): Loss: 0.5846 || Learning rate: lr=2.5e-05.
===> Epoch[285](280/324): Loss: 0.5008 || Learning rate: lr=2.5e-05.
===> Epoch[285](290/324): Loss: 0.5273 || Learning rate: lr=2.5e-05.
===> Epoch[285](300/324): Loss: 0.6626 || Learning rate: lr=2.5e-05.
===> Epoch[285](310/324): Loss: 0.5480 || Learning rate: lr=2.5e-05.
===> Epoch[285](320/324): Loss: 0.8553 || Learning rate: lr=2.5e-05.
===> Epoch[286](10/324): Loss: 0.6113 || Learning rate: lr=2.5e-05.
===> Epoch[286](20/324): Loss: 0.6218 || Learning rate: lr=2.5e-05.
===> Epoch[286](30/324): Loss: 0.6083 || Learning rate: lr=2.5e-05.
===> Epoch[286](40/324): Loss: 0.8430 || Learning rate: lr=2.5e-05.
===> Epoch[286](50/324): Loss: 0.8729 || Learning rate: lr=2.5e-05.
===> Epoch[286](60/324): Loss: 1.0314 || Learning rate: lr=2.5e-05.
===> Epoch[286](70/324): Loss: 0.7144 || Learning rate: lr=2.5e-05.
===> Epoch[286](80/324): Loss: 0.7912 || Learning rate: lr=2.5e-05.
===> Epoch[286](90/324): Loss: 0.7855 || Learning rate: lr=2.5e-05.
===> Epoch[286](100/324): Loss: 0.5069 || Learning rate: lr=2.5e-05.
===> Epoch[286](110/324): Loss: 0.8902 || Learning rate: lr=2.5e-05.
===> Epoch[286](120/324): Loss: 0.4153 || Learning rate: lr=2.5e-05.
===> Epoch[286](130/324): Loss: 0.7397 || Learning rate: lr=2.5e-05.
===> Epoch[286](140/324): Loss: 0.6380 || Learning rate: lr=2.5e-05.
===> Epoch[286](150/324): Loss: 0.5562 || Learning rate: lr=2.5e-05.
===> Epoch[286](160/324): Loss: 0.8873 || Learning rate: lr=2.5e-05.
===> Epoch[286](170/324): Loss: 0.6102 || Learning rate: lr=2.5e-05.
===> Epoch[286](180/324): Loss: 0.6872 || Learning rate: lr=2.5e-05.
===> Epoch[286](190/324): Loss: 0.5838 || Learning rate: lr=2.5e-05.
===> Epoch[286](200/324): Loss: 0.4299 || Learning rate: lr=2.5e-05.
===> Epoch[286](210/324): Loss: 0.7332 || Learning rate: lr=2.5e-05.
===> Epoch[286](220/324): Loss: 0.6286 || Learning rate: lr=2.5e-05.
===> Epoch[286](230/324): Loss: 1.0594 || Learning rate: lr=2.5e-05.
===> Epoch[286](240/324): Loss: 0.6636 || Learning rate: lr=2.5e-05.
===> Epoch[286](250/324): Loss: 0.7052 || Learning rate: lr=2.5e-05.
===> Epoch[286](260/324): Loss: 0.4599 || Learning rate: lr=2.5e-05.
===> Epoch[286](270/324): Loss: 0.7639 || Learning rate: lr=2.5e-05.
===> Epoch[286](280/324): Loss: 0.4930 || Learning rate: lr=2.5e-05.
===> Epoch[286](290/324): Loss: 0.8761 || Learning rate: lr=2.5e-05.
===> Epoch[286](300/324): Loss: 0.7616 || Learning rate: lr=2.5e-05.
===> Epoch[286](310/324): Loss: 0.7853 || Learning rate: lr=2.5e-05.
===> Epoch[286](320/324): Loss: 0.8180 || Learning rate: lr=2.5e-05.
===> Epoch[287](10/324): Loss: 0.4475 || Learning rate: lr=2.5e-05.
===> Epoch[287](20/324): Loss: 0.6511 || Learning rate: lr=2.5e-05.
===> Epoch[287](30/324): Loss: 0.4811 || Learning rate: lr=2.5e-05.
===> Epoch[287](40/324): Loss: 0.5951 || Learning rate: lr=2.5e-05.
===> Epoch[287](50/324): Loss: 0.7317 || Learning rate: lr=2.5e-05.
===> Epoch[287](60/324): Loss: 0.5988 || Learning rate: lr=2.5e-05.
===> Epoch[287](70/324): Loss: 0.5671 || Learning rate: lr=2.5e-05.
===> Epoch[287](80/324): Loss: 0.7140 || Learning rate: lr=2.5e-05.
===> Epoch[287](90/324): Loss: 0.7946 || Learning rate: lr=2.5e-05.
===> Epoch[287](100/324): Loss: 0.5953 || Learning rate: lr=2.5e-05.
===> Epoch[287](110/324): Loss: 0.7370 || Learning rate: lr=2.5e-05.
===> Epoch[287](120/324): Loss: 0.7086 || Learning rate: lr=2.5e-05.
===> Epoch[287](130/324): Loss: 0.5919 || Learning rate: lr=2.5e-05.
===> Epoch[287](140/324): Loss: 0.7233 || Learning rate: lr=2.5e-05.
===> Epoch[287](150/324): Loss: 0.7622 || Learning rate: lr=2.5e-05.
===> Epoch[287](160/324): Loss: 0.9855 || Learning rate: lr=2.5e-05.
===> Epoch[287](170/324): Loss: 0.4270 || Learning rate: lr=2.5e-05.
===> Epoch[287](180/324): Loss: 0.8171 || Learning rate: lr=2.5e-05.
===> Epoch[287](190/324): Loss: 0.7273 || Learning rate: lr=2.5e-05.
===> Epoch[287](200/324): Loss: 0.5069 || Learning rate: lr=2.5e-05.
===> Epoch[287](210/324): Loss: 0.8294 || Learning rate: lr=2.5e-05.
===> Epoch[287](220/324): Loss: 1.1088 || Learning rate: lr=2.5e-05.
===> Epoch[287](230/324): Loss: 0.3953 || Learning rate: lr=2.5e-05.
===> Epoch[287](240/324): Loss: 0.8130 || Learning rate: lr=2.5e-05.
===> Epoch[287](250/324): Loss: 0.6242 || Learning rate: lr=2.5e-05.
===> Epoch[287](260/324): Loss: 0.6816 || Learning rate: lr=2.5e-05.
===> Epoch[287](270/324): Loss: 1.0192 || Learning rate: lr=2.5e-05.
===> Epoch[287](280/324): Loss: 0.7429 || Learning rate: lr=2.5e-05.
===> Epoch[287](290/324): Loss: 0.5649 || Learning rate: lr=2.5e-05.
===> Epoch[287](300/324): Loss: 0.5281 || Learning rate: lr=2.5e-05.
===> Epoch[287](310/324): Loss: 0.4236 || Learning rate: lr=2.5e-05.
===> Epoch[287](320/324): Loss: 0.7432 || Learning rate: lr=2.5e-05.
===> Epoch[288](10/324): Loss: 0.7697 || Learning rate: lr=2.5e-05.
===> Epoch[288](20/324): Loss: 0.5142 || Learning rate: lr=2.5e-05.
===> Epoch[288](30/324): Loss: 0.8104 || Learning rate: lr=2.5e-05.
===> Epoch[288](40/324): Loss: 0.7364 || Learning rate: lr=2.5e-05.
===> Epoch[288](50/324): Loss: 0.8138 || Learning rate: lr=2.5e-05.
===> Epoch[288](60/324): Loss: 0.7367 || Learning rate: lr=2.5e-05.
===> Epoch[288](70/324): Loss: 0.6527 || Learning rate: lr=2.5e-05.
===> Epoch[288](80/324): Loss: 0.7696 || Learning rate: lr=2.5e-05.
===> Epoch[288](90/324): Loss: 0.4557 || Learning rate: lr=2.5e-05.
===> Epoch[288](100/324): Loss: 0.8100 || Learning rate: lr=2.5e-05.
===> Epoch[288](110/324): Loss: 0.4314 || Learning rate: lr=2.5e-05.
===> Epoch[288](120/324): Loss: 0.5471 || Learning rate: lr=2.5e-05.
===> Epoch[288](130/324): Loss: 0.6647 || Learning rate: lr=2.5e-05.
===> Epoch[288](140/324): Loss: 0.7612 || Learning rate: lr=2.5e-05.
===> Epoch[288](150/324): Loss: 0.4786 || Learning rate: lr=2.5e-05.
===> Epoch[288](160/324): Loss: 0.7358 || Learning rate: lr=2.5e-05.
===> Epoch[288](170/324): Loss: 0.5674 || Learning rate: lr=2.5e-05.
===> Epoch[288](180/324): Loss: 0.8458 || Learning rate: lr=2.5e-05.
===> Epoch[288](190/324): Loss: 0.7846 || Learning rate: lr=2.5e-05.
===> Epoch[288](200/324): Loss: 0.4112 || Learning rate: lr=2.5e-05.
===> Epoch[288](210/324): Loss: 0.5708 || Learning rate: lr=2.5e-05.
===> Epoch[288](220/324): Loss: 0.7773 || Learning rate: lr=2.5e-05.
===> Epoch[288](230/324): Loss: 0.6389 || Learning rate: lr=2.5e-05.
===> Epoch[288](240/324): Loss: 0.8446 || Learning rate: lr=2.5e-05.
===> Epoch[288](250/324): Loss: 0.4723 || Learning rate: lr=2.5e-05.
===> Epoch[288](260/324): Loss: 0.4296 || Learning rate: lr=2.5e-05.
===> Epoch[288](270/324): Loss: 0.4247 || Learning rate: lr=2.5e-05.
===> Epoch[288](280/324): Loss: 0.7472 || Learning rate: lr=2.5e-05.
===> Epoch[288](290/324): Loss: 0.6840 || Learning rate: lr=2.5e-05.
===> Epoch[288](300/324): Loss: 0.6127 || Learning rate: lr=2.5e-05.
===> Epoch[288](310/324): Loss: 0.9770 || Learning rate: lr=2.5e-05.
===> Epoch[288](320/324): Loss: 0.7061 || Learning rate: lr=2.5e-05.
===> Epoch[289](10/324): Loss: 0.7393 || Learning rate: lr=2.5e-05.
===> Epoch[289](20/324): Loss: 0.5870 || Learning rate: lr=2.5e-05.
===> Epoch[289](30/324): Loss: 0.7663 || Learning rate: lr=2.5e-05.
===> Epoch[289](40/324): Loss: 0.5585 || Learning rate: lr=2.5e-05.
===> Epoch[289](50/324): Loss: 0.7719 || Learning rate: lr=2.5e-05.
===> Epoch[289](60/324): Loss: 0.6971 || Learning rate: lr=2.5e-05.
===> Epoch[289](70/324): Loss: 0.6987 || Learning rate: lr=2.5e-05.
===> Epoch[289](80/324): Loss: 0.8840 || Learning rate: lr=2.5e-05.
===> Epoch[289](90/324): Loss: 0.8226 || Learning rate: lr=2.5e-05.
===> Epoch[289](100/324): Loss: 0.4633 || Learning rate: lr=2.5e-05.
===> Epoch[289](110/324): Loss: 0.8330 || Learning rate: lr=2.5e-05.
===> Epoch[289](120/324): Loss: 0.7346 || Learning rate: lr=2.5e-05.
===> Epoch[289](130/324): Loss: 0.7869 || Learning rate: lr=2.5e-05.
===> Epoch[289](140/324): Loss: 0.7151 || Learning rate: lr=2.5e-05.
===> Epoch[289](150/324): Loss: 0.5194 || Learning rate: lr=2.5e-05.
===> Epoch[289](160/324): Loss: 0.5093 || Learning rate: lr=2.5e-05.
===> Epoch[289](170/324): Loss: 0.7294 || Learning rate: lr=2.5e-05.
===> Epoch[289](180/324): Loss: 0.4960 || Learning rate: lr=2.5e-05.
===> Epoch[289](190/324): Loss: 0.4773 || Learning rate: lr=2.5e-05.
===> Epoch[289](200/324): Loss: 0.6737 || Learning rate: lr=2.5e-05.
===> Epoch[289](210/324): Loss: 0.7809 || Learning rate: lr=2.5e-05.
===> Epoch[289](220/324): Loss: 0.4811 || Learning rate: lr=2.5e-05.
===> Epoch[289](230/324): Loss: 0.8846 || Learning rate: lr=2.5e-05.
===> Epoch[289](240/324): Loss: 0.7307 || Learning rate: lr=2.5e-05.
===> Epoch[289](250/324): Loss: 0.4901 || Learning rate: lr=2.5e-05.
===> Epoch[289](260/324): Loss: 0.5349 || Learning rate: lr=2.5e-05.
===> Epoch[289](270/324): Loss: 0.6090 || Learning rate: lr=2.5e-05.
===> Epoch[289](280/324): Loss: 0.7886 || Learning rate: lr=2.5e-05.
===> Epoch[289](290/324): Loss: 0.7346 || Learning rate: lr=2.5e-05.
===> Epoch[289](300/324): Loss: 0.4973 || Learning rate: lr=2.5e-05.
===> Epoch[289](310/324): Loss: 0.8741 || Learning rate: lr=2.5e-05.
===> Epoch[289](320/324): Loss: 0.5801 || Learning rate: lr=2.5e-05.
===> Epoch[290](10/324): Loss: 0.4886 || Learning rate: lr=2.5e-05.
===> Epoch[290](20/324): Loss: 0.6226 || Learning rate: lr=2.5e-05.
===> Epoch[290](30/324): Loss: 0.5170 || Learning rate: lr=2.5e-05.
===> Epoch[290](40/324): Loss: 0.7040 || Learning rate: lr=2.5e-05.
===> Epoch[290](50/324): Loss: 0.5925 || Learning rate: lr=2.5e-05.
===> Epoch[290](60/324): Loss: 0.6884 || Learning rate: lr=2.5e-05.
===> Epoch[290](70/324): Loss: 0.6560 || Learning rate: lr=2.5e-05.
===> Epoch[290](80/324): Loss: 0.5994 || Learning rate: lr=2.5e-05.
===> Epoch[290](90/324): Loss: 0.6457 || Learning rate: lr=2.5e-05.
===> Epoch[290](100/324): Loss: 0.5475 || Learning rate: lr=2.5e-05.
===> Epoch[290](110/324): Loss: 0.9313 || Learning rate: lr=2.5e-05.
===> Epoch[290](120/324): Loss: 0.5973 || Learning rate: lr=2.5e-05.
===> Epoch[290](130/324): Loss: 0.8250 || Learning rate: lr=2.5e-05.
===> Epoch[290](140/324): Loss: 0.5628 || Learning rate: lr=2.5e-05.
===> Epoch[290](150/324): Loss: 0.8106 || Learning rate: lr=2.5e-05.
===> Epoch[290](160/324): Loss: 0.8621 || Learning rate: lr=2.5e-05.
===> Epoch[290](170/324): Loss: 0.6853 || Learning rate: lr=2.5e-05.
===> Epoch[290](180/324): Loss: 0.6175 || Learning rate: lr=2.5e-05.
===> Epoch[290](190/324): Loss: 0.5254 || Learning rate: lr=2.5e-05.
===> Epoch[290](200/324): Loss: 0.6286 || Learning rate: lr=2.5e-05.
===> Epoch[290](210/324): Loss: 0.6910 || Learning rate: lr=2.5e-05.
===> Epoch[290](220/324): Loss: 0.6160 || Learning rate: lr=2.5e-05.
===> Epoch[290](230/324): Loss: 0.6640 || Learning rate: lr=2.5e-05.
===> Epoch[290](240/324): Loss: 0.8953 || Learning rate: lr=2.5e-05.
===> Epoch[290](250/324): Loss: 1.0656 || Learning rate: lr=2.5e-05.
===> Epoch[290](260/324): Loss: 0.3890 || Learning rate: lr=2.5e-05.
===> Epoch[290](270/324): Loss: 0.7652 || Learning rate: lr=2.5e-05.
===> Epoch[290](280/324): Loss: 0.8251 || Learning rate: lr=2.5e-05.
===> Epoch[290](290/324): Loss: 0.5629 || Learning rate: lr=2.5e-05.
===> Epoch[290](300/324): Loss: 0.7790 || Learning rate: lr=2.5e-05.
===> Epoch[290](310/324): Loss: 0.7734 || Learning rate: lr=2.5e-05.
===> Epoch[290](320/324): Loss: 0.6395 || Learning rate: lr=2.5e-05.
===> Epoch[291](10/324): Loss: 0.6318 || Learning rate: lr=2.5e-05.
===> Epoch[291](20/324): Loss: 0.7979 || Learning rate: lr=2.5e-05.
===> Epoch[291](30/324): Loss: 0.7307 || Learning rate: lr=2.5e-05.
===> Epoch[291](40/324): Loss: 0.6536 || Learning rate: lr=2.5e-05.
===> Epoch[291](50/324): Loss: 0.5448 || Learning rate: lr=2.5e-05.
===> Epoch[291](60/324): Loss: 0.5836 || Learning rate: lr=2.5e-05.
===> Epoch[291](70/324): Loss: 0.5603 || Learning rate: lr=2.5e-05.
===> Epoch[291](80/324): Loss: 0.7551 || Learning rate: lr=2.5e-05.
===> Epoch[291](90/324): Loss: 0.6764 || Learning rate: lr=2.5e-05.
===> Epoch[291](100/324): Loss: 0.7377 || Learning rate: lr=2.5e-05.
===> Epoch[291](110/324): Loss: 0.5605 || Learning rate: lr=2.5e-05.
===> Epoch[291](120/324): Loss: 0.7215 || Learning rate: lr=2.5e-05.
===> Epoch[291](130/324): Loss: 0.6717 || Learning rate: lr=2.5e-05.
===> Epoch[291](140/324): Loss: 1.0078 || Learning rate: lr=2.5e-05.
===> Epoch[291](150/324): Loss: 0.4012 || Learning rate: lr=2.5e-05.
===> Epoch[291](160/324): Loss: 0.7877 || Learning rate: lr=2.5e-05.
===> Epoch[291](170/324): Loss: 0.5711 || Learning rate: lr=2.5e-05.
===> Epoch[291](180/324): Loss: 0.5500 || Learning rate: lr=2.5e-05.
===> Epoch[291](190/324): Loss: 0.7422 || Learning rate: lr=2.5e-05.
===> Epoch[291](200/324): Loss: 0.5899 || Learning rate: lr=2.5e-05.
===> Epoch[291](210/324): Loss: 0.5758 || Learning rate: lr=2.5e-05.
===> Epoch[291](220/324): Loss: 0.5911 || Learning rate: lr=2.5e-05.
===> Epoch[291](230/324): Loss: 0.3725 || Learning rate: lr=2.5e-05.
===> Epoch[291](240/324): Loss: 0.4887 || Learning rate: lr=2.5e-05.
===> Epoch[291](250/324): Loss: 0.6631 || Learning rate: lr=2.5e-05.
===> Epoch[291](260/324): Loss: 0.8312 || Learning rate: lr=2.5e-05.
===> Epoch[291](270/324): Loss: 0.9583 || Learning rate: lr=2.5e-05.
===> Epoch[291](280/324): Loss: 0.5083 || Learning rate: lr=2.5e-05.
===> Epoch[291](290/324): Loss: 0.7165 || Learning rate: lr=2.5e-05.
===> Epoch[291](300/324): Loss: 0.7829 || Learning rate: lr=2.5e-05.
===> Epoch[291](310/324): Loss: 0.6001 || Learning rate: lr=2.5e-05.
===> Epoch[291](320/324): Loss: 0.7939 || Learning rate: lr=2.5e-05.
===> Epoch[292](10/324): Loss: 0.8118 || Learning rate: lr=2.5e-05.
===> Epoch[292](20/324): Loss: 0.5490 || Learning rate: lr=2.5e-05.
===> Epoch[292](30/324): Loss: 0.4868 || Learning rate: lr=2.5e-05.
===> Epoch[292](40/324): Loss: 0.7471 || Learning rate: lr=2.5e-05.
===> Epoch[292](50/324): Loss: 0.6980 || Learning rate: lr=2.5e-05.
===> Epoch[292](60/324): Loss: 0.5675 || Learning rate: lr=2.5e-05.
===> Epoch[292](70/324): Loss: 0.8436 || Learning rate: lr=2.5e-05.
===> Epoch[292](80/324): Loss: 0.6945 || Learning rate: lr=2.5e-05.
===> Epoch[292](90/324): Loss: 0.5972 || Learning rate: lr=2.5e-05.
===> Epoch[292](100/324): Loss: 0.6069 || Learning rate: lr=2.5e-05.
===> Epoch[292](110/324): Loss: 0.6582 || Learning rate: lr=2.5e-05.
===> Epoch[292](120/324): Loss: 0.5831 || Learning rate: lr=2.5e-05.
===> Epoch[292](130/324): Loss: 0.5584 || Learning rate: lr=2.5e-05.
===> Epoch[292](140/324): Loss: 0.9216 || Learning rate: lr=2.5e-05.
===> Epoch[292](150/324): Loss: 0.5721 || Learning rate: lr=2.5e-05.
===> Epoch[292](160/324): Loss: 0.9420 || Learning rate: lr=2.5e-05.
===> Epoch[292](170/324): Loss: 0.7152 || Learning rate: lr=2.5e-05.
===> Epoch[292](180/324): Loss: 0.7324 || Learning rate: lr=2.5e-05.
===> Epoch[292](190/324): Loss: 0.4239 || Learning rate: lr=2.5e-05.
===> Epoch[292](200/324): Loss: 0.5711 || Learning rate: lr=2.5e-05.
===> Epoch[292](210/324): Loss: 0.4528 || Learning rate: lr=2.5e-05.
===> Epoch[292](220/324): Loss: 0.7473 || Learning rate: lr=2.5e-05.
===> Epoch[292](230/324): Loss: 0.6120 || Learning rate: lr=2.5e-05.
===> Epoch[292](240/324): Loss: 0.4959 || Learning rate: lr=2.5e-05.
===> Epoch[292](250/324): Loss: 0.6618 || Learning rate: lr=2.5e-05.
===> Epoch[292](260/324): Loss: 0.7358 || Learning rate: lr=2.5e-05.
===> Epoch[292](270/324): Loss: 0.6631 || Learning rate: lr=2.5e-05.
===> Epoch[292](280/324): Loss: 0.6321 || Learning rate: lr=2.5e-05.
===> Epoch[292](290/324): Loss: 0.9109 || Learning rate: lr=2.5e-05.
===> Epoch[292](300/324): Loss: 0.4834 || Learning rate: lr=2.5e-05.
===> Epoch[292](310/324): Loss: 0.7649 || Learning rate: lr=2.5e-05.
===> Epoch[292](320/324): Loss: 0.8323 || Learning rate: lr=2.5e-05.
===> Epoch[293](10/324): Loss: 0.8395 || Learning rate: lr=2.5e-05.
===> Epoch[293](20/324): Loss: 1.0991 || Learning rate: lr=2.5e-05.
===> Epoch[293](30/324): Loss: 0.7040 || Learning rate: lr=2.5e-05.
===> Epoch[293](40/324): Loss: 0.6098 || Learning rate: lr=2.5e-05.
===> Epoch[293](50/324): Loss: 0.7303 || Learning rate: lr=2.5e-05.
===> Epoch[293](60/324): Loss: 0.7269 || Learning rate: lr=2.5e-05.
===> Epoch[293](70/324): Loss: 0.5618 || Learning rate: lr=2.5e-05.
===> Epoch[293](80/324): Loss: 0.5914 || Learning rate: lr=2.5e-05.
===> Epoch[293](90/324): Loss: 0.4236 || Learning rate: lr=2.5e-05.
===> Epoch[293](100/324): Loss: 0.5866 || Learning rate: lr=2.5e-05.
===> Epoch[293](110/324): Loss: 0.4811 || Learning rate: lr=2.5e-05.
===> Epoch[293](120/324): Loss: 0.8895 || Learning rate: lr=2.5e-05.
===> Epoch[293](130/324): Loss: 0.5387 || Learning rate: lr=2.5e-05.
===> Epoch[293](140/324): Loss: 0.6732 || Learning rate: lr=2.5e-05.
===> Epoch[293](150/324): Loss: 0.5078 || Learning rate: lr=2.5e-05.
===> Epoch[293](160/324): Loss: 0.6907 || Learning rate: lr=2.5e-05.
===> Epoch[293](170/324): Loss: 0.9333 || Learning rate: lr=2.5e-05.
===> Epoch[293](180/324): Loss: 0.5302 || Learning rate: lr=2.5e-05.
===> Epoch[293](190/324): Loss: 0.6866 || Learning rate: lr=2.5e-05.
===> Epoch[293](200/324): Loss: 0.7574 || Learning rate: lr=2.5e-05.
===> Epoch[293](210/324): Loss: 0.8116 || Learning rate: lr=2.5e-05.
===> Epoch[293](220/324): Loss: 0.8068 || Learning rate: lr=2.5e-05.
===> Epoch[293](230/324): Loss: 0.6406 || Learning rate: lr=2.5e-05.
===> Epoch[293](240/324): Loss: 0.4950 || Learning rate: lr=2.5e-05.
===> Epoch[293](250/324): Loss: 0.6575 || Learning rate: lr=2.5e-05.
===> Epoch[293](260/324): Loss: 0.5256 || Learning rate: lr=2.5e-05.
===> Epoch[293](270/324): Loss: 0.9991 || Learning rate: lr=2.5e-05.
===> Epoch[293](280/324): Loss: 0.7236 || Learning rate: lr=2.5e-05.
===> Epoch[293](290/324): Loss: 0.6165 || Learning rate: lr=2.5e-05.
===> Epoch[293](300/324): Loss: 0.6823 || Learning rate: lr=2.5e-05.
===> Epoch[293](310/324): Loss: 0.6608 || Learning rate: lr=2.5e-05.
===> Epoch[293](320/324): Loss: 0.5674 || Learning rate: lr=2.5e-05.
===> Epoch[294](10/324): Loss: 0.5662 || Learning rate: lr=2.5e-05.
===> Epoch[294](20/324): Loss: 0.6600 || Learning rate: lr=2.5e-05.
===> Epoch[294](30/324): Loss: 1.1649 || Learning rate: lr=2.5e-05.
===> Epoch[294](40/324): Loss: 0.6767 || Learning rate: lr=2.5e-05.
===> Epoch[294](50/324): Loss: 0.8002 || Learning rate: lr=2.5e-05.
===> Epoch[294](60/324): Loss: 0.9352 || Learning rate: lr=2.5e-05.
===> Epoch[294](70/324): Loss: 1.0957 || Learning rate: lr=2.5e-05.
===> Epoch[294](80/324): Loss: 0.7812 || Learning rate: lr=2.5e-05.
===> Epoch[294](90/324): Loss: 0.7554 || Learning rate: lr=2.5e-05.
===> Epoch[294](100/324): Loss: 0.6501 || Learning rate: lr=2.5e-05.
===> Epoch[294](110/324): Loss: 0.6215 || Learning rate: lr=2.5e-05.
===> Epoch[294](120/324): Loss: 0.6002 || Learning rate: lr=2.5e-05.
===> Epoch[294](130/324): Loss: 0.5748 || Learning rate: lr=2.5e-05.
===> Epoch[294](140/324): Loss: 0.7391 || Learning rate: lr=2.5e-05.
===> Epoch[294](150/324): Loss: 0.8439 || Learning rate: lr=2.5e-05.
===> Epoch[294](160/324): Loss: 0.8647 || Learning rate: lr=2.5e-05.
===> Epoch[294](170/324): Loss: 0.7366 || Learning rate: lr=2.5e-05.
===> Epoch[294](180/324): Loss: 0.7828 || Learning rate: lr=2.5e-05.
===> Epoch[294](190/324): Loss: 0.6617 || Learning rate: lr=2.5e-05.
===> Epoch[294](200/324): Loss: 0.9078 || Learning rate: lr=2.5e-05.
===> Epoch[294](210/324): Loss: 0.6917 || Learning rate: lr=2.5e-05.
===> Epoch[294](220/324): Loss: 0.6715 || Learning rate: lr=2.5e-05.
===> Epoch[294](230/324): Loss: 0.5843 || Learning rate: lr=2.5e-05.
===> Epoch[294](240/324): Loss: 0.4975 || Learning rate: lr=2.5e-05.
===> Epoch[294](250/324): Loss: 0.6007 || Learning rate: lr=2.5e-05.
===> Epoch[294](260/324): Loss: 0.5178 || Learning rate: lr=2.5e-05.
===> Epoch[294](270/324): Loss: 0.9699 || Learning rate: lr=2.5e-05.
===> Epoch[294](280/324): Loss: 0.9975 || Learning rate: lr=2.5e-05.
===> Epoch[294](290/324): Loss: 0.6055 || Learning rate: lr=2.5e-05.
===> Epoch[294](300/324): Loss: 0.6278 || Learning rate: lr=2.5e-05.
===> Epoch[294](310/324): Loss: 0.5897 || Learning rate: lr=2.5e-05.
===> Epoch[294](320/324): Loss: 0.8073 || Learning rate: lr=2.5e-05.
===> Epoch[295](10/324): Loss: 0.9967 || Learning rate: lr=2.5e-05.
===> Epoch[295](20/324): Loss: 0.5669 || Learning rate: lr=2.5e-05.
===> Epoch[295](30/324): Loss: 0.5941 || Learning rate: lr=2.5e-05.
===> Epoch[295](40/324): Loss: 0.5183 || Learning rate: lr=2.5e-05.
===> Epoch[295](50/324): Loss: 0.6491 || Learning rate: lr=2.5e-05.
===> Epoch[295](60/324): Loss: 0.5025 || Learning rate: lr=2.5e-05.
===> Epoch[295](70/324): Loss: 0.5510 || Learning rate: lr=2.5e-05.
===> Epoch[295](80/324): Loss: 0.6327 || Learning rate: lr=2.5e-05.
===> Epoch[295](90/324): Loss: 0.4215 || Learning rate: lr=2.5e-05.
===> Epoch[295](100/324): Loss: 0.8672 || Learning rate: lr=2.5e-05.
===> Epoch[295](110/324): Loss: 0.8556 || Learning rate: lr=2.5e-05.
===> Epoch[295](120/324): Loss: 0.7222 || Learning rate: lr=2.5e-05.
===> Epoch[295](130/324): Loss: 0.5950 || Learning rate: lr=2.5e-05.
===> Epoch[295](140/324): Loss: 0.8124 || Learning rate: lr=2.5e-05.
===> Epoch[295](150/324): Loss: 0.6218 || Learning rate: lr=2.5e-05.
===> Epoch[295](160/324): Loss: 0.5180 || Learning rate: lr=2.5e-05.
===> Epoch[295](170/324): Loss: 0.8873 || Learning rate: lr=2.5e-05.
===> Epoch[295](180/324): Loss: 0.6173 || Learning rate: lr=2.5e-05.
===> Epoch[295](190/324): Loss: 0.5047 || Learning rate: lr=2.5e-05.
===> Epoch[295](200/324): Loss: 0.5053 || Learning rate: lr=2.5e-05.
===> Epoch[295](210/324): Loss: 0.6838 || Learning rate: lr=2.5e-05.
===> Epoch[295](220/324): Loss: 0.6274 || Learning rate: lr=2.5e-05.
===> Epoch[295](230/324): Loss: 0.5744 || Learning rate: lr=2.5e-05.
===> Epoch[295](240/324): Loss: 0.8563 || Learning rate: lr=2.5e-05.
===> Epoch[295](250/324): Loss: 0.4777 || Learning rate: lr=2.5e-05.
===> Epoch[295](260/324): Loss: 0.4863 || Learning rate: lr=2.5e-05.
===> Epoch[295](270/324): Loss: 0.6367 || Learning rate: lr=2.5e-05.
===> Epoch[295](280/324): Loss: 0.6215 || Learning rate: lr=2.5e-05.
===> Epoch[295](290/324): Loss: 0.8615 || Learning rate: lr=2.5e-05.
===> Epoch[295](300/324): Loss: 0.9207 || Learning rate: lr=2.5e-05.
===> Epoch[295](310/324): Loss: 0.6490 || Learning rate: lr=2.5e-05.
===> Epoch[295](320/324): Loss: 0.9487 || Learning rate: lr=2.5e-05.
===> Epoch[296](10/324): Loss: 0.7622 || Learning rate: lr=2.5e-05.
===> Epoch[296](20/324): Loss: 0.3386 || Learning rate: lr=2.5e-05.
===> Epoch[296](30/324): Loss: 0.6484 || Learning rate: lr=2.5e-05.
===> Epoch[296](40/324): Loss: 0.5693 || Learning rate: lr=2.5e-05.
===> Epoch[296](50/324): Loss: 0.6253 || Learning rate: lr=2.5e-05.
===> Epoch[296](60/324): Loss: 0.8255 || Learning rate: lr=2.5e-05.
===> Epoch[296](70/324): Loss: 0.5209 || Learning rate: lr=2.5e-05.
===> Epoch[296](80/324): Loss: 0.7253 || Learning rate: lr=2.5e-05.
===> Epoch[296](90/324): Loss: 0.6245 || Learning rate: lr=2.5e-05.
===> Epoch[296](100/324): Loss: 0.6799 || Learning rate: lr=2.5e-05.
===> Epoch[296](110/324): Loss: 0.8019 || Learning rate: lr=2.5e-05.
===> Epoch[296](120/324): Loss: 0.6285 || Learning rate: lr=2.5e-05.
===> Epoch[296](130/324): Loss: 0.6503 || Learning rate: lr=2.5e-05.
===> Epoch[296](140/324): Loss: 0.6484 || Learning rate: lr=2.5e-05.
===> Epoch[296](150/324): Loss: 0.4304 || Learning rate: lr=2.5e-05.
===> Epoch[296](160/324): Loss: 0.4565 || Learning rate: lr=2.5e-05.
===> Epoch[296](170/324): Loss: 0.6713 || Learning rate: lr=2.5e-05.
===> Epoch[296](180/324): Loss: 0.9288 || Learning rate: lr=2.5e-05.
===> Epoch[296](190/324): Loss: 0.7496 || Learning rate: lr=2.5e-05.
===> Epoch[296](200/324): Loss: 0.6057 || Learning rate: lr=2.5e-05.
===> Epoch[296](210/324): Loss: 0.5826 || Learning rate: lr=2.5e-05.
===> Epoch[296](220/324): Loss: 0.7776 || Learning rate: lr=2.5e-05.
===> Epoch[296](230/324): Loss: 1.0030 || Learning rate: lr=2.5e-05.
===> Epoch[296](240/324): Loss: 0.4479 || Learning rate: lr=2.5e-05.
===> Epoch[296](250/324): Loss: 0.5648 || Learning rate: lr=2.5e-05.
===> Epoch[296](260/324): Loss: 0.5346 || Learning rate: lr=2.5e-05.
===> Epoch[296](270/324): Loss: 0.7935 || Learning rate: lr=2.5e-05.
===> Epoch[296](280/324): Loss: 0.7421 || Learning rate: lr=2.5e-05.
===> Epoch[296](290/324): Loss: 0.5786 || Learning rate: lr=2.5e-05.
===> Epoch[296](300/324): Loss: 0.8158 || Learning rate: lr=2.5e-05.
===> Epoch[296](310/324): Loss: 0.6714 || Learning rate: lr=2.5e-05.
===> Epoch[296](320/324): Loss: 0.8301 || Learning rate: lr=2.5e-05.
===> Epoch[297](10/324): Loss: 0.5832 || Learning rate: lr=2.5e-05.
===> Epoch[297](20/324): Loss: 0.4442 || Learning rate: lr=2.5e-05.
===> Epoch[297](30/324): Loss: 0.4864 || Learning rate: lr=2.5e-05.
===> Epoch[297](40/324): Loss: 0.9679 || Learning rate: lr=2.5e-05.
===> Epoch[297](50/324): Loss: 0.6257 || Learning rate: lr=2.5e-05.
===> Epoch[297](60/324): Loss: 0.8757 || Learning rate: lr=2.5e-05.
===> Epoch[297](70/324): Loss: 0.6878 || Learning rate: lr=2.5e-05.
===> Epoch[297](80/324): Loss: 0.6421 || Learning rate: lr=2.5e-05.
===> Epoch[297](90/324): Loss: 0.4515 || Learning rate: lr=2.5e-05.
===> Epoch[297](100/324): Loss: 0.6997 || Learning rate: lr=2.5e-05.
===> Epoch[297](110/324): Loss: 0.5837 || Learning rate: lr=2.5e-05.
===> Epoch[297](120/324): Loss: 0.7471 || Learning rate: lr=2.5e-05.
===> Epoch[297](130/324): Loss: 0.5525 || Learning rate: lr=2.5e-05.
===> Epoch[297](140/324): Loss: 0.5955 || Learning rate: lr=2.5e-05.
===> Epoch[297](150/324): Loss: 0.4325 || Learning rate: lr=2.5e-05.
===> Epoch[297](160/324): Loss: 1.0975 || Learning rate: lr=2.5e-05.
===> Epoch[297](170/324): Loss: 0.6727 || Learning rate: lr=2.5e-05.
===> Epoch[297](180/324): Loss: 0.5801 || Learning rate: lr=2.5e-05.
===> Epoch[297](190/324): Loss: 1.0570 || Learning rate: lr=2.5e-05.
===> Epoch[297](200/324): Loss: 0.4251 || Learning rate: lr=2.5e-05.
===> Epoch[297](210/324): Loss: 0.7765 || Learning rate: lr=2.5e-05.
===> Epoch[297](220/324): Loss: 0.6623 || Learning rate: lr=2.5e-05.
===> Epoch[297](230/324): Loss: 1.1701 || Learning rate: lr=2.5e-05.
===> Epoch[297](240/324): Loss: 0.3708 || Learning rate: lr=2.5e-05.
===> Epoch[297](250/324): Loss: 0.8497 || Learning rate: lr=2.5e-05.
===> Epoch[297](260/324): Loss: 0.9226 || Learning rate: lr=2.5e-05.
===> Epoch[297](270/324): Loss: 0.5394 || Learning rate: lr=2.5e-05.
===> Epoch[297](280/324): Loss: 0.8403 || Learning rate: lr=2.5e-05.
===> Epoch[297](290/324): Loss: 0.4452 || Learning rate: lr=2.5e-05.
===> Epoch[297](300/324): Loss: 0.5274 || Learning rate: lr=2.5e-05.
===> Epoch[297](310/324): Loss: 0.6074 || Learning rate: lr=2.5e-05.
===> Epoch[297](320/324): Loss: 0.6164 || Learning rate: lr=2.5e-05.
===> Epoch[298](10/324): Loss: 0.8323 || Learning rate: lr=2.5e-05.
===> Epoch[298](20/324): Loss: 0.8024 || Learning rate: lr=2.5e-05.
===> Epoch[298](30/324): Loss: 0.7421 || Learning rate: lr=2.5e-05.
===> Epoch[298](40/324): Loss: 0.5886 || Learning rate: lr=2.5e-05.
===> Epoch[298](50/324): Loss: 0.6629 || Learning rate: lr=2.5e-05.
===> Epoch[298](60/324): Loss: 0.5682 || Learning rate: lr=2.5e-05.
===> Epoch[298](70/324): Loss: 0.6820 || Learning rate: lr=2.5e-05.
===> Epoch[298](80/324): Loss: 0.4998 || Learning rate: lr=2.5e-05.
===> Epoch[298](90/324): Loss: 0.5593 || Learning rate: lr=2.5e-05.
===> Epoch[298](100/324): Loss: 0.7411 || Learning rate: lr=2.5e-05.
===> Epoch[298](110/324): Loss: 0.7453 || Learning rate: lr=2.5e-05.
===> Epoch[298](120/324): Loss: 0.9170 || Learning rate: lr=2.5e-05.
===> Epoch[298](130/324): Loss: 0.8515 || Learning rate: lr=2.5e-05.
===> Epoch[298](140/324): Loss: 0.5135 || Learning rate: lr=2.5e-05.
===> Epoch[298](150/324): Loss: 0.6100 || Learning rate: lr=2.5e-05.
===> Epoch[298](160/324): Loss: 0.5033 || Learning rate: lr=2.5e-05.
===> Epoch[298](170/324): Loss: 0.6301 || Learning rate: lr=2.5e-05.
===> Epoch[298](180/324): Loss: 0.4683 || Learning rate: lr=2.5e-05.
===> Epoch[298](190/324): Loss: 0.6985 || Learning rate: lr=2.5e-05.
===> Epoch[298](200/324): Loss: 0.5316 || Learning rate: lr=2.5e-05.
===> Epoch[298](210/324): Loss: 0.8285 || Learning rate: lr=2.5e-05.
===> Epoch[298](220/324): Loss: 0.7234 || Learning rate: lr=2.5e-05.
===> Epoch[298](230/324): Loss: 0.5953 || Learning rate: lr=2.5e-05.
===> Epoch[298](240/324): Loss: 0.7375 || Learning rate: lr=2.5e-05.
===> Epoch[298](250/324): Loss: 0.9710 || Learning rate: lr=2.5e-05.
===> Epoch[298](260/324): Loss: 1.5443 || Learning rate: lr=2.5e-05.
===> Epoch[298](270/324): Loss: 0.4279 || Learning rate: lr=2.5e-05.
===> Epoch[298](280/324): Loss: 0.6543 || Learning rate: lr=2.5e-05.
===> Epoch[298](290/324): Loss: 0.8163 || Learning rate: lr=2.5e-05.
===> Epoch[298](300/324): Loss: 0.5756 || Learning rate: lr=2.5e-05.
===> Epoch[298](310/324): Loss: 0.7246 || Learning rate: lr=2.5e-05.
===> Epoch[298](320/324): Loss: 0.5293 || Learning rate: lr=2.5e-05.
===> Epoch[299](10/324): Loss: 0.5275 || Learning rate: lr=2.5e-05.
===> Epoch[299](20/324): Loss: 0.6690 || Learning rate: lr=2.5e-05.
===> Epoch[299](30/324): Loss: 0.8755 || Learning rate: lr=2.5e-05.
===> Epoch[299](40/324): Loss: 0.6884 || Learning rate: lr=2.5e-05.
===> Epoch[299](50/324): Loss: 0.8266 || Learning rate: lr=2.5e-05.
===> Epoch[299](60/324): Loss: 0.9850 || Learning rate: lr=2.5e-05.
===> Epoch[299](70/324): Loss: 0.7069 || Learning rate: lr=2.5e-05.
===> Epoch[299](80/324): Loss: 0.5730 || Learning rate: lr=2.5e-05.
===> Epoch[299](90/324): Loss: 0.8365 || Learning rate: lr=2.5e-05.
===> Epoch[299](100/324): Loss: 0.7054 || Learning rate: lr=2.5e-05.
===> Epoch[299](110/324): Loss: 0.6446 || Learning rate: lr=2.5e-05.
===> Epoch[299](120/324): Loss: 0.6212 || Learning rate: lr=2.5e-05.
===> Epoch[299](130/324): Loss: 0.6947 || Learning rate: lr=2.5e-05.
===> Epoch[299](140/324): Loss: 0.8757 || Learning rate: lr=2.5e-05.
===> Epoch[299](150/324): Loss: 0.6711 || Learning rate: lr=2.5e-05.
===> Epoch[299](160/324): Loss: 0.7672 || Learning rate: lr=2.5e-05.
===> Epoch[299](170/324): Loss: 0.5878 || Learning rate: lr=2.5e-05.
===> Epoch[299](180/324): Loss: 0.5457 || Learning rate: lr=2.5e-05.
===> Epoch[299](190/324): Loss: 0.3649 || Learning rate: lr=2.5e-05.
===> Epoch[299](200/324): Loss: 0.5358 || Learning rate: lr=2.5e-05.
===> Epoch[299](210/324): Loss: 0.9774 || Learning rate: lr=2.5e-05.
===> Epoch[299](220/324): Loss: 0.5802 || Learning rate: lr=2.5e-05.
===> Epoch[299](230/324): Loss: 0.8168 || Learning rate: lr=2.5e-05.
===> Epoch[299](240/324): Loss: 0.5530 || Learning rate: lr=2.5e-05.
===> Epoch[299](250/324): Loss: 0.7938 || Learning rate: lr=2.5e-05.
===> Epoch[299](260/324): Loss: 0.4777 || Learning rate: lr=2.5e-05.
===> Epoch[299](270/324): Loss: 0.5179 || Learning rate: lr=2.5e-05.
===> Epoch[299](280/324): Loss: 0.7206 || Learning rate: lr=2.5e-05.
===> Epoch[299](290/324): Loss: 0.6906 || Learning rate: lr=2.5e-05.
===> Epoch[299](300/324): Loss: 0.7234 || Learning rate: lr=2.5e-05.
===> Epoch[299](310/324): Loss: 0.7907 || Learning rate: lr=2.5e-05.
===> Epoch[299](320/324): Loss: 0.6872 || Learning rate: lr=2.5e-05.
===> Epoch[300](10/324): Loss: 0.5569 || Learning rate: lr=2.5e-05.
===> Epoch[300](20/324): Loss: 0.9453 || Learning rate: lr=2.5e-05.
===> Epoch[300](30/324): Loss: 1.1604 || Learning rate: lr=2.5e-05.
===> Epoch[300](40/324): Loss: 0.7141 || Learning rate: lr=2.5e-05.
===> Epoch[300](50/324): Loss: 0.5139 || Learning rate: lr=2.5e-05.
===> Epoch[300](60/324): Loss: 0.6552 || Learning rate: lr=2.5e-05.
===> Epoch[300](70/324): Loss: 1.0538 || Learning rate: lr=2.5e-05.
===> Epoch[300](80/324): Loss: 0.6749 || Learning rate: lr=2.5e-05.
===> Epoch[300](90/324): Loss: 0.7102 || Learning rate: lr=2.5e-05.
===> Epoch[300](100/324): Loss: 0.8036 || Learning rate: lr=2.5e-05.
===> Epoch[300](110/324): Loss: 0.8300 || Learning rate: lr=2.5e-05.
===> Epoch[300](120/324): Loss: 0.7656 || Learning rate: lr=2.5e-05.
===> Epoch[300](130/324): Loss: 0.4704 || Learning rate: lr=2.5e-05.
===> Epoch[300](140/324): Loss: 0.5336 || Learning rate: lr=2.5e-05.
===> Epoch[300](150/324): Loss: 1.0477 || Learning rate: lr=2.5e-05.
===> Epoch[300](160/324): Loss: 0.5841 || Learning rate: lr=2.5e-05.
===> Epoch[300](170/324): Loss: 0.6548 || Learning rate: lr=2.5e-05.
===> Epoch[300](180/324): Loss: 0.7945 || Learning rate: lr=2.5e-05.
===> Epoch[300](190/324): Loss: 0.4469 || Learning rate: lr=2.5e-05.
===> Epoch[300](200/324): Loss: 0.8040 || Learning rate: lr=2.5e-05.
===> Epoch[300](210/324): Loss: 0.5200 || Learning rate: lr=2.5e-05.
===> Epoch[300](220/324): Loss: 0.5298 || Learning rate: lr=2.5e-05.
===> Epoch[300](230/324): Loss: 0.8535 || Learning rate: lr=2.5e-05.
===> Epoch[300](240/324): Loss: 0.7965 || Learning rate: lr=2.5e-05.
===> Epoch[300](250/324): Loss: 0.6493 || Learning rate: lr=2.5e-05.
===> Epoch[300](260/324): Loss: 0.7732 || Learning rate: lr=2.5e-05.
===> Epoch[300](270/324): Loss: 0.7221 || Learning rate: lr=2.5e-05.
===> Epoch[300](280/324): Loss: 0.5340 || Learning rate: lr=2.5e-05.
===> Epoch[300](290/324): Loss: 0.6854 || Learning rate: lr=2.5e-05.
===> Epoch[300](300/324): Loss: 0.7139 || Learning rate: lr=2.5e-05.
===> Epoch[300](310/324): Loss: 0.6225 || Learning rate: lr=2.5e-05.
===> Epoch[300](320/324): Loss: 0.4031 || Learning rate: lr=2.5e-05.
Checkpoint saved to weights/epoch_v2_300.pth
===> Epoch[301](10/324): Loss: 0.6847 || Learning rate: lr=1.25e-05.
===> Epoch[301](20/324): Loss: 0.7819 || Learning rate: lr=1.25e-05.
===> Epoch[301](30/324): Loss: 0.9572 || Learning rate: lr=1.25e-05.
===> Epoch[301](40/324): Loss: 0.7550 || Learning rate: lr=1.25e-05.
===> Epoch[301](50/324): Loss: 0.4706 || Learning rate: lr=1.25e-05.
===> Epoch[301](60/324): Loss: 0.9603 || Learning rate: lr=1.25e-05.
===> Epoch[301](70/324): Loss: 0.8785 || Learning rate: lr=1.25e-05.
===> Epoch[301](80/324): Loss: 0.7935 || Learning rate: lr=1.25e-05.
===> Epoch[301](90/324): Loss: 0.4653 || Learning rate: lr=1.25e-05.
===> Epoch[301](100/324): Loss: 0.5606 || Learning rate: lr=1.25e-05.
===> Epoch[301](110/324): Loss: 0.5416 || Learning rate: lr=1.25e-05.
===> Epoch[301](120/324): Loss: 0.6854 || Learning rate: lr=1.25e-05.
===> Epoch[301](130/324): Loss: 0.8620 || Learning rate: lr=1.25e-05.
===> Epoch[301](140/324): Loss: 0.8218 || Learning rate: lr=1.25e-05.
===> Epoch[301](150/324): Loss: 0.6358 || Learning rate: lr=1.25e-05.
===> Epoch[301](160/324): Loss: 0.6408 || Learning rate: lr=1.25e-05.
===> Epoch[301](170/324): Loss: 1.0580 || Learning rate: lr=1.25e-05.
===> Epoch[301](180/324): Loss: 0.6910 || Learning rate: lr=1.25e-05.
===> Epoch[301](190/324): Loss: 0.5790 || Learning rate: lr=1.25e-05.
===> Epoch[301](200/324): Loss: 0.4989 || Learning rate: lr=1.25e-05.
===> Epoch[301](210/324): Loss: 0.4738 || Learning rate: lr=1.25e-05.
===> Epoch[301](220/324): Loss: 0.6661 || Learning rate: lr=1.25e-05.
===> Epoch[301](230/324): Loss: 0.7856 || Learning rate: lr=1.25e-05.
===> Epoch[301](240/324): Loss: 0.5708 || Learning rate: lr=1.25e-05.
===> Epoch[301](250/324): Loss: 0.7588 || Learning rate: lr=1.25e-05.
===> Epoch[301](260/324): Loss: 0.6578 || Learning rate: lr=1.25e-05.
===> Epoch[301](270/324): Loss: 0.5835 || Learning rate: lr=1.25e-05.
===> Epoch[301](280/324): Loss: 0.5369 || Learning rate: lr=1.25e-05.
===> Epoch[301](290/324): Loss: 0.4315 || Learning rate: lr=1.25e-05.
===> Epoch[301](300/324): Loss: 0.6710 || Learning rate: lr=1.25e-05.
===> Epoch[301](310/324): Loss: 0.7279 || Learning rate: lr=1.25e-05.
===> Epoch[301](320/324): Loss: 0.6290 || Learning rate: lr=1.25e-05.
===> Epoch[302](10/324): Loss: 0.6525 || Learning rate: lr=1.25e-05.
===> Epoch[302](20/324): Loss: 0.6344 || Learning rate: lr=1.25e-05.
===> Epoch[302](30/324): Loss: 0.4781 || Learning rate: lr=1.25e-05.
===> Epoch[302](40/324): Loss: 0.6236 || Learning rate: lr=1.25e-05.
===> Epoch[302](50/324): Loss: 0.5895 || Learning rate: lr=1.25e-05.
===> Epoch[302](60/324): Loss: 0.6777 || Learning rate: lr=1.25e-05.
===> Epoch[302](70/324): Loss: 0.8451 || Learning rate: lr=1.25e-05.
===> Epoch[302](80/324): Loss: 0.6338 || Learning rate: lr=1.25e-05.
===> Epoch[302](90/324): Loss: 0.5605 || Learning rate: lr=1.25e-05.
===> Epoch[302](100/324): Loss: 0.6684 || Learning rate: lr=1.25e-05.
===> Epoch[302](110/324): Loss: 0.8011 || Learning rate: lr=1.25e-05.
===> Epoch[302](120/324): Loss: 0.6979 || Learning rate: lr=1.25e-05.
===> Epoch[302](130/324): Loss: 0.7132 || Learning rate: lr=1.25e-05.
===> Epoch[302](140/324): Loss: 0.7796 || Learning rate: lr=1.25e-05.
===> Epoch[302](150/324): Loss: 0.9234 || Learning rate: lr=1.25e-05.
===> Epoch[302](160/324): Loss: 0.8443 || Learning rate: lr=1.25e-05.
===> Epoch[302](170/324): Loss: 0.6485 || Learning rate: lr=1.25e-05.
===> Epoch[302](180/324): Loss: 0.6252 || Learning rate: lr=1.25e-05.
===> Epoch[302](190/324): Loss: 0.5967 || Learning rate: lr=1.25e-05.
===> Epoch[302](200/324): Loss: 0.6044 || Learning rate: lr=1.25e-05.
===> Epoch[302](210/324): Loss: 0.5595 || Learning rate: lr=1.25e-05.
===> Epoch[302](220/324): Loss: 0.5961 || Learning rate: lr=1.25e-05.
===> Epoch[302](230/324): Loss: 1.2696 || Learning rate: lr=1.25e-05.
===> Epoch[302](240/324): Loss: 0.4650 || Learning rate: lr=1.25e-05.
===> Epoch[302](250/324): Loss: 0.7391 || Learning rate: lr=1.25e-05.
===> Epoch[302](260/324): Loss: 0.6005 || Learning rate: lr=1.25e-05.
===> Epoch[302](270/324): Loss: 0.6418 || Learning rate: lr=1.25e-05.
===> Epoch[302](280/324): Loss: 0.4579 || Learning rate: lr=1.25e-05.
===> Epoch[302](290/324): Loss: 0.5310 || Learning rate: lr=1.25e-05.
===> Epoch[302](300/324): Loss: 0.5642 || Learning rate: lr=1.25e-05.
===> Epoch[302](310/324): Loss: 0.7412 || Learning rate: lr=1.25e-05.
===> Epoch[302](320/324): Loss: 0.5361 || Learning rate: lr=1.25e-05.
===> Epoch[303](10/324): Loss: 0.7463 || Learning rate: lr=1.25e-05.
===> Epoch[303](20/324): Loss: 0.7619 || Learning rate: lr=1.25e-05.
===> Epoch[303](30/324): Loss: 0.6048 || Learning rate: lr=1.25e-05.
===> Epoch[303](40/324): Loss: 0.7545 || Learning rate: lr=1.25e-05.
===> Epoch[303](50/324): Loss: 0.8282 || Learning rate: lr=1.25e-05.
===> Epoch[303](60/324): Loss: 0.6350 || Learning rate: lr=1.25e-05.
===> Epoch[303](70/324): Loss: 0.6625 || Learning rate: lr=1.25e-05.
===> Epoch[303](80/324): Loss: 0.4227 || Learning rate: lr=1.25e-05.
===> Epoch[303](90/324): Loss: 0.4680 || Learning rate: lr=1.25e-05.
===> Epoch[303](100/324): Loss: 0.4548 || Learning rate: lr=1.25e-05.
===> Epoch[303](110/324): Loss: 0.6476 || Learning rate: lr=1.25e-05.
===> Epoch[303](120/324): Loss: 0.5732 || Learning rate: lr=1.25e-05.
===> Epoch[303](130/324): Loss: 0.6405 || Learning rate: lr=1.25e-05.
===> Epoch[303](140/324): Loss: 0.6773 || Learning rate: lr=1.25e-05.
===> Epoch[303](150/324): Loss: 0.7032 || Learning rate: lr=1.25e-05.
===> Epoch[303](160/324): Loss: 0.6202 || Learning rate: lr=1.25e-05.
===> Epoch[303](170/324): Loss: 0.5298 || Learning rate: lr=1.25e-05.
===> Epoch[303](180/324): Loss: 0.6762 || Learning rate: lr=1.25e-05.
===> Epoch[303](190/324): Loss: 0.8109 || Learning rate: lr=1.25e-05.
===> Epoch[303](200/324): Loss: 0.4872 || Learning rate: lr=1.25e-05.
===> Epoch[303](210/324): Loss: 0.6247 || Learning rate: lr=1.25e-05.
===> Epoch[303](220/324): Loss: 0.6606 || Learning rate: lr=1.25e-05.
===> Epoch[303](230/324): Loss: 0.6905 || Learning rate: lr=1.25e-05.
===> Epoch[303](240/324): Loss: 0.5370 || Learning rate: lr=1.25e-05.
===> Epoch[303](250/324): Loss: 0.5140 || Learning rate: lr=1.25e-05.
===> Epoch[303](260/324): Loss: 0.6403 || Learning rate: lr=1.25e-05.
===> Epoch[303](270/324): Loss: 0.7959 || Learning rate: lr=1.25e-05.
===> Epoch[303](280/324): Loss: 0.6307 || Learning rate: lr=1.25e-05.
===> Epoch[303](290/324): Loss: 0.6364 || Learning rate: lr=1.25e-05.
===> Epoch[303](300/324): Loss: 0.5979 || Learning rate: lr=1.25e-05.
===> Epoch[303](310/324): Loss: 0.4326 || Learning rate: lr=1.25e-05.
===> Epoch[303](320/324): Loss: 0.7215 || Learning rate: lr=1.25e-05.
===> Epoch[304](10/324): Loss: 0.6217 || Learning rate: lr=1.25e-05.
===> Epoch[304](20/324): Loss: 0.6177 || Learning rate: lr=1.25e-05.
===> Epoch[304](30/324): Loss: 0.7585 || Learning rate: lr=1.25e-05.
===> Epoch[304](40/324): Loss: 0.7134 || Learning rate: lr=1.25e-05.
===> Epoch[304](50/324): Loss: 0.5531 || Learning rate: lr=1.25e-05.
===> Epoch[304](60/324): Loss: 0.5435 || Learning rate: lr=1.25e-05.
===> Epoch[304](70/324): Loss: 0.7360 || Learning rate: lr=1.25e-05.
===> Epoch[304](80/324): Loss: 0.5402 || Learning rate: lr=1.25e-05.
===> Epoch[304](90/324): Loss: 0.7390 || Learning rate: lr=1.25e-05.
===> Epoch[304](100/324): Loss: 0.6472 || Learning rate: lr=1.25e-05.
===> Epoch[304](110/324): Loss: 0.5742 || Learning rate: lr=1.25e-05.
===> Epoch[304](120/324): Loss: 0.8781 || Learning rate: lr=1.25e-05.
===> Epoch[304](130/324): Loss: 0.4457 || Learning rate: lr=1.25e-05.
===> Epoch[304](140/324): Loss: 0.5544 || Learning rate: lr=1.25e-05.
===> Epoch[304](150/324): Loss: 0.7806 || Learning rate: lr=1.25e-05.
===> Epoch[304](160/324): Loss: 0.6554 || Learning rate: lr=1.25e-05.
===> Epoch[304](170/324): Loss: 0.7002 || Learning rate: lr=1.25e-05.
===> Epoch[304](180/324): Loss: 0.7036 || Learning rate: lr=1.25e-05.
===> Epoch[304](190/324): Loss: 0.5235 || Learning rate: lr=1.25e-05.
===> Epoch[304](200/324): Loss: 0.5953 || Learning rate: lr=1.25e-05.
===> Epoch[304](210/324): Loss: 0.7525 || Learning rate: lr=1.25e-05.
===> Epoch[304](220/324): Loss: 0.5682 || Learning rate: lr=1.25e-05.
===> Epoch[304](230/324): Loss: 0.6092 || Learning rate: lr=1.25e-05.
===> Epoch[304](240/324): Loss: 0.5934 || Learning rate: lr=1.25e-05.
===> Epoch[304](250/324): Loss: 0.8740 || Learning rate: lr=1.25e-05.
===> Epoch[304](260/324): Loss: 1.0874 || Learning rate: lr=1.25e-05.
===> Epoch[304](270/324): Loss: 0.4484 || Learning rate: lr=1.25e-05.
===> Epoch[304](280/324): Loss: 0.7377 || Learning rate: lr=1.25e-05.
===> Epoch[304](290/324): Loss: 0.6361 || Learning rate: lr=1.25e-05.
===> Epoch[304](300/324): Loss: 0.7306 || Learning rate: lr=1.25e-05.
===> Epoch[304](310/324): Loss: 0.8474 || Learning rate: lr=1.25e-05.
===> Epoch[304](320/324): Loss: 0.8406 || Learning rate: lr=1.25e-05.
===> Epoch[305](10/324): Loss: 0.8577 || Learning rate: lr=1.25e-05.
===> Epoch[305](20/324): Loss: 0.6550 || Learning rate: lr=1.25e-05.
===> Epoch[305](30/324): Loss: 0.6750 || Learning rate: lr=1.25e-05.
===> Epoch[305](40/324): Loss: 0.5199 || Learning rate: lr=1.25e-05.
===> Epoch[305](50/324): Loss: 0.5766 || Learning rate: lr=1.25e-05.
===> Epoch[305](60/324): Loss: 0.3447 || Learning rate: lr=1.25e-05.
===> Epoch[305](70/324): Loss: 0.9610 || Learning rate: lr=1.25e-05.
===> Epoch[305](80/324): Loss: 0.7368 || Learning rate: lr=1.25e-05.
===> Epoch[305](90/324): Loss: 0.7198 || Learning rate: lr=1.25e-05.
===> Epoch[305](100/324): Loss: 0.6881 || Learning rate: lr=1.25e-05.
===> Epoch[305](110/324): Loss: 0.6661 || Learning rate: lr=1.25e-05.
===> Epoch[305](120/324): Loss: 0.3774 || Learning rate: lr=1.25e-05.
===> Epoch[305](130/324): Loss: 0.4862 || Learning rate: lr=1.25e-05.
===> Epoch[305](140/324): Loss: 0.6805 || Learning rate: lr=1.25e-05.
===> Epoch[305](150/324): Loss: 0.5948 || Learning rate: lr=1.25e-05.
===> Epoch[305](160/324): Loss: 0.7525 || Learning rate: lr=1.25e-05.
===> Epoch[305](170/324): Loss: 0.6649 || Learning rate: lr=1.25e-05.
===> Epoch[305](180/324): Loss: 0.6780 || Learning rate: lr=1.25e-05.
===> Epoch[305](190/324): Loss: 0.6730 || Learning rate: lr=1.25e-05.
===> Epoch[305](200/324): Loss: 0.5243 || Learning rate: lr=1.25e-05.
===> Epoch[305](210/324): Loss: 0.9185 || Learning rate: lr=1.25e-05.
===> Epoch[305](220/324): Loss: 0.8718 || Learning rate: lr=1.25e-05.
===> Epoch[305](230/324): Loss: 0.8798 || Learning rate: lr=1.25e-05.
===> Epoch[305](240/324): Loss: 0.7829 || Learning rate: lr=1.25e-05.
===> Epoch[305](250/324): Loss: 0.6724 || Learning rate: lr=1.25e-05.
===> Epoch[305](260/324): Loss: 0.6573 || Learning rate: lr=1.25e-05.
===> Epoch[305](270/324): Loss: 0.4654 || Learning rate: lr=1.25e-05.
===> Epoch[305](280/324): Loss: 0.7072 || Learning rate: lr=1.25e-05.
===> Epoch[305](290/324): Loss: 0.5722 || Learning rate: lr=1.25e-05.
===> Epoch[305](300/324): Loss: 0.6766 || Learning rate: lr=1.25e-05.
===> Epoch[305](310/324): Loss: 0.6703 || Learning rate: lr=1.25e-05.
===> Epoch[305](320/324): Loss: 0.4547 || Learning rate: lr=1.25e-05.
===> Epoch[306](10/324): Loss: 0.6564 || Learning rate: lr=1.25e-05.
===> Epoch[306](20/324): Loss: 0.6466 || Learning rate: lr=1.25e-05.
===> Epoch[306](30/324): Loss: 0.7637 || Learning rate: lr=1.25e-05.
===> Epoch[306](40/324): Loss: 0.5523 || Learning rate: lr=1.25e-05.
===> Epoch[306](50/324): Loss: 0.5978 || Learning rate: lr=1.25e-05.
===> Epoch[306](60/324): Loss: 0.4972 || Learning rate: lr=1.25e-05.
===> Epoch[306](70/324): Loss: 0.6105 || Learning rate: lr=1.25e-05.
===> Epoch[306](80/324): Loss: 0.9630 || Learning rate: lr=1.25e-05.
===> Epoch[306](90/324): Loss: 0.9326 || Learning rate: lr=1.25e-05.
===> Epoch[306](100/324): Loss: 0.6327 || Learning rate: lr=1.25e-05.
===> Epoch[306](110/324): Loss: 0.6136 || Learning rate: lr=1.25e-05.
===> Epoch[306](120/324): Loss: 0.6072 || Learning rate: lr=1.25e-05.
===> Epoch[306](130/324): Loss: 0.6003 || Learning rate: lr=1.25e-05.
===> Epoch[306](140/324): Loss: 0.5231 || Learning rate: lr=1.25e-05.
===> Epoch[306](150/324): Loss: 0.7530 || Learning rate: lr=1.25e-05.
===> Epoch[306](160/324): Loss: 0.6000 || Learning rate: lr=1.25e-05.
===> Epoch[306](170/324): Loss: 0.7519 || Learning rate: lr=1.25e-05.
===> Epoch[306](180/324): Loss: 0.8814 || Learning rate: lr=1.25e-05.
===> Epoch[306](190/324): Loss: 0.7977 || Learning rate: lr=1.25e-05.
===> Epoch[306](200/324): Loss: 0.5433 || Learning rate: lr=1.25e-05.
===> Epoch[306](210/324): Loss: 0.5738 || Learning rate: lr=1.25e-05.
===> Epoch[306](220/324): Loss: 0.5005 || Learning rate: lr=1.25e-05.
===> Epoch[306](230/324): Loss: 0.7217 || Learning rate: lr=1.25e-05.
===> Epoch[306](240/324): Loss: 0.5424 || Learning rate: lr=1.25e-05.
===> Epoch[306](250/324): Loss: 0.9603 || Learning rate: lr=1.25e-05.
===> Epoch[306](260/324): Loss: 0.4484 || Learning rate: lr=1.25e-05.
===> Epoch[306](270/324): Loss: 0.7100 || Learning rate: lr=1.25e-05.
===> Epoch[306](280/324): Loss: 0.6275 || Learning rate: lr=1.25e-05.
===> Epoch[306](290/324): Loss: 0.7834 || Learning rate: lr=1.25e-05.
===> Epoch[306](300/324): Loss: 0.5058 || Learning rate: lr=1.25e-05.
===> Epoch[306](310/324): Loss: 0.8820 || Learning rate: lr=1.25e-05.
===> Epoch[306](320/324): Loss: 0.6803 || Learning rate: lr=1.25e-05.
===> Epoch[307](10/324): Loss: 0.6177 || Learning rate: lr=1.25e-05.
===> Epoch[307](20/324): Loss: 0.7508 || Learning rate: lr=1.25e-05.
===> Epoch[307](30/324): Loss: 0.5564 || Learning rate: lr=1.25e-05.
===> Epoch[307](40/324): Loss: 0.7355 || Learning rate: lr=1.25e-05.
===> Epoch[307](50/324): Loss: 0.5171 || Learning rate: lr=1.25e-05.
===> Epoch[307](60/324): Loss: 0.6388 || Learning rate: lr=1.25e-05.
===> Epoch[307](70/324): Loss: 0.6785 || Learning rate: lr=1.25e-05.
===> Epoch[307](80/324): Loss: 0.5498 || Learning rate: lr=1.25e-05.
===> Epoch[307](90/324): Loss: 0.5745 || Learning rate: lr=1.25e-05.
===> Epoch[307](100/324): Loss: 0.4866 || Learning rate: lr=1.25e-05.
===> Epoch[307](110/324): Loss: 0.6122 || Learning rate: lr=1.25e-05.
===> Epoch[307](120/324): Loss: 0.6183 || Learning rate: lr=1.25e-05.
===> Epoch[307](130/324): Loss: 0.9207 || Learning rate: lr=1.25e-05.
===> Epoch[307](140/324): Loss: 0.7623 || Learning rate: lr=1.25e-05.
===> Epoch[307](150/324): Loss: 0.5711 || Learning rate: lr=1.25e-05.
===> Epoch[307](160/324): Loss: 0.8907 || Learning rate: lr=1.25e-05.
===> Epoch[307](170/324): Loss: 0.5207 || Learning rate: lr=1.25e-05.
===> Epoch[307](180/324): Loss: 0.7013 || Learning rate: lr=1.25e-05.
===> Epoch[307](190/324): Loss: 0.6963 || Learning rate: lr=1.25e-05.
===> Epoch[307](200/324): Loss: 0.5730 || Learning rate: lr=1.25e-05.
===> Epoch[307](210/324): Loss: 0.6411 || Learning rate: lr=1.25e-05.
===> Epoch[307](220/324): Loss: 0.5635 || Learning rate: lr=1.25e-05.
===> Epoch[307](230/324): Loss: 0.6364 || Learning rate: lr=1.25e-05.
===> Epoch[307](240/324): Loss: 0.6020 || Learning rate: lr=1.25e-05.
===> Epoch[307](250/324): Loss: 0.7150 || Learning rate: lr=1.25e-05.
===> Epoch[307](260/324): Loss: 0.5515 || Learning rate: lr=1.25e-05.
===> Epoch[307](270/324): Loss: 0.5822 || Learning rate: lr=1.25e-05.
===> Epoch[307](280/324): Loss: 1.0491 || Learning rate: lr=1.25e-05.
===> Epoch[307](290/324): Loss: 0.8928 || Learning rate: lr=1.25e-05.
===> Epoch[307](300/324): Loss: 0.6994 || Learning rate: lr=1.25e-05.
===> Epoch[307](310/324): Loss: 0.4772 || Learning rate: lr=1.25e-05.
===> Epoch[307](320/324): Loss: 0.8538 || Learning rate: lr=1.25e-05.
===> Epoch[308](10/324): Loss: 0.7982 || Learning rate: lr=1.25e-05.
===> Epoch[308](20/324): Loss: 1.1381 || Learning rate: lr=1.25e-05.
===> Epoch[308](30/324): Loss: 0.7628 || Learning rate: lr=1.25e-05.
===> Epoch[308](40/324): Loss: 0.7200 || Learning rate: lr=1.25e-05.
===> Epoch[308](50/324): Loss: 1.0341 || Learning rate: lr=1.25e-05.
===> Epoch[308](60/324): Loss: 0.9121 || Learning rate: lr=1.25e-05.
===> Epoch[308](70/324): Loss: 0.5224 || Learning rate: lr=1.25e-05.
===> Epoch[308](80/324): Loss: 0.6378 || Learning rate: lr=1.25e-05.
===> Epoch[308](90/324): Loss: 0.6069 || Learning rate: lr=1.25e-05.
===> Epoch[308](100/324): Loss: 0.5769 || Learning rate: lr=1.25e-05.
===> Epoch[308](110/324): Loss: 0.5078 || Learning rate: lr=1.25e-05.
===> Epoch[308](120/324): Loss: 0.5701 || Learning rate: lr=1.25e-05.
===> Epoch[308](130/324): Loss: 0.6870 || Learning rate: lr=1.25e-05.
===> Epoch[308](140/324): Loss: 0.7039 || Learning rate: lr=1.25e-05.
===> Epoch[308](150/324): Loss: 0.7788 || Learning rate: lr=1.25e-05.
===> Epoch[308](160/324): Loss: 0.6523 || Learning rate: lr=1.25e-05.
===> Epoch[308](170/324): Loss: 0.7744 || Learning rate: lr=1.25e-05.
===> Epoch[308](180/324): Loss: 0.4308 || Learning rate: lr=1.25e-05.
===> Epoch[308](190/324): Loss: 0.5332 || Learning rate: lr=1.25e-05.
===> Epoch[308](200/324): Loss: 0.5942 || Learning rate: lr=1.25e-05.
===> Epoch[308](210/324): Loss: 0.5105 || Learning rate: lr=1.25e-05.
===> Epoch[308](220/324): Loss: 0.9469 || Learning rate: lr=1.25e-05.
===> Epoch[308](230/324): Loss: 0.6539 || Learning rate: lr=1.25e-05.
===> Epoch[308](240/324): Loss: 0.6529 || Learning rate: lr=1.25e-05.
===> Epoch[308](250/324): Loss: 0.6096 || Learning rate: lr=1.25e-05.
===> Epoch[308](260/324): Loss: 0.5958 || Learning rate: lr=1.25e-05.
===> Epoch[308](270/324): Loss: 0.4716 || Learning rate: lr=1.25e-05.
===> Epoch[308](280/324): Loss: 0.7284 || Learning rate: lr=1.25e-05.
===> Epoch[308](290/324): Loss: 0.8146 || Learning rate: lr=1.25e-05.
===> Epoch[308](300/324): Loss: 0.7576 || Learning rate: lr=1.25e-05.
===> Epoch[308](310/324): Loss: 0.7662 || Learning rate: lr=1.25e-05.
===> Epoch[308](320/324): Loss: 0.4173 || Learning rate: lr=1.25e-05.
===> Epoch[309](10/324): Loss: 0.6768 || Learning rate: lr=1.25e-05.
===> Epoch[309](20/324): Loss: 0.8358 || Learning rate: lr=1.25e-05.
===> Epoch[309](30/324): Loss: 0.6677 || Learning rate: lr=1.25e-05.
===> Epoch[309](40/324): Loss: 0.5323 || Learning rate: lr=1.25e-05.
===> Epoch[309](50/324): Loss: 0.7659 || Learning rate: lr=1.25e-05.
===> Epoch[309](60/324): Loss: 0.8893 || Learning rate: lr=1.25e-05.
===> Epoch[309](70/324): Loss: 0.8252 || Learning rate: lr=1.25e-05.
===> Epoch[309](80/324): Loss: 0.5421 || Learning rate: lr=1.25e-05.
===> Epoch[309](90/324): Loss: 0.5223 || Learning rate: lr=1.25e-05.
===> Epoch[309](100/324): Loss: 0.7814 || Learning rate: lr=1.25e-05.
===> Epoch[309](110/324): Loss: 0.5464 || Learning rate: lr=1.25e-05.
===> Epoch[309](120/324): Loss: 0.7875 || Learning rate: lr=1.25e-05.
===> Epoch[309](130/324): Loss: 0.6348 || Learning rate: lr=1.25e-05.
===> Epoch[309](140/324): Loss: 0.4946 || Learning rate: lr=1.25e-05.
===> Epoch[309](150/324): Loss: 0.7616 || Learning rate: lr=1.25e-05.
===> Epoch[309](160/324): Loss: 0.6194 || Learning rate: lr=1.25e-05.
===> Epoch[309](170/324): Loss: 0.8071 || Learning rate: lr=1.25e-05.
===> Epoch[309](180/324): Loss: 0.4919 || Learning rate: lr=1.25e-05.
===> Epoch[309](190/324): Loss: 0.7028 || Learning rate: lr=1.25e-05.
===> Epoch[309](200/324): Loss: 0.7487 || Learning rate: lr=1.25e-05.
===> Epoch[309](210/324): Loss: 0.5678 || Learning rate: lr=1.25e-05.
===> Epoch[309](220/324): Loss: 0.5064 || Learning rate: lr=1.25e-05.
===> Epoch[309](230/324): Loss: 0.7974 || Learning rate: lr=1.25e-05.
===> Epoch[309](240/324): Loss: 0.6355 || Learning rate: lr=1.25e-05.
===> Epoch[309](250/324): Loss: 0.6808 || Learning rate: lr=1.25e-05.
===> Epoch[309](260/324): Loss: 0.5934 || Learning rate: lr=1.25e-05.
===> Epoch[309](270/324): Loss: 0.5901 || Learning rate: lr=1.25e-05.
===> Epoch[309](280/324): Loss: 0.5524 || Learning rate: lr=1.25e-05.
===> Epoch[309](290/324): Loss: 0.4083 || Learning rate: lr=1.25e-05.
===> Epoch[309](300/324): Loss: 0.9486 || Learning rate: lr=1.25e-05.
===> Epoch[309](310/324): Loss: 0.7611 || Learning rate: lr=1.25e-05.
===> Epoch[309](320/324): Loss: 0.8549 || Learning rate: lr=1.25e-05.
===> Epoch[310](10/324): Loss: 0.8334 || Learning rate: lr=1.25e-05.
===> Epoch[310](20/324): Loss: 0.5339 || Learning rate: lr=1.25e-05.
===> Epoch[310](30/324): Loss: 0.8612 || Learning rate: lr=1.25e-05.
===> Epoch[310](40/324): Loss: 0.9891 || Learning rate: lr=1.25e-05.
===> Epoch[310](50/324): Loss: 0.5820 || Learning rate: lr=1.25e-05.
===> Epoch[310](60/324): Loss: 0.9294 || Learning rate: lr=1.25e-05.
===> Epoch[310](70/324): Loss: 0.7220 || Learning rate: lr=1.25e-05.
===> Epoch[310](80/324): Loss: 0.7145 || Learning rate: lr=1.25e-05.
===> Epoch[310](90/324): Loss: 0.6792 || Learning rate: lr=1.25e-05.
===> Epoch[310](100/324): Loss: 0.6357 || Learning rate: lr=1.25e-05.
===> Epoch[310](110/324): Loss: 0.6508 || Learning rate: lr=1.25e-05.
===> Epoch[310](120/324): Loss: 0.5461 || Learning rate: lr=1.25e-05.
===> Epoch[310](130/324): Loss: 0.5478 || Learning rate: lr=1.25e-05.
===> Epoch[310](140/324): Loss: 0.5027 || Learning rate: lr=1.25e-05.
===> Epoch[310](150/324): Loss: 0.5725 || Learning rate: lr=1.25e-05.
===> Epoch[310](160/324): Loss: 0.5894 || Learning rate: lr=1.25e-05.
===> Epoch[310](170/324): Loss: 0.5259 || Learning rate: lr=1.25e-05.
===> Epoch[310](180/324): Loss: 0.7011 || Learning rate: lr=1.25e-05.
===> Epoch[310](190/324): Loss: 0.9116 || Learning rate: lr=1.25e-05.
===> Epoch[310](200/324): Loss: 0.7105 || Learning rate: lr=1.25e-05.
===> Epoch[310](210/324): Loss: 0.6851 || Learning rate: lr=1.25e-05.
===> Epoch[310](220/324): Loss: 0.6395 || Learning rate: lr=1.25e-05.
===> Epoch[310](230/324): Loss: 0.6114 || Learning rate: lr=1.25e-05.
===> Epoch[310](240/324): Loss: 0.6441 || Learning rate: lr=1.25e-05.
===> Epoch[310](250/324): Loss: 0.4970 || Learning rate: lr=1.25e-05.
===> Epoch[310](260/324): Loss: 0.4402 || Learning rate: lr=1.25e-05.
===> Epoch[310](270/324): Loss: 0.4895 || Learning rate: lr=1.25e-05.
===> Epoch[310](280/324): Loss: 0.9753 || Learning rate: lr=1.25e-05.
===> Epoch[310](290/324): Loss: 0.4728 || Learning rate: lr=1.25e-05.
===> Epoch[310](300/324): Loss: 0.5481 || Learning rate: lr=1.25e-05.
===> Epoch[310](310/324): Loss: 0.7345 || Learning rate: lr=1.25e-05.
===> Epoch[310](320/324): Loss: 0.4315 || Learning rate: lr=1.25e-05.
===> Epoch[311](10/324): Loss: 0.4575 || Learning rate: lr=1.25e-05.
===> Epoch[311](20/324): Loss: 0.5137 || Learning rate: lr=1.25e-05.
===> Epoch[311](30/324): Loss: 0.5485 || Learning rate: lr=1.25e-05.
===> Epoch[311](40/324): Loss: 0.6418 || Learning rate: lr=1.25e-05.
===> Epoch[311](50/324): Loss: 0.8127 || Learning rate: lr=1.25e-05.
===> Epoch[311](60/324): Loss: 0.9876 || Learning rate: lr=1.25e-05.
===> Epoch[311](70/324): Loss: 0.5770 || Learning rate: lr=1.25e-05.
===> Epoch[311](80/324): Loss: 0.7035 || Learning rate: lr=1.25e-05.
===> Epoch[311](90/324): Loss: 0.6217 || Learning rate: lr=1.25e-05.
===> Epoch[311](100/324): Loss: 0.9522 || Learning rate: lr=1.25e-05.
===> Epoch[311](110/324): Loss: 0.6403 || Learning rate: lr=1.25e-05.
===> Epoch[311](120/324): Loss: 0.7003 || Learning rate: lr=1.25e-05.
===> Epoch[311](130/324): Loss: 0.7544 || Learning rate: lr=1.25e-05.
===> Epoch[311](140/324): Loss: 0.5720 || Learning rate: lr=1.25e-05.
===> Epoch[311](150/324): Loss: 0.4310 || Learning rate: lr=1.25e-05.
===> Epoch[311](160/324): Loss: 0.5631 || Learning rate: lr=1.25e-05.
===> Epoch[311](170/324): Loss: 0.6712 || Learning rate: lr=1.25e-05.
===> Epoch[311](180/324): Loss: 0.6190 || Learning rate: lr=1.25e-05.
===> Epoch[311](190/324): Loss: 0.6469 || Learning rate: lr=1.25e-05.
===> Epoch[311](200/324): Loss: 0.5668 || Learning rate: lr=1.25e-05.
===> Epoch[311](210/324): Loss: 0.7812 || Learning rate: lr=1.25e-05.
===> Epoch[311](220/324): Loss: 0.5324 || Learning rate: lr=1.25e-05.
===> Epoch[311](230/324): Loss: 0.6383 || Learning rate: lr=1.25e-05.
===> Epoch[311](240/324): Loss: 0.4985 || Learning rate: lr=1.25e-05.
===> Epoch[311](250/324): Loss: 0.3878 || Learning rate: lr=1.25e-05.
===> Epoch[311](260/324): Loss: 0.4524 || Learning rate: lr=1.25e-05.
===> Epoch[311](270/324): Loss: 0.3550 || Learning rate: lr=1.25e-05.
===> Epoch[311](280/324): Loss: 0.9551 || Learning rate: lr=1.25e-05.
===> Epoch[311](290/324): Loss: 0.7161 || Learning rate: lr=1.25e-05.
===> Epoch[311](300/324): Loss: 0.4683 || Learning rate: lr=1.25e-05.
===> Epoch[311](310/324): Loss: 0.6460 || Learning rate: lr=1.25e-05.
===> Epoch[311](320/324): Loss: 0.6297 || Learning rate: lr=1.25e-05.
===> Epoch[312](10/324): Loss: 0.4974 || Learning rate: lr=1.25e-05.
===> Epoch[312](20/324): Loss: 0.7142 || Learning rate: lr=1.25e-05.
===> Epoch[312](30/324): Loss: 0.4674 || Learning rate: lr=1.25e-05.
===> Epoch[312](40/324): Loss: 0.8519 || Learning rate: lr=1.25e-05.
===> Epoch[312](50/324): Loss: 0.5715 || Learning rate: lr=1.25e-05.
===> Epoch[312](60/324): Loss: 0.7573 || Learning rate: lr=1.25e-05.
===> Epoch[312](70/324): Loss: 0.8995 || Learning rate: lr=1.25e-05.
===> Epoch[312](80/324): Loss: 0.4615 || Learning rate: lr=1.25e-05.
===> Epoch[312](90/324): Loss: 1.0126 || Learning rate: lr=1.25e-05.
===> Epoch[312](100/324): Loss: 0.5782 || Learning rate: lr=1.25e-05.
===> Epoch[312](110/324): Loss: 0.6736 || Learning rate: lr=1.25e-05.
===> Epoch[312](120/324): Loss: 0.5401 || Learning rate: lr=1.25e-05.
===> Epoch[312](130/324): Loss: 0.4728 || Learning rate: lr=1.25e-05.
===> Epoch[312](140/324): Loss: 0.6095 || Learning rate: lr=1.25e-05.
===> Epoch[312](150/324): Loss: 0.6618 || Learning rate: lr=1.25e-05.
===> Epoch[312](160/324): Loss: 0.8276 || Learning rate: lr=1.25e-05.
===> Epoch[312](170/324): Loss: 0.6232 || Learning rate: lr=1.25e-05.
===> Epoch[312](180/324): Loss: 0.7655 || Learning rate: lr=1.25e-05.
===> Epoch[312](190/324): Loss: 0.5471 || Learning rate: lr=1.25e-05.
===> Epoch[312](200/324): Loss: 0.5100 || Learning rate: lr=1.25e-05.
===> Epoch[312](210/324): Loss: 0.5180 || Learning rate: lr=1.25e-05.
===> Epoch[312](220/324): Loss: 0.6223 || Learning rate: lr=1.25e-05.
===> Epoch[312](230/324): Loss: 0.7131 || Learning rate: lr=1.25e-05.
===> Epoch[312](240/324): Loss: 0.9773 || Learning rate: lr=1.25e-05.
===> Epoch[312](250/324): Loss: 0.7271 || Learning rate: lr=1.25e-05.
===> Epoch[312](260/324): Loss: 0.9334 || Learning rate: lr=1.25e-05.
===> Epoch[312](270/324): Loss: 0.8658 || Learning rate: lr=1.25e-05.
===> Epoch[312](280/324): Loss: 0.4200 || Learning rate: lr=1.25e-05.
===> Epoch[312](290/324): Loss: 0.5061 || Learning rate: lr=1.25e-05.
===> Epoch[312](300/324): Loss: 0.5370 || Learning rate: lr=1.25e-05.
===> Epoch[312](310/324): Loss: 1.1532 || Learning rate: lr=1.25e-05.
===> Epoch[312](320/324): Loss: 0.7974 || Learning rate: lr=1.25e-05.
===> Epoch[313](10/324): Loss: 0.6551 || Learning rate: lr=1.25e-05.
===> Epoch[313](20/324): Loss: 0.6372 || Learning rate: lr=1.25e-05.
===> Epoch[313](30/324): Loss: 0.6211 || Learning rate: lr=1.25e-05.
===> Epoch[313](40/324): Loss: 0.8546 || Learning rate: lr=1.25e-05.
===> Epoch[313](50/324): Loss: 0.6330 || Learning rate: lr=1.25e-05.
===> Epoch[313](60/324): Loss: 0.6144 || Learning rate: lr=1.25e-05.
===> Epoch[313](70/324): Loss: 0.6762 || Learning rate: lr=1.25e-05.
===> Epoch[313](80/324): Loss: 0.6515 || Learning rate: lr=1.25e-05.
===> Epoch[313](90/324): Loss: 0.7198 || Learning rate: lr=1.25e-05.
===> Epoch[313](100/324): Loss: 0.6212 || Learning rate: lr=1.25e-05.
===> Epoch[313](110/324): Loss: 0.7247 || Learning rate: lr=1.25e-05.
===> Epoch[313](120/324): Loss: 0.7289 || Learning rate: lr=1.25e-05.
===> Epoch[313](130/324): Loss: 0.5147 || Learning rate: lr=1.25e-05.
===> Epoch[313](140/324): Loss: 0.8661 || Learning rate: lr=1.25e-05.
===> Epoch[313](150/324): Loss: 0.4689 || Learning rate: lr=1.25e-05.
===> Epoch[313](160/324): Loss: 0.6538 || Learning rate: lr=1.25e-05.
===> Epoch[313](170/324): Loss: 0.8298 || Learning rate: lr=1.25e-05.
===> Epoch[313](180/324): Loss: 1.0556 || Learning rate: lr=1.25e-05.
===> Epoch[313](190/324): Loss: 0.8719 || Learning rate: lr=1.25e-05.
===> Epoch[313](200/324): Loss: 0.6675 || Learning rate: lr=1.25e-05.
===> Epoch[313](210/324): Loss: 0.6212 || Learning rate: lr=1.25e-05.
===> Epoch[313](220/324): Loss: 0.7811 || Learning rate: lr=1.25e-05.
===> Epoch[313](230/324): Loss: 0.5879 || Learning rate: lr=1.25e-05.
===> Epoch[313](240/324): Loss: 0.5073 || Learning rate: lr=1.25e-05.
===> Epoch[313](250/324): Loss: 0.6174 || Learning rate: lr=1.25e-05.
===> Epoch[313](260/324): Loss: 0.6885 || Learning rate: lr=1.25e-05.
===> Epoch[313](270/324): Loss: 0.4966 || Learning rate: lr=1.25e-05.
===> Epoch[313](280/324): Loss: 0.4328 || Learning rate: lr=1.25e-05.
===> Epoch[313](290/324): Loss: 0.4998 || Learning rate: lr=1.25e-05.
===> Epoch[313](300/324): Loss: 0.5060 || Learning rate: lr=1.25e-05.
===> Epoch[313](310/324): Loss: 0.6218 || Learning rate: lr=1.25e-05.
===> Epoch[313](320/324): Loss: 0.8605 || Learning rate: lr=1.25e-05.
===> Epoch[314](10/324): Loss: 0.6073 || Learning rate: lr=1.25e-05.
===> Epoch[314](20/324): Loss: 0.7974 || Learning rate: lr=1.25e-05.
===> Epoch[314](30/324): Loss: 0.5607 || Learning rate: lr=1.25e-05.
===> Epoch[314](40/324): Loss: 0.6982 || Learning rate: lr=1.25e-05.
===> Epoch[314](50/324): Loss: 0.6484 || Learning rate: lr=1.25e-05.
===> Epoch[314](60/324): Loss: 0.4607 || Learning rate: lr=1.25e-05.
===> Epoch[314](70/324): Loss: 0.7046 || Learning rate: lr=1.25e-05.
===> Epoch[314](80/324): Loss: 0.9644 || Learning rate: lr=1.25e-05.
===> Epoch[314](90/324): Loss: 0.6734 || Learning rate: lr=1.25e-05.
===> Epoch[314](100/324): Loss: 0.6233 || Learning rate: lr=1.25e-05.
===> Epoch[314](110/324): Loss: 0.6166 || Learning rate: lr=1.25e-05.
===> Epoch[314](120/324): Loss: 0.6304 || Learning rate: lr=1.25e-05.
===> Epoch[314](130/324): Loss: 0.9369 || Learning rate: lr=1.25e-05.
===> Epoch[314](140/324): Loss: 0.6291 || Learning rate: lr=1.25e-05.
===> Epoch[314](150/324): Loss: 0.6599 || Learning rate: lr=1.25e-05.
===> Epoch[314](160/324): Loss: 0.5007 || Learning rate: lr=1.25e-05.
===> Epoch[314](170/324): Loss: 0.5697 || Learning rate: lr=1.25e-05.
===> Epoch[314](180/324): Loss: 0.9223 || Learning rate: lr=1.25e-05.
===> Epoch[314](190/324): Loss: 0.6063 || Learning rate: lr=1.25e-05.
===> Epoch[314](200/324): Loss: 0.7389 || Learning rate: lr=1.25e-05.
===> Epoch[314](210/324): Loss: 0.6683 || Learning rate: lr=1.25e-05.
===> Epoch[314](220/324): Loss: 0.6284 || Learning rate: lr=1.25e-05.
===> Epoch[314](230/324): Loss: 0.6466 || Learning rate: lr=1.25e-05.
===> Epoch[314](240/324): Loss: 0.6529 || Learning rate: lr=1.25e-05.
===> Epoch[314](250/324): Loss: 0.6300 || Learning rate: lr=1.25e-05.
===> Epoch[314](260/324): Loss: 0.4245 || Learning rate: lr=1.25e-05.
===> Epoch[314](270/324): Loss: 0.7710 || Learning rate: lr=1.25e-05.
===> Epoch[314](280/324): Loss: 0.7101 || Learning rate: lr=1.25e-05.
===> Epoch[314](290/324): Loss: 0.5766 || Learning rate: lr=1.25e-05.
===> Epoch[314](300/324): Loss: 0.6937 || Learning rate: lr=1.25e-05.
===> Epoch[314](310/324): Loss: 0.4577 || Learning rate: lr=1.25e-05.
===> Epoch[314](320/324): Loss: 0.8076 || Learning rate: lr=1.25e-05.
===> Epoch[315](10/324): Loss: 0.6165 || Learning rate: lr=1.25e-05.
===> Epoch[315](20/324): Loss: 1.0088 || Learning rate: lr=1.25e-05.
===> Epoch[315](30/324): Loss: 0.7339 || Learning rate: lr=1.25e-05.
===> Epoch[315](40/324): Loss: 0.8088 || Learning rate: lr=1.25e-05.
===> Epoch[315](50/324): Loss: 0.5885 || Learning rate: lr=1.25e-05.
===> Epoch[315](60/324): Loss: 0.5454 || Learning rate: lr=1.25e-05.
===> Epoch[315](70/324): Loss: 0.6476 || Learning rate: lr=1.25e-05.
===> Epoch[315](80/324): Loss: 0.5618 || Learning rate: lr=1.25e-05.
===> Epoch[315](90/324): Loss: 0.6272 || Learning rate: lr=1.25e-05.
===> Epoch[315](100/324): Loss: 0.7591 || Learning rate: lr=1.25e-05.
===> Epoch[315](110/324): Loss: 0.8343 || Learning rate: lr=1.25e-05.
===> Epoch[315](120/324): Loss: 0.7025 || Learning rate: lr=1.25e-05.
===> Epoch[315](130/324): Loss: 0.5213 || Learning rate: lr=1.25e-05.
===> Epoch[315](140/324): Loss: 0.5634 || Learning rate: lr=1.25e-05.
===> Epoch[315](150/324): Loss: 0.5038 || Learning rate: lr=1.25e-05.
===> Epoch[315](160/324): Loss: 0.7658 || Learning rate: lr=1.25e-05.
===> Epoch[315](170/324): Loss: 0.5862 || Learning rate: lr=1.25e-05.
===> Epoch[315](180/324): Loss: 0.5892 || Learning rate: lr=1.25e-05.
===> Epoch[315](190/324): Loss: 0.5622 || Learning rate: lr=1.25e-05.
===> Epoch[315](200/324): Loss: 0.9728 || Learning rate: lr=1.25e-05.
===> Epoch[315](210/324): Loss: 0.6145 || Learning rate: lr=1.25e-05.
===> Epoch[315](220/324): Loss: 0.6570 || Learning rate: lr=1.25e-05.
===> Epoch[315](230/324): Loss: 0.9399 || Learning rate: lr=1.25e-05.
===> Epoch[315](240/324): Loss: 0.7526 || Learning rate: lr=1.25e-05.
===> Epoch[315](250/324): Loss: 0.5378 || Learning rate: lr=1.25e-05.
===> Epoch[315](260/324): Loss: 0.6695 || Learning rate: lr=1.25e-05.
===> Epoch[315](270/324): Loss: 0.6428 || Learning rate: lr=1.25e-05.
===> Epoch[315](280/324): Loss: 0.5997 || Learning rate: lr=1.25e-05.
===> Epoch[315](290/324): Loss: 0.5755 || Learning rate: lr=1.25e-05.
===> Epoch[315](300/324): Loss: 0.6161 || Learning rate: lr=1.25e-05.
===> Epoch[315](310/324): Loss: 0.5091 || Learning rate: lr=1.25e-05.
===> Epoch[315](320/324): Loss: 0.5377 || Learning rate: lr=1.25e-05.
===> Epoch[316](10/324): Loss: 0.6871 || Learning rate: lr=1.25e-05.
===> Epoch[316](20/324): Loss: 0.5040 || Learning rate: lr=1.25e-05.
===> Epoch[316](30/324): Loss: 0.5839 || Learning rate: lr=1.25e-05.
===> Epoch[316](40/324): Loss: 0.5651 || Learning rate: lr=1.25e-05.
===> Epoch[316](50/324): Loss: 0.7088 || Learning rate: lr=1.25e-05.
===> Epoch[316](60/324): Loss: 0.7128 || Learning rate: lr=1.25e-05.
===> Epoch[316](70/324): Loss: 0.8198 || Learning rate: lr=1.25e-05.
===> Epoch[316](80/324): Loss: 0.5742 || Learning rate: lr=1.25e-05.
===> Epoch[316](90/324): Loss: 0.6060 || Learning rate: lr=1.25e-05.
===> Epoch[316](100/324): Loss: 0.8351 || Learning rate: lr=1.25e-05.
===> Epoch[316](110/324): Loss: 0.5809 || Learning rate: lr=1.25e-05.
===> Epoch[316](120/324): Loss: 0.6186 || Learning rate: lr=1.25e-05.
===> Epoch[316](130/324): Loss: 0.5972 || Learning rate: lr=1.25e-05.
===> Epoch[316](140/324): Loss: 0.6287 || Learning rate: lr=1.25e-05.
===> Epoch[316](150/324): Loss: 0.7400 || Learning rate: lr=1.25e-05.
===> Epoch[316](160/324): Loss: 0.5337 || Learning rate: lr=1.25e-05.
===> Epoch[316](170/324): Loss: 0.7233 || Learning rate: lr=1.25e-05.
===> Epoch[316](180/324): Loss: 0.6648 || Learning rate: lr=1.25e-05.
===> Epoch[316](190/324): Loss: 0.5795 || Learning rate: lr=1.25e-05.
===> Epoch[316](200/324): Loss: 0.8148 || Learning rate: lr=1.25e-05.
===> Epoch[316](210/324): Loss: 0.6140 || Learning rate: lr=1.25e-05.
===> Epoch[316](220/324): Loss: 0.6630 || Learning rate: lr=1.25e-05.
===> Epoch[316](230/324): Loss: 0.6914 || Learning rate: lr=1.25e-05.
===> Epoch[316](240/324): Loss: 0.5933 || Learning rate: lr=1.25e-05.
===> Epoch[316](250/324): Loss: 0.5706 || Learning rate: lr=1.25e-05.
===> Epoch[316](260/324): Loss: 1.0605 || Learning rate: lr=1.25e-05.
===> Epoch[316](270/324): Loss: 0.5243 || Learning rate: lr=1.25e-05.
===> Epoch[316](280/324): Loss: 0.5358 || Learning rate: lr=1.25e-05.
===> Epoch[316](290/324): Loss: 0.7408 || Learning rate: lr=1.25e-05.
===> Epoch[316](300/324): Loss: 0.7795 || Learning rate: lr=1.25e-05.
===> Epoch[316](310/324): Loss: 0.7602 || Learning rate: lr=1.25e-05.
===> Epoch[316](320/324): Loss: 0.5365 || Learning rate: lr=1.25e-05.
===> Epoch[317](10/324): Loss: 0.7483 || Learning rate: lr=1.25e-05.
===> Epoch[317](20/324): Loss: 0.7661 || Learning rate: lr=1.25e-05.
===> Epoch[317](30/324): Loss: 0.6044 || Learning rate: lr=1.25e-05.
===> Epoch[317](40/324): Loss: 1.1197 || Learning rate: lr=1.25e-05.
===> Epoch[317](50/324): Loss: 0.7017 || Learning rate: lr=1.25e-05.
===> Epoch[317](60/324): Loss: 0.5987 || Learning rate: lr=1.25e-05.
===> Epoch[317](70/324): Loss: 0.7343 || Learning rate: lr=1.25e-05.
===> Epoch[317](80/324): Loss: 0.6423 || Learning rate: lr=1.25e-05.
===> Epoch[317](90/324): Loss: 0.6176 || Learning rate: lr=1.25e-05.
===> Epoch[317](100/324): Loss: 0.5768 || Learning rate: lr=1.25e-05.
===> Epoch[317](110/324): Loss: 0.5275 || Learning rate: lr=1.25e-05.
===> Epoch[317](120/324): Loss: 0.7949 || Learning rate: lr=1.25e-05.
===> Epoch[317](130/324): Loss: 0.4395 || Learning rate: lr=1.25e-05.
===> Epoch[317](140/324): Loss: 0.7723 || Learning rate: lr=1.25e-05.
===> Epoch[317](150/324): Loss: 0.7224 || Learning rate: lr=1.25e-05.
===> Epoch[317](160/324): Loss: 0.5592 || Learning rate: lr=1.25e-05.
===> Epoch[317](170/324): Loss: 0.4756 || Learning rate: lr=1.25e-05.
===> Epoch[317](180/324): Loss: 0.6889 || Learning rate: lr=1.25e-05.
===> Epoch[317](190/324): Loss: 0.5499 || Learning rate: lr=1.25e-05.
===> Epoch[317](200/324): Loss: 0.6832 || Learning rate: lr=1.25e-05.
===> Epoch[317](210/324): Loss: 0.8095 || Learning rate: lr=1.25e-05.
===> Epoch[317](220/324): Loss: 0.9500 || Learning rate: lr=1.25e-05.
===> Epoch[317](230/324): Loss: 0.9342 || Learning rate: lr=1.25e-05.
===> Epoch[317](240/324): Loss: 0.5918 || Learning rate: lr=1.25e-05.
===> Epoch[317](250/324): Loss: 0.5739 || Learning rate: lr=1.25e-05.
===> Epoch[317](260/324): Loss: 0.7049 || Learning rate: lr=1.25e-05.
===> Epoch[317](270/324): Loss: 0.5196 || Learning rate: lr=1.25e-05.
===> Epoch[317](280/324): Loss: 0.5576 || Learning rate: lr=1.25e-05.
===> Epoch[317](290/324): Loss: 0.4884 || Learning rate: lr=1.25e-05.
===> Epoch[317](300/324): Loss: 0.5220 || Learning rate: lr=1.25e-05.
===> Epoch[317](310/324): Loss: 0.7191 || Learning rate: lr=1.25e-05.
===> Epoch[317](320/324): Loss: 0.6558 || Learning rate: lr=1.25e-05.
===> Epoch[318](10/324): Loss: 0.4745 || Learning rate: lr=1.25e-05.
===> Epoch[318](20/324): Loss: 0.7242 || Learning rate: lr=1.25e-05.
===> Epoch[318](30/324): Loss: 1.0785 || Learning rate: lr=1.25e-05.
===> Epoch[318](40/324): Loss: 0.6276 || Learning rate: lr=1.25e-05.
===> Epoch[318](50/324): Loss: 0.4891 || Learning rate: lr=1.25e-05.
===> Epoch[318](60/324): Loss: 0.6882 || Learning rate: lr=1.25e-05.
===> Epoch[318](70/324): Loss: 0.6518 || Learning rate: lr=1.25e-05.
===> Epoch[318](80/324): Loss: 0.5709 || Learning rate: lr=1.25e-05.
===> Epoch[318](90/324): Loss: 0.6908 || Learning rate: lr=1.25e-05.
===> Epoch[318](100/324): Loss: 0.5358 || Learning rate: lr=1.25e-05.
===> Epoch[318](110/324): Loss: 0.6344 || Learning rate: lr=1.25e-05.
===> Epoch[318](120/324): Loss: 0.7289 || Learning rate: lr=1.25e-05.
===> Epoch[318](130/324): Loss: 0.6372 || Learning rate: lr=1.25e-05.
===> Epoch[318](140/324): Loss: 0.4269 || Learning rate: lr=1.25e-05.
===> Epoch[318](150/324): Loss: 0.8332 || Learning rate: lr=1.25e-05.
===> Epoch[318](160/324): Loss: 0.4883 || Learning rate: lr=1.25e-05.
===> Epoch[318](170/324): Loss: 0.7073 || Learning rate: lr=1.25e-05.
===> Epoch[318](180/324): Loss: 0.6492 || Learning rate: lr=1.25e-05.
===> Epoch[318](190/324): Loss: 0.8344 || Learning rate: lr=1.25e-05.
===> Epoch[318](200/324): Loss: 0.7220 || Learning rate: lr=1.25e-05.
===> Epoch[318](210/324): Loss: 0.6842 || Learning rate: lr=1.25e-05.
===> Epoch[318](220/324): Loss: 0.6059 || Learning rate: lr=1.25e-05.
===> Epoch[318](230/324): Loss: 0.5607 || Learning rate: lr=1.25e-05.
===> Epoch[318](240/324): Loss: 0.8019 || Learning rate: lr=1.25e-05.
===> Epoch[318](250/324): Loss: 0.4809 || Learning rate: lr=1.25e-05.
===> Epoch[318](260/324): Loss: 0.5502 || Learning rate: lr=1.25e-05.
===> Epoch[318](270/324): Loss: 0.8156 || Learning rate: lr=1.25e-05.
===> Epoch[318](280/324): Loss: 0.7394 || Learning rate: lr=1.25e-05.
===> Epoch[318](290/324): Loss: 0.6287 || Learning rate: lr=1.25e-05.
===> Epoch[318](300/324): Loss: 0.6363 || Learning rate: lr=1.25e-05.
===> Epoch[318](310/324): Loss: 0.7562 || Learning rate: lr=1.25e-05.
===> Epoch[318](320/324): Loss: 0.8240 || Learning rate: lr=1.25e-05.
===> Epoch[319](10/324): Loss: 0.8119 || Learning rate: lr=1.25e-05.
===> Epoch[319](20/324): Loss: 0.7264 || Learning rate: lr=1.25e-05.
===> Epoch[319](30/324): Loss: 0.4346 || Learning rate: lr=1.25e-05.
===> Epoch[319](40/324): Loss: 0.9222 || Learning rate: lr=1.25e-05.
===> Epoch[319](50/324): Loss: 0.7727 || Learning rate: lr=1.25e-05.
===> Epoch[319](60/324): Loss: 0.6855 || Learning rate: lr=1.25e-05.
===> Epoch[319](70/324): Loss: 0.5532 || Learning rate: lr=1.25e-05.
===> Epoch[319](80/324): Loss: 0.5324 || Learning rate: lr=1.25e-05.
===> Epoch[319](90/324): Loss: 1.1010 || Learning rate: lr=1.25e-05.
===> Epoch[319](100/324): Loss: 0.5274 || Learning rate: lr=1.25e-05.
===> Epoch[319](110/324): Loss: 0.6057 || Learning rate: lr=1.25e-05.
===> Epoch[319](120/324): Loss: 0.7828 || Learning rate: lr=1.25e-05.
===> Epoch[319](130/324): Loss: 0.6309 || Learning rate: lr=1.25e-05.
===> Epoch[319](140/324): Loss: 0.8024 || Learning rate: lr=1.25e-05.
===> Epoch[319](150/324): Loss: 0.5522 || Learning rate: lr=1.25e-05.
===> Epoch[319](160/324): Loss: 0.6898 || Learning rate: lr=1.25e-05.
===> Epoch[319](170/324): Loss: 0.4402 || Learning rate: lr=1.25e-05.
===> Epoch[319](180/324): Loss: 0.7968 || Learning rate: lr=1.25e-05.
===> Epoch[319](190/324): Loss: 0.6884 || Learning rate: lr=1.25e-05.
===> Epoch[319](200/324): Loss: 0.6135 || Learning rate: lr=1.25e-05.
===> Epoch[319](210/324): Loss: 0.8379 || Learning rate: lr=1.25e-05.
===> Epoch[319](220/324): Loss: 0.6415 || Learning rate: lr=1.25e-05.
===> Epoch[319](230/324): Loss: 0.7167 || Learning rate: lr=1.25e-05.
===> Epoch[319](240/324): Loss: 0.5135 || Learning rate: lr=1.25e-05.
===> Epoch[319](250/324): Loss: 0.5692 || Learning rate: lr=1.25e-05.
===> Epoch[319](260/324): Loss: 0.4320 || Learning rate: lr=1.25e-05.
===> Epoch[319](270/324): Loss: 0.4941 || Learning rate: lr=1.25e-05.
===> Epoch[319](280/324): Loss: 0.7938 || Learning rate: lr=1.25e-05.
===> Epoch[319](290/324): Loss: 0.6696 || Learning rate: lr=1.25e-05.
===> Epoch[319](300/324): Loss: 0.8283 || Learning rate: lr=1.25e-05.
===> Epoch[319](310/324): Loss: 0.5717 || Learning rate: lr=1.25e-05.
===> Epoch[319](320/324): Loss: 0.6118 || Learning rate: lr=1.25e-05.
===> Epoch[320](10/324): Loss: 0.6035 || Learning rate: lr=1.25e-05.
===> Epoch[320](20/324): Loss: 0.5662 || Learning rate: lr=1.25e-05.
===> Epoch[320](30/324): Loss: 0.6272 || Learning rate: lr=1.25e-05.
===> Epoch[320](40/324): Loss: 0.6347 || Learning rate: lr=1.25e-05.
===> Epoch[320](50/324): Loss: 0.9649 || Learning rate: lr=1.25e-05.
===> Epoch[320](60/324): Loss: 0.4910 || Learning rate: lr=1.25e-05.
===> Epoch[320](70/324): Loss: 0.5942 || Learning rate: lr=1.25e-05.
===> Epoch[320](80/324): Loss: 0.6618 || Learning rate: lr=1.25e-05.
===> Epoch[320](90/324): Loss: 0.6141 || Learning rate: lr=1.25e-05.
===> Epoch[320](100/324): Loss: 0.7801 || Learning rate: lr=1.25e-05.
===> Epoch[320](110/324): Loss: 0.6239 || Learning rate: lr=1.25e-05.
===> Epoch[320](120/324): Loss: 0.5777 || Learning rate: lr=1.25e-05.
===> Epoch[320](130/324): Loss: 0.5567 || Learning rate: lr=1.25e-05.
===> Epoch[320](140/324): Loss: 0.6315 || Learning rate: lr=1.25e-05.
===> Epoch[320](150/324): Loss: 0.8012 || Learning rate: lr=1.25e-05.
===> Epoch[320](160/324): Loss: 0.5914 || Learning rate: lr=1.25e-05.
===> Epoch[320](170/324): Loss: 0.6897 || Learning rate: lr=1.25e-05.
===> Epoch[320](180/324): Loss: 0.7173 || Learning rate: lr=1.25e-05.
===> Epoch[320](190/324): Loss: 0.7051 || Learning rate: lr=1.25e-05.
===> Epoch[320](200/324): Loss: 0.7551 || Learning rate: lr=1.25e-05.
===> Epoch[320](210/324): Loss: 0.4737 || Learning rate: lr=1.25e-05.
===> Epoch[320](220/324): Loss: 0.8084 || Learning rate: lr=1.25e-05.
===> Epoch[320](230/324): Loss: 0.5626 || Learning rate: lr=1.25e-05.
===> Epoch[320](240/324): Loss: 0.6046 || Learning rate: lr=1.25e-05.
===> Epoch[320](250/324): Loss: 0.8509 || Learning rate: lr=1.25e-05.
===> Epoch[320](260/324): Loss: 0.8073 || Learning rate: lr=1.25e-05.
===> Epoch[320](270/324): Loss: 0.5899 || Learning rate: lr=1.25e-05.
===> Epoch[320](280/324): Loss: 0.6054 || Learning rate: lr=1.25e-05.
===> Epoch[320](290/324): Loss: 0.7192 || Learning rate: lr=1.25e-05.
===> Epoch[320](300/324): Loss: 0.8863 || Learning rate: lr=1.25e-05.
===> Epoch[320](310/324): Loss: 0.6871 || Learning rate: lr=1.25e-05.
===> Epoch[320](320/324): Loss: 0.6888 || Learning rate: lr=1.25e-05.
Checkpoint saved to weights/epoch_v2_320.pth
===> Epoch[321](10/324): Loss: 0.8820 || Learning rate: lr=1.25e-05.
===> Epoch[321](20/324): Loss: 0.9661 || Learning rate: lr=1.25e-05.
===> Epoch[321](30/324): Loss: 0.4744 || Learning rate: lr=1.25e-05.
===> Epoch[321](40/324): Loss: 0.5779 || Learning rate: lr=1.25e-05.
===> Epoch[321](50/324): Loss: 0.7670 || Learning rate: lr=1.25e-05.
===> Epoch[321](60/324): Loss: 0.6984 || Learning rate: lr=1.25e-05.
===> Epoch[321](70/324): Loss: 1.0874 || Learning rate: lr=1.25e-05.
===> Epoch[321](80/324): Loss: 0.5758 || Learning rate: lr=1.25e-05.
===> Epoch[321](90/324): Loss: 0.9188 || Learning rate: lr=1.25e-05.
===> Epoch[321](100/324): Loss: 0.7229 || Learning rate: lr=1.25e-05.
===> Epoch[321](110/324): Loss: 0.7546 || Learning rate: lr=1.25e-05.
===> Epoch[321](120/324): Loss: 0.8034 || Learning rate: lr=1.25e-05.
===> Epoch[321](130/324): Loss: 0.7408 || Learning rate: lr=1.25e-05.
===> Epoch[321](140/324): Loss: 0.8485 || Learning rate: lr=1.25e-05.
===> Epoch[321](150/324): Loss: 0.4288 || Learning rate: lr=1.25e-05.
===> Epoch[321](160/324): Loss: 0.9234 || Learning rate: lr=1.25e-05.
===> Epoch[321](170/324): Loss: 0.5666 || Learning rate: lr=1.25e-05.
===> Epoch[321](180/324): Loss: 0.4979 || Learning rate: lr=1.25e-05.
===> Epoch[321](190/324): Loss: 0.7029 || Learning rate: lr=1.25e-05.
===> Epoch[321](200/324): Loss: 0.7891 || Learning rate: lr=1.25e-05.
===> Epoch[321](210/324): Loss: 0.7697 || Learning rate: lr=1.25e-05.
===> Epoch[321](220/324): Loss: 0.6397 || Learning rate: lr=1.25e-05.
===> Epoch[321](230/324): Loss: 0.6786 || Learning rate: lr=1.25e-05.
===> Epoch[321](240/324): Loss: 0.7066 || Learning rate: lr=1.25e-05.
===> Epoch[321](250/324): Loss: 0.8562 || Learning rate: lr=1.25e-05.
===> Epoch[321](260/324): Loss: 0.6426 || Learning rate: lr=1.25e-05.
===> Epoch[321](270/324): Loss: 0.6550 || Learning rate: lr=1.25e-05.
===> Epoch[321](280/324): Loss: 0.4282 || Learning rate: lr=1.25e-05.
===> Epoch[321](290/324): Loss: 0.5696 || Learning rate: lr=1.25e-05.
===> Epoch[321](300/324): Loss: 0.7602 || Learning rate: lr=1.25e-05.
===> Epoch[321](310/324): Loss: 0.3583 || Learning rate: lr=1.25e-05.
===> Epoch[321](320/324): Loss: 0.7196 || Learning rate: lr=1.25e-05.
===> Epoch[322](10/324): Loss: 0.7257 || Learning rate: lr=1.25e-05.
===> Epoch[322](20/324): Loss: 0.8188 || Learning rate: lr=1.25e-05.
===> Epoch[322](30/324): Loss: 0.4783 || Learning rate: lr=1.25e-05.
===> Epoch[322](40/324): Loss: 0.6513 || Learning rate: lr=1.25e-05.
===> Epoch[322](50/324): Loss: 0.7699 || Learning rate: lr=1.25e-05.
===> Epoch[322](60/324): Loss: 0.6745 || Learning rate: lr=1.25e-05.
===> Epoch[322](70/324): Loss: 0.8310 || Learning rate: lr=1.25e-05.
===> Epoch[322](80/324): Loss: 0.6885 || Learning rate: lr=1.25e-05.
===> Epoch[322](90/324): Loss: 0.5703 || Learning rate: lr=1.25e-05.
===> Epoch[322](100/324): Loss: 0.6555 || Learning rate: lr=1.25e-05.
===> Epoch[322](110/324): Loss: 0.7758 || Learning rate: lr=1.25e-05.
===> Epoch[322](120/324): Loss: 0.6221 || Learning rate: lr=1.25e-05.
===> Epoch[322](130/324): Loss: 0.8054 || Learning rate: lr=1.25e-05.
===> Epoch[322](140/324): Loss: 0.7053 || Learning rate: lr=1.25e-05.
===> Epoch[322](150/324): Loss: 0.5850 || Learning rate: lr=1.25e-05.
===> Epoch[322](160/324): Loss: 0.5995 || Learning rate: lr=1.25e-05.
===> Epoch[322](170/324): Loss: 0.6238 || Learning rate: lr=1.25e-05.
===> Epoch[322](180/324): Loss: 0.8199 || Learning rate: lr=1.25e-05.
===> Epoch[322](190/324): Loss: 0.6243 || Learning rate: lr=1.25e-05.
===> Epoch[322](200/324): Loss: 0.4778 || Learning rate: lr=1.25e-05.
===> Epoch[322](210/324): Loss: 0.8828 || Learning rate: lr=1.25e-05.
===> Epoch[322](220/324): Loss: 0.4975 || Learning rate: lr=1.25e-05.
===> Epoch[322](230/324): Loss: 0.5760 || Learning rate: lr=1.25e-05.
===> Epoch[322](240/324): Loss: 0.4908 || Learning rate: lr=1.25e-05.
===> Epoch[322](250/324): Loss: 0.4886 || Learning rate: lr=1.25e-05.
===> Epoch[322](260/324): Loss: 0.6098 || Learning rate: lr=1.25e-05.
===> Epoch[322](270/324): Loss: 0.6780 || Learning rate: lr=1.25e-05.
===> Epoch[322](280/324): Loss: 0.7235 || Learning rate: lr=1.25e-05.
===> Epoch[322](290/324): Loss: 0.5051 || Learning rate: lr=1.25e-05.
===> Epoch[322](300/324): Loss: 0.8153 || Learning rate: lr=1.25e-05.
===> Epoch[322](310/324): Loss: 0.3899 || Learning rate: lr=1.25e-05.
===> Epoch[322](320/324): Loss: 0.5322 || Learning rate: lr=1.25e-05.
===> Epoch[323](10/324): Loss: 0.5297 || Learning rate: lr=1.25e-05.
===> Epoch[323](20/324): Loss: 0.9635 || Learning rate: lr=1.25e-05.
===> Epoch[323](30/324): Loss: 0.9047 || Learning rate: lr=1.25e-05.
===> Epoch[323](40/324): Loss: 0.5131 || Learning rate: lr=1.25e-05.
===> Epoch[323](50/324): Loss: 0.5749 || Learning rate: lr=1.25e-05.
===> Epoch[323](60/324): Loss: 0.6491 || Learning rate: lr=1.25e-05.
===> Epoch[323](70/324): Loss: 0.5016 || Learning rate: lr=1.25e-05.
===> Epoch[323](80/324): Loss: 0.7086 || Learning rate: lr=1.25e-05.
===> Epoch[323](90/324): Loss: 0.7602 || Learning rate: lr=1.25e-05.
===> Epoch[323](100/324): Loss: 0.7981 || Learning rate: lr=1.25e-05.
===> Epoch[323](110/324): Loss: 0.4868 || Learning rate: lr=1.25e-05.
===> Epoch[323](120/324): Loss: 0.5599 || Learning rate: lr=1.25e-05.
===> Epoch[323](130/324): Loss: 0.4308 || Learning rate: lr=1.25e-05.
===> Epoch[323](140/324): Loss: 0.5901 || Learning rate: lr=1.25e-05.
===> Epoch[323](150/324): Loss: 0.5493 || Learning rate: lr=1.25e-05.
===> Epoch[323](160/324): Loss: 0.6453 || Learning rate: lr=1.25e-05.
===> Epoch[323](170/324): Loss: 0.4002 || Learning rate: lr=1.25e-05.
===> Epoch[323](180/324): Loss: 0.8833 || Learning rate: lr=1.25e-05.
===> Epoch[323](190/324): Loss: 0.6679 || Learning rate: lr=1.25e-05.
===> Epoch[323](200/324): Loss: 0.6654 || Learning rate: lr=1.25e-05.
===> Epoch[323](210/324): Loss: 0.6867 || Learning rate: lr=1.25e-05.
===> Epoch[323](220/324): Loss: 0.6840 || Learning rate: lr=1.25e-05.
===> Epoch[323](230/324): Loss: 0.8021 || Learning rate: lr=1.25e-05.
===> Epoch[323](240/324): Loss: 0.8078 || Learning rate: lr=1.25e-05.
===> Epoch[323](250/324): Loss: 0.5017 || Learning rate: lr=1.25e-05.
===> Epoch[323](260/324): Loss: 0.6333 || Learning rate: lr=1.25e-05.
===> Epoch[323](270/324): Loss: 0.7218 || Learning rate: lr=1.25e-05.
===> Epoch[323](280/324): Loss: 0.6495 || Learning rate: lr=1.25e-05.
===> Epoch[323](290/324): Loss: 0.5718 || Learning rate: lr=1.25e-05.
===> Epoch[323](300/324): Loss: 0.7822 || Learning rate: lr=1.25e-05.
===> Epoch[323](310/324): Loss: 0.9957 || Learning rate: lr=1.25e-05.
===> Epoch[323](320/324): Loss: 0.6221 || Learning rate: lr=1.25e-05.
===> Epoch[324](10/324): Loss: 0.5040 || Learning rate: lr=1.25e-05.
===> Epoch[324](20/324): Loss: 0.7147 || Learning rate: lr=1.25e-05.
===> Epoch[324](30/324): Loss: 0.7472 || Learning rate: lr=1.25e-05.
===> Epoch[324](40/324): Loss: 0.6487 || Learning rate: lr=1.25e-05.
===> Epoch[324](50/324): Loss: 0.9765 || Learning rate: lr=1.25e-05.
===> Epoch[324](60/324): Loss: 0.6471 || Learning rate: lr=1.25e-05.
===> Epoch[324](70/324): Loss: 0.4261 || Learning rate: lr=1.25e-05.
===> Epoch[324](80/324): Loss: 0.7022 || Learning rate: lr=1.25e-05.
===> Epoch[324](90/324): Loss: 0.6346 || Learning rate: lr=1.25e-05.
===> Epoch[324](100/324): Loss: 0.6382 || Learning rate: lr=1.25e-05.
===> Epoch[324](110/324): Loss: 0.6426 || Learning rate: lr=1.25e-05.
===> Epoch[324](120/324): Loss: 0.9701 || Learning rate: lr=1.25e-05.
===> Epoch[324](130/324): Loss: 0.5453 || Learning rate: lr=1.25e-05.
===> Epoch[324](140/324): Loss: 0.4343 || Learning rate: lr=1.25e-05.
===> Epoch[324](150/324): Loss: 0.6493 || Learning rate: lr=1.25e-05.
===> Epoch[324](160/324): Loss: 0.8099 || Learning rate: lr=1.25e-05.
===> Epoch[324](170/324): Loss: 0.5421 || Learning rate: lr=1.25e-05.
===> Epoch[324](180/324): Loss: 0.5272 || Learning rate: lr=1.25e-05.
===> Epoch[324](190/324): Loss: 0.5733 || Learning rate: lr=1.25e-05.
===> Epoch[324](200/324): Loss: 0.6410 || Learning rate: lr=1.25e-05.
===> Epoch[324](210/324): Loss: 0.8830 || Learning rate: lr=1.25e-05.
===> Epoch[324](220/324): Loss: 0.6071 || Learning rate: lr=1.25e-05.
===> Epoch[324](230/324): Loss: 0.8480 || Learning rate: lr=1.25e-05.
===> Epoch[324](240/324): Loss: 0.7091 || Learning rate: lr=1.25e-05.
===> Epoch[324](250/324): Loss: 1.0078 || Learning rate: lr=1.25e-05.
===> Epoch[324](260/324): Loss: 0.7962 || Learning rate: lr=1.25e-05.
===> Epoch[324](270/324): Loss: 0.8530 || Learning rate: lr=1.25e-05.
===> Epoch[324](280/324): Loss: 0.5195 || Learning rate: lr=1.25e-05.
===> Epoch[324](290/324): Loss: 0.4886 || Learning rate: lr=1.25e-05.
===> Epoch[324](300/324): Loss: 0.7898 || Learning rate: lr=1.25e-05.
===> Epoch[324](310/324): Loss: 0.4585 || Learning rate: lr=1.25e-05.
===> Epoch[324](320/324): Loss: 0.7451 || Learning rate: lr=1.25e-05.
===> Epoch[325](10/324): Loss: 0.6045 || Learning rate: lr=1.25e-05.
===> Epoch[325](20/324): Loss: 0.7382 || Learning rate: lr=1.25e-05.
===> Epoch[325](30/324): Loss: 0.5596 || Learning rate: lr=1.25e-05.
===> Epoch[325](40/324): Loss: 0.5024 || Learning rate: lr=1.25e-05.
===> Epoch[325](50/324): Loss: 0.6293 || Learning rate: lr=1.25e-05.
===> Epoch[325](60/324): Loss: 0.5756 || Learning rate: lr=1.25e-05.
===> Epoch[325](70/324): Loss: 0.8168 || Learning rate: lr=1.25e-05.
===> Epoch[325](80/324): Loss: 0.8786 || Learning rate: lr=1.25e-05.
===> Epoch[325](90/324): Loss: 0.7986 || Learning rate: lr=1.25e-05.
===> Epoch[325](100/324): Loss: 0.6112 || Learning rate: lr=1.25e-05.
===> Epoch[325](110/324): Loss: 0.6651 || Learning rate: lr=1.25e-05.
===> Epoch[325](120/324): Loss: 0.6272 || Learning rate: lr=1.25e-05.
===> Epoch[325](130/324): Loss: 0.6539 || Learning rate: lr=1.25e-05.
===> Epoch[325](140/324): Loss: 0.9249 || Learning rate: lr=1.25e-05.
===> Epoch[325](150/324): Loss: 0.5841 || Learning rate: lr=1.25e-05.
===> Epoch[325](160/324): Loss: 0.5263 || Learning rate: lr=1.25e-05.
===> Epoch[325](170/324): Loss: 0.5303 || Learning rate: lr=1.25e-05.
===> Epoch[325](180/324): Loss: 0.8757 || Learning rate: lr=1.25e-05.
===> Epoch[325](190/324): Loss: 0.6424 || Learning rate: lr=1.25e-05.
===> Epoch[325](200/324): Loss: 0.7101 || Learning rate: lr=1.25e-05.
===> Epoch[325](210/324): Loss: 0.6596 || Learning rate: lr=1.25e-05.
===> Epoch[325](220/324): Loss: 0.5461 || Learning rate: lr=1.25e-05.
===> Epoch[325](230/324): Loss: 0.5361 || Learning rate: lr=1.25e-05.
===> Epoch[325](240/324): Loss: 0.6559 || Learning rate: lr=1.25e-05.
===> Epoch[325](250/324): Loss: 0.6060 || Learning rate: lr=1.25e-05.
===> Epoch[325](260/324): Loss: 1.0542 || Learning rate: lr=1.25e-05.
===> Epoch[325](270/324): Loss: 0.8643 || Learning rate: lr=1.25e-05.
===> Epoch[325](280/324): Loss: 0.7923 || Learning rate: lr=1.25e-05.
===> Epoch[325](290/324): Loss: 0.7922 || Learning rate: lr=1.25e-05.
===> Epoch[325](300/324): Loss: 0.7472 || Learning rate: lr=1.25e-05.
===> Epoch[325](310/324): Loss: 0.4895 || Learning rate: lr=1.25e-05.
===> Epoch[325](320/324): Loss: 0.5801 || Learning rate: lr=1.25e-05.
===> Epoch[326](10/324): Loss: 0.5795 || Learning rate: lr=1.25e-05.
===> Epoch[326](20/324): Loss: 0.9046 || Learning rate: lr=1.25e-05.
===> Epoch[326](30/324): Loss: 0.6199 || Learning rate: lr=1.25e-05.
===> Epoch[326](40/324): Loss: 0.6763 || Learning rate: lr=1.25e-05.
===> Epoch[326](50/324): Loss: 0.6017 || Learning rate: lr=1.25e-05.
===> Epoch[326](60/324): Loss: 0.8149 || Learning rate: lr=1.25e-05.
===> Epoch[326](70/324): Loss: 0.5641 || Learning rate: lr=1.25e-05.
===> Epoch[326](80/324): Loss: 0.5700 || Learning rate: lr=1.25e-05.
===> Epoch[326](90/324): Loss: 0.6197 || Learning rate: lr=1.25e-05.
===> Epoch[326](100/324): Loss: 0.8177 || Learning rate: lr=1.25e-05.
===> Epoch[326](110/324): Loss: 0.5604 || Learning rate: lr=1.25e-05.
===> Epoch[326](120/324): Loss: 0.7085 || Learning rate: lr=1.25e-05.
===> Epoch[326](130/324): Loss: 0.5003 || Learning rate: lr=1.25e-05.
===> Epoch[326](140/324): Loss: 0.4047 || Learning rate: lr=1.25e-05.
===> Epoch[326](150/324): Loss: 0.8312 || Learning rate: lr=1.25e-05.
===> Epoch[326](160/324): Loss: 0.5280 || Learning rate: lr=1.25e-05.
===> Epoch[326](170/324): Loss: 0.6181 || Learning rate: lr=1.25e-05.
===> Epoch[326](180/324): Loss: 0.7513 || Learning rate: lr=1.25e-05.
===> Epoch[326](190/324): Loss: 0.6589 || Learning rate: lr=1.25e-05.
===> Epoch[326](200/324): Loss: 0.4738 || Learning rate: lr=1.25e-05.
===> Epoch[326](210/324): Loss: 0.6527 || Learning rate: lr=1.25e-05.
===> Epoch[326](220/324): Loss: 0.8427 || Learning rate: lr=1.25e-05.
===> Epoch[326](230/324): Loss: 0.4876 || Learning rate: lr=1.25e-05.
===> Epoch[326](240/324): Loss: 0.8589 || Learning rate: lr=1.25e-05.
===> Epoch[326](250/324): Loss: 0.8437 || Learning rate: lr=1.25e-05.
===> Epoch[326](260/324): Loss: 0.5725 || Learning rate: lr=1.25e-05.
===> Epoch[326](270/324): Loss: 0.6751 || Learning rate: lr=1.25e-05.
===> Epoch[326](280/324): Loss: 0.7310 || Learning rate: lr=1.25e-05.
===> Epoch[326](290/324): Loss: 0.8228 || Learning rate: lr=1.25e-05.
===> Epoch[326](300/324): Loss: 0.4443 || Learning rate: lr=1.25e-05.
===> Epoch[326](310/324): Loss: 0.6109 || Learning rate: lr=1.25e-05.
===> Epoch[326](320/324): Loss: 0.5644 || Learning rate: lr=1.25e-05.
===> Epoch[327](10/324): Loss: 0.5194 || Learning rate: lr=1.25e-05.
===> Epoch[327](20/324): Loss: 0.7459 || Learning rate: lr=1.25e-05.
===> Epoch[327](30/324): Loss: 0.7657 || Learning rate: lr=1.25e-05.
===> Epoch[327](40/324): Loss: 0.6810 || Learning rate: lr=1.25e-05.
===> Epoch[327](50/324): Loss: 0.9724 || Learning rate: lr=1.25e-05.
===> Epoch[327](60/324): Loss: 0.6296 || Learning rate: lr=1.25e-05.
===> Epoch[327](70/324): Loss: 0.4922 || Learning rate: lr=1.25e-05.
===> Epoch[327](80/324): Loss: 0.8493 || Learning rate: lr=1.25e-05.
===> Epoch[327](90/324): Loss: 0.7235 || Learning rate: lr=1.25e-05.
===> Epoch[327](100/324): Loss: 0.4104 || Learning rate: lr=1.25e-05.
===> Epoch[327](110/324): Loss: 0.7657 || Learning rate: lr=1.25e-05.
===> Epoch[327](120/324): Loss: 0.7082 || Learning rate: lr=1.25e-05.
===> Epoch[327](130/324): Loss: 0.4749 || Learning rate: lr=1.25e-05.
===> Epoch[327](140/324): Loss: 0.5468 || Learning rate: lr=1.25e-05.
===> Epoch[327](150/324): Loss: 0.8564 || Learning rate: lr=1.25e-05.
===> Epoch[327](160/324): Loss: 0.7690 || Learning rate: lr=1.25e-05.
===> Epoch[327](170/324): Loss: 0.7542 || Learning rate: lr=1.25e-05.
===> Epoch[327](180/324): Loss: 0.6346 || Learning rate: lr=1.25e-05.
===> Epoch[327](190/324): Loss: 0.5231 || Learning rate: lr=1.25e-05.
===> Epoch[327](200/324): Loss: 0.6836 || Learning rate: lr=1.25e-05.
===> Epoch[327](210/324): Loss: 1.2309 || Learning rate: lr=1.25e-05.
===> Epoch[327](220/324): Loss: 0.5185 || Learning rate: lr=1.25e-05.
===> Epoch[327](230/324): Loss: 0.5238 || Learning rate: lr=1.25e-05.
===> Epoch[327](240/324): Loss: 0.6726 || Learning rate: lr=1.25e-05.
===> Epoch[327](250/324): Loss: 0.4423 || Learning rate: lr=1.25e-05.
===> Epoch[327](260/324): Loss: 0.6320 || Learning rate: lr=1.25e-05.
===> Epoch[327](270/324): Loss: 0.7801 || Learning rate: lr=1.25e-05.
===> Epoch[327](280/324): Loss: 0.5314 || Learning rate: lr=1.25e-05.
===> Epoch[327](290/324): Loss: 0.5101 || Learning rate: lr=1.25e-05.
===> Epoch[327](300/324): Loss: 0.8044 || Learning rate: lr=1.25e-05.
===> Epoch[327](310/324): Loss: 0.7533 || Learning rate: lr=1.25e-05.
===> Epoch[327](320/324): Loss: 0.5029 || Learning rate: lr=1.25e-05.
===> Epoch[328](10/324): Loss: 0.6472 || Learning rate: lr=1.25e-05.
===> Epoch[328](20/324): Loss: 0.7408 || Learning rate: lr=1.25e-05.
===> Epoch[328](30/324): Loss: 0.6954 || Learning rate: lr=1.25e-05.
===> Epoch[328](40/324): Loss: 0.8897 || Learning rate: lr=1.25e-05.
===> Epoch[328](50/324): Loss: 0.7222 || Learning rate: lr=1.25e-05.
===> Epoch[328](60/324): Loss: 0.5705 || Learning rate: lr=1.25e-05.
===> Epoch[328](70/324): Loss: 0.6707 || Learning rate: lr=1.25e-05.
===> Epoch[328](80/324): Loss: 0.8946 || Learning rate: lr=1.25e-05.
===> Epoch[328](90/324): Loss: 0.7796 || Learning rate: lr=1.25e-05.
===> Epoch[328](100/324): Loss: 0.6142 || Learning rate: lr=1.25e-05.
===> Epoch[328](110/324): Loss: 0.5795 || Learning rate: lr=1.25e-05.
===> Epoch[328](120/324): Loss: 0.8140 || Learning rate: lr=1.25e-05.
===> Epoch[328](130/324): Loss: 0.6106 || Learning rate: lr=1.25e-05.
===> Epoch[328](140/324): Loss: 0.6892 || Learning rate: lr=1.25e-05.
===> Epoch[328](150/324): Loss: 0.5099 || Learning rate: lr=1.25e-05.
===> Epoch[328](160/324): Loss: 0.4140 || Learning rate: lr=1.25e-05.
===> Epoch[328](170/324): Loss: 0.7777 || Learning rate: lr=1.25e-05.
===> Epoch[328](180/324): Loss: 0.6096 || Learning rate: lr=1.25e-05.
===> Epoch[328](190/324): Loss: 0.7948 || Learning rate: lr=1.25e-05.
===> Epoch[328](200/324): Loss: 0.5678 || Learning rate: lr=1.25e-05.
===> Epoch[328](210/324): Loss: 0.7057 || Learning rate: lr=1.25e-05.
===> Epoch[328](220/324): Loss: 0.7155 || Learning rate: lr=1.25e-05.
===> Epoch[328](230/324): Loss: 0.5144 || Learning rate: lr=1.25e-05.
===> Epoch[328](240/324): Loss: 0.7366 || Learning rate: lr=1.25e-05.
===> Epoch[328](250/324): Loss: 0.8895 || Learning rate: lr=1.25e-05.
===> Epoch[328](260/324): Loss: 0.6071 || Learning rate: lr=1.25e-05.
===> Epoch[328](270/324): Loss: 0.7555 || Learning rate: lr=1.25e-05.
===> Epoch[328](280/324): Loss: 0.6608 || Learning rate: lr=1.25e-05.
===> Epoch[328](290/324): Loss: 0.8137 || Learning rate: lr=1.25e-05.
===> Epoch[328](300/324): Loss: 0.9879 || Learning rate: lr=1.25e-05.
===> Epoch[328](310/324): Loss: 0.7405 || Learning rate: lr=1.25e-05.
===> Epoch[328](320/324): Loss: 0.5744 || Learning rate: lr=1.25e-05.
===> Epoch[329](10/324): Loss: 0.6218 || Learning rate: lr=1.25e-05.
===> Epoch[329](20/324): Loss: 0.5540 || Learning rate: lr=1.25e-05.
===> Epoch[329](30/324): Loss: 0.5832 || Learning rate: lr=1.25e-05.
===> Epoch[329](40/324): Loss: 0.6679 || Learning rate: lr=1.25e-05.
===> Epoch[329](50/324): Loss: 0.9688 || Learning rate: lr=1.25e-05.
===> Epoch[329](60/324): Loss: 0.7396 || Learning rate: lr=1.25e-05.
===> Epoch[329](70/324): Loss: 0.5031 || Learning rate: lr=1.25e-05.
===> Epoch[329](80/324): Loss: 0.7934 || Learning rate: lr=1.25e-05.
===> Epoch[329](90/324): Loss: 0.5579 || Learning rate: lr=1.25e-05.
===> Epoch[329](100/324): Loss: 0.5105 || Learning rate: lr=1.25e-05.
===> Epoch[329](110/324): Loss: 0.5615 || Learning rate: lr=1.25e-05.
===> Epoch[329](120/324): Loss: 0.9222 || Learning rate: lr=1.25e-05.
===> Epoch[329](130/324): Loss: 0.5392 || Learning rate: lr=1.25e-05.
===> Epoch[329](140/324): Loss: 0.7457 || Learning rate: lr=1.25e-05.
===> Epoch[329](150/324): Loss: 0.4998 || Learning rate: lr=1.25e-05.
===> Epoch[329](160/324): Loss: 0.5107 || Learning rate: lr=1.25e-05.
===> Epoch[329](170/324): Loss: 0.6659 || Learning rate: lr=1.25e-05.
===> Epoch[329](180/324): Loss: 0.5875 || Learning rate: lr=1.25e-05.
===> Epoch[329](190/324): Loss: 0.5469 || Learning rate: lr=1.25e-05.
===> Epoch[329](200/324): Loss: 0.7747 || Learning rate: lr=1.25e-05.
===> Epoch[329](210/324): Loss: 0.6963 || Learning rate: lr=1.25e-05.
===> Epoch[329](220/324): Loss: 0.6788 || Learning rate: lr=1.25e-05.
===> Epoch[329](230/324): Loss: 0.4685 || Learning rate: lr=1.25e-05.
===> Epoch[329](240/324): Loss: 0.7654 || Learning rate: lr=1.25e-05.
===> Epoch[329](250/324): Loss: 0.6476 || Learning rate: lr=1.25e-05.
===> Epoch[329](260/324): Loss: 0.4110 || Learning rate: lr=1.25e-05.
===> Epoch[329](270/324): Loss: 0.5693 || Learning rate: lr=1.25e-05.
===> Epoch[329](280/324): Loss: 0.7718 || Learning rate: lr=1.25e-05.
===> Epoch[329](290/324): Loss: 0.3963 || Learning rate: lr=1.25e-05.
===> Epoch[329](300/324): Loss: 0.6819 || Learning rate: lr=1.25e-05.
===> Epoch[329](310/324): Loss: 0.7274 || Learning rate: lr=1.25e-05.
===> Epoch[329](320/324): Loss: 0.6846 || Learning rate: lr=1.25e-05.
===> Epoch[330](10/324): Loss: 0.7633 || Learning rate: lr=1.25e-05.
===> Epoch[330](20/324): Loss: 0.7891 || Learning rate: lr=1.25e-05.
===> Epoch[330](30/324): Loss: 0.4336 || Learning rate: lr=1.25e-05.
===> Epoch[330](40/324): Loss: 0.5000 || Learning rate: lr=1.25e-05.
===> Epoch[330](50/324): Loss: 0.8472 || Learning rate: lr=1.25e-05.
===> Epoch[330](60/324): Loss: 0.4033 || Learning rate: lr=1.25e-05.
===> Epoch[330](70/324): Loss: 0.5810 || Learning rate: lr=1.25e-05.
===> Epoch[330](80/324): Loss: 0.4757 || Learning rate: lr=1.25e-05.
===> Epoch[330](90/324): Loss: 0.4333 || Learning rate: lr=1.25e-05.
===> Epoch[330](100/324): Loss: 0.5780 || Learning rate: lr=1.25e-05.
===> Epoch[330](110/324): Loss: 0.6699 || Learning rate: lr=1.25e-05.
===> Epoch[330](120/324): Loss: 0.7215 || Learning rate: lr=1.25e-05.
===> Epoch[330](130/324): Loss: 0.8753 || Learning rate: lr=1.25e-05.
===> Epoch[330](140/324): Loss: 1.0212 || Learning rate: lr=1.25e-05.
===> Epoch[330](150/324): Loss: 0.5589 || Learning rate: lr=1.25e-05.
===> Epoch[330](160/324): Loss: 0.6357 || Learning rate: lr=1.25e-05.
===> Epoch[330](170/324): Loss: 0.8103 || Learning rate: lr=1.25e-05.
===> Epoch[330](180/324): Loss: 0.4961 || Learning rate: lr=1.25e-05.
===> Epoch[330](190/324): Loss: 0.7552 || Learning rate: lr=1.25e-05.
===> Epoch[330](200/324): Loss: 0.7180 || Learning rate: lr=1.25e-05.
===> Epoch[330](210/324): Loss: 0.7712 || Learning rate: lr=1.25e-05.
===> Epoch[330](220/324): Loss: 0.6090 || Learning rate: lr=1.25e-05.
===> Epoch[330](230/324): Loss: 0.5414 || Learning rate: lr=1.25e-05.
===> Epoch[330](240/324): Loss: 0.6485 || Learning rate: lr=1.25e-05.
===> Epoch[330](250/324): Loss: 0.5204 || Learning rate: lr=1.25e-05.
===> Epoch[330](260/324): Loss: 0.6228 || Learning rate: lr=1.25e-05.
===> Epoch[330](270/324): Loss: 0.7479 || Learning rate: lr=1.25e-05.
===> Epoch[330](280/324): Loss: 0.6715 || Learning rate: lr=1.25e-05.
===> Epoch[330](290/324): Loss: 0.5421 || Learning rate: lr=1.25e-05.
===> Epoch[330](300/324): Loss: 0.4430 || Learning rate: lr=1.25e-05.
===> Epoch[330](310/324): Loss: 0.4620 || Learning rate: lr=1.25e-05.
===> Epoch[330](320/324): Loss: 0.5963 || Learning rate: lr=1.25e-05.
===> Epoch[331](10/324): Loss: 0.7150 || Learning rate: lr=1.25e-05.
===> Epoch[331](20/324): Loss: 0.6988 || Learning rate: lr=1.25e-05.
===> Epoch[331](30/324): Loss: 0.7791 || Learning rate: lr=1.25e-05.
===> Epoch[331](40/324): Loss: 0.5961 || Learning rate: lr=1.25e-05.
===> Epoch[331](50/324): Loss: 0.5529 || Learning rate: lr=1.25e-05.
===> Epoch[331](60/324): Loss: 0.7008 || Learning rate: lr=1.25e-05.
===> Epoch[331](70/324): Loss: 0.5918 || Learning rate: lr=1.25e-05.
===> Epoch[331](80/324): Loss: 0.6425 || Learning rate: lr=1.25e-05.
===> Epoch[331](90/324): Loss: 0.6213 || Learning rate: lr=1.25e-05.
===> Epoch[331](100/324): Loss: 0.6502 || Learning rate: lr=1.25e-05.
===> Epoch[331](110/324): Loss: 0.9235 || Learning rate: lr=1.25e-05.
===> Epoch[331](120/324): Loss: 0.5401 || Learning rate: lr=1.25e-05.
===> Epoch[331](130/324): Loss: 0.5708 || Learning rate: lr=1.25e-05.
===> Epoch[331](140/324): Loss: 0.5086 || Learning rate: lr=1.25e-05.
===> Epoch[331](150/324): Loss: 0.8142 || Learning rate: lr=1.25e-05.
===> Epoch[331](160/324): Loss: 0.4847 || Learning rate: lr=1.25e-05.
===> Epoch[331](170/324): Loss: 0.6381 || Learning rate: lr=1.25e-05.
===> Epoch[331](180/324): Loss: 0.7091 || Learning rate: lr=1.25e-05.
===> Epoch[331](190/324): Loss: 0.5968 || Learning rate: lr=1.25e-05.
===> Epoch[331](200/324): Loss: 0.6566 || Learning rate: lr=1.25e-05.
===> Epoch[331](210/324): Loss: 0.6368 || Learning rate: lr=1.25e-05.
===> Epoch[331](220/324): Loss: 1.0763 || Learning rate: lr=1.25e-05.
===> Epoch[331](230/324): Loss: 0.5871 || Learning rate: lr=1.25e-05.
===> Epoch[331](240/324): Loss: 0.6614 || Learning rate: lr=1.25e-05.
===> Epoch[331](250/324): Loss: 0.6480 || Learning rate: lr=1.25e-05.
===> Epoch[331](260/324): Loss: 0.5815 || Learning rate: lr=1.25e-05.
===> Epoch[331](270/324): Loss: 0.8430 || Learning rate: lr=1.25e-05.
===> Epoch[331](280/324): Loss: 0.7070 || Learning rate: lr=1.25e-05.
===> Epoch[331](290/324): Loss: 0.9461 || Learning rate: lr=1.25e-05.
===> Epoch[331](300/324): Loss: 0.4265 || Learning rate: lr=1.25e-05.
===> Epoch[331](310/324): Loss: 0.6248 || Learning rate: lr=1.25e-05.
===> Epoch[331](320/324): Loss: 0.5702 || Learning rate: lr=1.25e-05.
===> Epoch[332](10/324): Loss: 0.4908 || Learning rate: lr=1.25e-05.
===> Epoch[332](20/324): Loss: 0.8598 || Learning rate: lr=1.25e-05.
===> Epoch[332](30/324): Loss: 0.6859 || Learning rate: lr=1.25e-05.
===> Epoch[332](40/324): Loss: 1.0380 || Learning rate: lr=1.25e-05.
===> Epoch[332](50/324): Loss: 0.5683 || Learning rate: lr=1.25e-05.
===> Epoch[332](60/324): Loss: 0.7298 || Learning rate: lr=1.25e-05.
===> Epoch[332](70/324): Loss: 0.6640 || Learning rate: lr=1.25e-05.
===> Epoch[332](80/324): Loss: 0.4719 || Learning rate: lr=1.25e-05.
===> Epoch[332](90/324): Loss: 0.6965 || Learning rate: lr=1.25e-05.
===> Epoch[332](100/324): Loss: 0.5814 || Learning rate: lr=1.25e-05.
===> Epoch[332](110/324): Loss: 0.7401 || Learning rate: lr=1.25e-05.
===> Epoch[332](120/324): Loss: 0.5465 || Learning rate: lr=1.25e-05.
===> Epoch[332](130/324): Loss: 0.6737 || Learning rate: lr=1.25e-05.
===> Epoch[332](140/324): Loss: 0.7964 || Learning rate: lr=1.25e-05.
===> Epoch[332](150/324): Loss: 0.6310 || Learning rate: lr=1.25e-05.
===> Epoch[332](160/324): Loss: 0.5343 || Learning rate: lr=1.25e-05.
===> Epoch[332](170/324): Loss: 0.7000 || Learning rate: lr=1.25e-05.
===> Epoch[332](180/324): Loss: 0.4079 || Learning rate: lr=1.25e-05.
===> Epoch[332](190/324): Loss: 0.4604 || Learning rate: lr=1.25e-05.
===> Epoch[332](200/324): Loss: 0.8434 || Learning rate: lr=1.25e-05.
===> Epoch[332](210/324): Loss: 0.5033 || Learning rate: lr=1.25e-05.
===> Epoch[332](220/324): Loss: 0.8107 || Learning rate: lr=1.25e-05.
===> Epoch[332](230/324): Loss: 0.6250 || Learning rate: lr=1.25e-05.
===> Epoch[332](240/324): Loss: 0.5350 || Learning rate: lr=1.25e-05.
===> Epoch[332](250/324): Loss: 0.8219 || Learning rate: lr=1.25e-05.
===> Epoch[332](260/324): Loss: 0.7605 || Learning rate: lr=1.25e-05.
===> Epoch[332](270/324): Loss: 0.7294 || Learning rate: lr=1.25e-05.
===> Epoch[332](280/324): Loss: 0.6492 || Learning rate: lr=1.25e-05.
===> Epoch[332](290/324): Loss: 0.7024 || Learning rate: lr=1.25e-05.
===> Epoch[332](300/324): Loss: 0.6068 || Learning rate: lr=1.25e-05.
===> Epoch[332](310/324): Loss: 0.6155 || Learning rate: lr=1.25e-05.
===> Epoch[332](320/324): Loss: 0.8370 || Learning rate: lr=1.25e-05.
===> Epoch[333](10/324): Loss: 0.6481 || Learning rate: lr=1.25e-05.
===> Epoch[333](20/324): Loss: 0.8684 || Learning rate: lr=1.25e-05.
===> Epoch[333](30/324): Loss: 0.6827 || Learning rate: lr=1.25e-05.
===> Epoch[333](40/324): Loss: 0.7878 || Learning rate: lr=1.25e-05.
===> Epoch[333](50/324): Loss: 0.9015 || Learning rate: lr=1.25e-05.
===> Epoch[333](60/324): Loss: 0.5556 || Learning rate: lr=1.25e-05.
===> Epoch[333](70/324): Loss: 0.6011 || Learning rate: lr=1.25e-05.
===> Epoch[333](80/324): Loss: 0.7964 || Learning rate: lr=1.25e-05.
===> Epoch[333](90/324): Loss: 0.7166 || Learning rate: lr=1.25e-05.
===> Epoch[333](100/324): Loss: 0.7991 || Learning rate: lr=1.25e-05.
===> Epoch[333](110/324): Loss: 0.6134 || Learning rate: lr=1.25e-05.
===> Epoch[333](120/324): Loss: 0.7322 || Learning rate: lr=1.25e-05.
===> Epoch[333](130/324): Loss: 0.4398 || Learning rate: lr=1.25e-05.
===> Epoch[333](140/324): Loss: 0.9164 || Learning rate: lr=1.25e-05.
===> Epoch[333](150/324): Loss: 0.7592 || Learning rate: lr=1.25e-05.
===> Epoch[333](160/324): Loss: 0.5636 || Learning rate: lr=1.25e-05.
===> Epoch[333](170/324): Loss: 0.6679 || Learning rate: lr=1.25e-05.
===> Epoch[333](180/324): Loss: 0.8295 || Learning rate: lr=1.25e-05.
===> Epoch[333](190/324): Loss: 0.5908 || Learning rate: lr=1.25e-05.
===> Epoch[333](200/324): Loss: 0.5531 || Learning rate: lr=1.25e-05.
===> Epoch[333](210/324): Loss: 0.7086 || Learning rate: lr=1.25e-05.
===> Epoch[333](220/324): Loss: 0.5807 || Learning rate: lr=1.25e-05.
===> Epoch[333](230/324): Loss: 0.7446 || Learning rate: lr=1.25e-05.
===> Epoch[333](240/324): Loss: 0.4631 || Learning rate: lr=1.25e-05.
===> Epoch[333](250/324): Loss: 0.6070 || Learning rate: lr=1.25e-05.
===> Epoch[333](260/324): Loss: 0.5189 || Learning rate: lr=1.25e-05.
===> Epoch[333](270/324): Loss: 0.6237 || Learning rate: lr=1.25e-05.
===> Epoch[333](280/324): Loss: 0.6030 || Learning rate: lr=1.25e-05.
===> Epoch[333](290/324): Loss: 0.6484 || Learning rate: lr=1.25e-05.
===> Epoch[333](300/324): Loss: 0.5728 || Learning rate: lr=1.25e-05.
===> Epoch[333](310/324): Loss: 0.6320 || Learning rate: lr=1.25e-05.
===> Epoch[333](320/324): Loss: 0.5774 || Learning rate: lr=1.25e-05.
===> Epoch[334](10/324): Loss: 0.4868 || Learning rate: lr=1.25e-05.
===> Epoch[334](20/324): Loss: 0.5653 || Learning rate: lr=1.25e-05.
===> Epoch[334](30/324): Loss: 0.6182 || Learning rate: lr=1.25e-05.
===> Epoch[334](40/324): Loss: 0.6090 || Learning rate: lr=1.25e-05.
===> Epoch[334](50/324): Loss: 0.5050 || Learning rate: lr=1.25e-05.
===> Epoch[334](60/324): Loss: 0.5832 || Learning rate: lr=1.25e-05.
===> Epoch[334](70/324): Loss: 0.6194 || Learning rate: lr=1.25e-05.
===> Epoch[334](80/324): Loss: 0.3599 || Learning rate: lr=1.25e-05.
===> Epoch[334](90/324): Loss: 0.4847 || Learning rate: lr=1.25e-05.
===> Epoch[334](100/324): Loss: 0.6002 || Learning rate: lr=1.25e-05.
===> Epoch[334](110/324): Loss: 0.5651 || Learning rate: lr=1.25e-05.
===> Epoch[334](120/324): Loss: 0.7327 || Learning rate: lr=1.25e-05.
===> Epoch[334](130/324): Loss: 0.5751 || Learning rate: lr=1.25e-05.
===> Epoch[334](140/324): Loss: 0.5707 || Learning rate: lr=1.25e-05.
===> Epoch[334](150/324): Loss: 0.5823 || Learning rate: lr=1.25e-05.
===> Epoch[334](160/324): Loss: 0.6810 || Learning rate: lr=1.25e-05.
===> Epoch[334](170/324): Loss: 0.8529 || Learning rate: lr=1.25e-05.
===> Epoch[334](180/324): Loss: 0.5284 || Learning rate: lr=1.25e-05.
===> Epoch[334](190/324): Loss: 0.9413 || Learning rate: lr=1.25e-05.
===> Epoch[334](200/324): Loss: 0.5548 || Learning rate: lr=1.25e-05.
===> Epoch[334](210/324): Loss: 0.5414 || Learning rate: lr=1.25e-05.
===> Epoch[334](220/324): Loss: 0.6750 || Learning rate: lr=1.25e-05.
===> Epoch[334](230/324): Loss: 0.6677 || Learning rate: lr=1.25e-05.
===> Epoch[334](240/324): Loss: 0.6482 || Learning rate: lr=1.25e-05.
===> Epoch[334](250/324): Loss: 0.6556 || Learning rate: lr=1.25e-05.
===> Epoch[334](260/324): Loss: 0.7442 || Learning rate: lr=1.25e-05.
===> Epoch[334](270/324): Loss: 0.4827 || Learning rate: lr=1.25e-05.
===> Epoch[334](280/324): Loss: 0.6661 || Learning rate: lr=1.25e-05.
===> Epoch[334](290/324): Loss: 0.7256 || Learning rate: lr=1.25e-05.
===> Epoch[334](300/324): Loss: 0.9051 || Learning rate: lr=1.25e-05.
===> Epoch[334](310/324): Loss: 0.6399 || Learning rate: lr=1.25e-05.
===> Epoch[334](320/324): Loss: 0.8593 || Learning rate: lr=1.25e-05.
===> Epoch[335](10/324): Loss: 0.6471 || Learning rate: lr=1.25e-05.
===> Epoch[335](20/324): Loss: 0.5725 || Learning rate: lr=1.25e-05.
===> Epoch[335](30/324): Loss: 0.6820 || Learning rate: lr=1.25e-05.
===> Epoch[335](40/324): Loss: 0.6369 || Learning rate: lr=1.25e-05.
===> Epoch[335](50/324): Loss: 0.4878 || Learning rate: lr=1.25e-05.
===> Epoch[335](60/324): Loss: 0.8514 || Learning rate: lr=1.25e-05.
===> Epoch[335](70/324): Loss: 0.4607 || Learning rate: lr=1.25e-05.
===> Epoch[335](80/324): Loss: 0.6343 || Learning rate: lr=1.25e-05.
===> Epoch[335](90/324): Loss: 0.4663 || Learning rate: lr=1.25e-05.
===> Epoch[335](100/324): Loss: 0.8831 || Learning rate: lr=1.25e-05.
===> Epoch[335](110/324): Loss: 0.5570 || Learning rate: lr=1.25e-05.
===> Epoch[335](120/324): Loss: 0.6771 || Learning rate: lr=1.25e-05.
===> Epoch[335](130/324): Loss: 0.7193 || Learning rate: lr=1.25e-05.
===> Epoch[335](140/324): Loss: 0.4491 || Learning rate: lr=1.25e-05.
===> Epoch[335](150/324): Loss: 0.8872 || Learning rate: lr=1.25e-05.
===> Epoch[335](160/324): Loss: 0.6124 || Learning rate: lr=1.25e-05.
===> Epoch[335](170/324): Loss: 1.0156 || Learning rate: lr=1.25e-05.
===> Epoch[335](180/324): Loss: 0.5675 || Learning rate: lr=1.25e-05.
===> Epoch[335](190/324): Loss: 0.5837 || Learning rate: lr=1.25e-05.
===> Epoch[335](200/324): Loss: 0.5446 || Learning rate: lr=1.25e-05.
===> Epoch[335](210/324): Loss: 0.8392 || Learning rate: lr=1.25e-05.
===> Epoch[335](220/324): Loss: 0.8249 || Learning rate: lr=1.25e-05.
===> Epoch[335](230/324): Loss: 0.6575 || Learning rate: lr=1.25e-05.
===> Epoch[335](240/324): Loss: 0.5758 || Learning rate: lr=1.25e-05.
===> Epoch[335](250/324): Loss: 0.5971 || Learning rate: lr=1.25e-05.
===> Epoch[335](260/324): Loss: 0.7429 || Learning rate: lr=1.25e-05.
===> Epoch[335](270/324): Loss: 0.4771 || Learning rate: lr=1.25e-05.
===> Epoch[335](280/324): Loss: 0.8992 || Learning rate: lr=1.25e-05.
===> Epoch[335](290/324): Loss: 0.8038 || Learning rate: lr=1.25e-05.
===> Epoch[335](300/324): Loss: 0.6172 || Learning rate: lr=1.25e-05.
===> Epoch[335](310/324): Loss: 0.5545 || Learning rate: lr=1.25e-05.
===> Epoch[335](320/324): Loss: 0.5618 || Learning rate: lr=1.25e-05.
===> Epoch[336](10/324): Loss: 0.6984 || Learning rate: lr=1.25e-05.
===> Epoch[336](20/324): Loss: 0.7430 || Learning rate: lr=1.25e-05.
===> Epoch[336](30/324): Loss: 0.4095 || Learning rate: lr=1.25e-05.
===> Epoch[336](40/324): Loss: 0.6168 || Learning rate: lr=1.25e-05.
===> Epoch[336](50/324): Loss: 0.7772 || Learning rate: lr=1.25e-05.
===> Epoch[336](60/324): Loss: 0.7180 || Learning rate: lr=1.25e-05.
===> Epoch[336](70/324): Loss: 0.5863 || Learning rate: lr=1.25e-05.
===> Epoch[336](80/324): Loss: 0.7772 || Learning rate: lr=1.25e-05.
===> Epoch[336](90/324): Loss: 0.6960 || Learning rate: lr=1.25e-05.
===> Epoch[336](100/324): Loss: 0.5354 || Learning rate: lr=1.25e-05.
===> Epoch[336](110/324): Loss: 0.6426 || Learning rate: lr=1.25e-05.
===> Epoch[336](120/324): Loss: 0.4152 || Learning rate: lr=1.25e-05.
===> Epoch[336](130/324): Loss: 0.7438 || Learning rate: lr=1.25e-05.
===> Epoch[336](140/324): Loss: 0.6229 || Learning rate: lr=1.25e-05.
===> Epoch[336](150/324): Loss: 0.6703 || Learning rate: lr=1.25e-05.
===> Epoch[336](160/324): Loss: 0.5088 || Learning rate: lr=1.25e-05.
===> Epoch[336](170/324): Loss: 0.6219 || Learning rate: lr=1.25e-05.
===> Epoch[336](180/324): Loss: 0.7994 || Learning rate: lr=1.25e-05.
===> Epoch[336](190/324): Loss: 0.7000 || Learning rate: lr=1.25e-05.
===> Epoch[336](200/324): Loss: 0.8558 || Learning rate: lr=1.25e-05.
===> Epoch[336](210/324): Loss: 0.8605 || Learning rate: lr=1.25e-05.
===> Epoch[336](220/324): Loss: 0.7104 || Learning rate: lr=1.25e-05.
===> Epoch[336](230/324): Loss: 0.5865 || Learning rate: lr=1.25e-05.
===> Epoch[336](240/324): Loss: 0.5608 || Learning rate: lr=1.25e-05.
===> Epoch[336](250/324): Loss: 0.5619 || Learning rate: lr=1.25e-05.
===> Epoch[336](260/324): Loss: 0.5604 || Learning rate: lr=1.25e-05.
===> Epoch[336](270/324): Loss: 0.5970 || Learning rate: lr=1.25e-05.
===> Epoch[336](280/324): Loss: 0.5226 || Learning rate: lr=1.25e-05.
===> Epoch[336](290/324): Loss: 0.6569 || Learning rate: lr=1.25e-05.
===> Epoch[336](300/324): Loss: 0.7308 || Learning rate: lr=1.25e-05.
===> Epoch[336](310/324): Loss: 0.6682 || Learning rate: lr=1.25e-05.
===> Epoch[336](320/324): Loss: 0.8523 || Learning rate: lr=1.25e-05.
===> Epoch[337](10/324): Loss: 0.5214 || Learning rate: lr=1.25e-05.
===> Epoch[337](20/324): Loss: 0.4214 || Learning rate: lr=1.25e-05.
===> Epoch[337](30/324): Loss: 0.7164 || Learning rate: lr=1.25e-05.
===> Epoch[337](40/324): Loss: 0.7025 || Learning rate: lr=1.25e-05.
===> Epoch[337](50/324): Loss: 0.5116 || Learning rate: lr=1.25e-05.
===> Epoch[337](60/324): Loss: 0.5868 || Learning rate: lr=1.25e-05.
===> Epoch[337](70/324): Loss: 0.5882 || Learning rate: lr=1.25e-05.
===> Epoch[337](80/324): Loss: 0.7385 || Learning rate: lr=1.25e-05.
===> Epoch[337](90/324): Loss: 0.6786 || Learning rate: lr=1.25e-05.
===> Epoch[337](100/324): Loss: 0.7913 || Learning rate: lr=1.25e-05.
===> Epoch[337](110/324): Loss: 0.6343 || Learning rate: lr=1.25e-05.
===> Epoch[337](120/324): Loss: 0.8340 || Learning rate: lr=1.25e-05.
===> Epoch[337](130/324): Loss: 0.6484 || Learning rate: lr=1.25e-05.
===> Epoch[337](140/324): Loss: 0.5781 || Learning rate: lr=1.25e-05.
===> Epoch[337](150/324): Loss: 0.6662 || Learning rate: lr=1.25e-05.
===> Epoch[337](160/324): Loss: 0.6087 || Learning rate: lr=1.25e-05.
===> Epoch[337](170/324): Loss: 0.7785 || Learning rate: lr=1.25e-05.
===> Epoch[337](180/324): Loss: 0.6048 || Learning rate: lr=1.25e-05.
===> Epoch[337](190/324): Loss: 0.7754 || Learning rate: lr=1.25e-05.
===> Epoch[337](200/324): Loss: 0.4653 || Learning rate: lr=1.25e-05.
===> Epoch[337](210/324): Loss: 0.7345 || Learning rate: lr=1.25e-05.
===> Epoch[337](220/324): Loss: 0.4933 || Learning rate: lr=1.25e-05.
===> Epoch[337](230/324): Loss: 0.7225 || Learning rate: lr=1.25e-05.
===> Epoch[337](240/324): Loss: 0.6035 || Learning rate: lr=1.25e-05.
===> Epoch[337](250/324): Loss: 0.7630 || Learning rate: lr=1.25e-05.
===> Epoch[337](260/324): Loss: 0.5288 || Learning rate: lr=1.25e-05.
===> Epoch[337](270/324): Loss: 0.4780 || Learning rate: lr=1.25e-05.
===> Epoch[337](280/324): Loss: 0.7347 || Learning rate: lr=1.25e-05.
===> Epoch[337](290/324): Loss: 0.7603 || Learning rate: lr=1.25e-05.
===> Epoch[337](300/324): Loss: 0.5528 || Learning rate: lr=1.25e-05.
===> Epoch[337](310/324): Loss: 0.6778 || Learning rate: lr=1.25e-05.
===> Epoch[337](320/324): Loss: 0.5850 || Learning rate: lr=1.25e-05.
===> Epoch[338](10/324): Loss: 0.6955 || Learning rate: lr=1.25e-05.
===> Epoch[338](20/324): Loss: 0.5836 || Learning rate: lr=1.25e-05.
===> Epoch[338](30/324): Loss: 0.6907 || Learning rate: lr=1.25e-05.
===> Epoch[338](40/324): Loss: 0.5970 || Learning rate: lr=1.25e-05.
===> Epoch[338](50/324): Loss: 0.9980 || Learning rate: lr=1.25e-05.
===> Epoch[338](60/324): Loss: 0.6396 || Learning rate: lr=1.25e-05.
===> Epoch[338](70/324): Loss: 0.5201 || Learning rate: lr=1.25e-05.
===> Epoch[338](80/324): Loss: 0.4923 || Learning rate: lr=1.25e-05.
===> Epoch[338](90/324): Loss: 0.9738 || Learning rate: lr=1.25e-05.
===> Epoch[338](100/324): Loss: 0.9036 || Learning rate: lr=1.25e-05.
===> Epoch[338](110/324): Loss: 0.5749 || Learning rate: lr=1.25e-05.
===> Epoch[338](120/324): Loss: 0.4807 || Learning rate: lr=1.25e-05.
===> Epoch[338](130/324): Loss: 0.6302 || Learning rate: lr=1.25e-05.
===> Epoch[338](140/324): Loss: 0.6841 || Learning rate: lr=1.25e-05.
===> Epoch[338](150/324): Loss: 0.7289 || Learning rate: lr=1.25e-05.
===> Epoch[338](160/324): Loss: 0.5680 || Learning rate: lr=1.25e-05.
===> Epoch[338](170/324): Loss: 0.5295 || Learning rate: lr=1.25e-05.
===> Epoch[338](180/324): Loss: 0.6042 || Learning rate: lr=1.25e-05.
===> Epoch[338](190/324): Loss: 0.5139 || Learning rate: lr=1.25e-05.
===> Epoch[338](200/324): Loss: 0.6401 || Learning rate: lr=1.25e-05.
===> Epoch[338](210/324): Loss: 0.6153 || Learning rate: lr=1.25e-05.
===> Epoch[338](220/324): Loss: 0.5195 || Learning rate: lr=1.25e-05.
===> Epoch[338](230/324): Loss: 0.4715 || Learning rate: lr=1.25e-05.
===> Epoch[338](240/324): Loss: 0.5121 || Learning rate: lr=1.25e-05.
===> Epoch[338](250/324): Loss: 0.6369 || Learning rate: lr=1.25e-05.
===> Epoch[338](260/324): Loss: 0.6989 || Learning rate: lr=1.25e-05.
===> Epoch[338](270/324): Loss: 0.5885 || Learning rate: lr=1.25e-05.
===> Epoch[338](280/324): Loss: 0.9956 || Learning rate: lr=1.25e-05.
===> Epoch[338](290/324): Loss: 0.6978 || Learning rate: lr=1.25e-05.
===> Epoch[338](300/324): Loss: 0.6724 || Learning rate: lr=1.25e-05.
===> Epoch[338](310/324): Loss: 0.6649 || Learning rate: lr=1.25e-05.
===> Epoch[338](320/324): Loss: 0.6872 || Learning rate: lr=1.25e-05.
===> Epoch[339](10/324): Loss: 0.7385 || Learning rate: lr=1.25e-05.
===> Epoch[339](20/324): Loss: 0.7341 || Learning rate: lr=1.25e-05.
===> Epoch[339](30/324): Loss: 0.6968 || Learning rate: lr=1.25e-05.
===> Epoch[339](40/324): Loss: 0.9930 || Learning rate: lr=1.25e-05.
===> Epoch[339](50/324): Loss: 0.4423 || Learning rate: lr=1.25e-05.
===> Epoch[339](60/324): Loss: 1.1425 || Learning rate: lr=1.25e-05.
===> Epoch[339](70/324): Loss: 0.4723 || Learning rate: lr=1.25e-05.
===> Epoch[339](80/324): Loss: 0.4840 || Learning rate: lr=1.25e-05.
===> Epoch[339](90/324): Loss: 0.5926 || Learning rate: lr=1.25e-05.
===> Epoch[339](100/324): Loss: 0.8265 || Learning rate: lr=1.25e-05.
===> Epoch[339](110/324): Loss: 0.6013 || Learning rate: lr=1.25e-05.
===> Epoch[339](120/324): Loss: 0.7133 || Learning rate: lr=1.25e-05.
===> Epoch[339](130/324): Loss: 0.6830 || Learning rate: lr=1.25e-05.
===> Epoch[339](140/324): Loss: 0.6627 || Learning rate: lr=1.25e-05.
===> Epoch[339](150/324): Loss: 0.6564 || Learning rate: lr=1.25e-05.
===> Epoch[339](160/324): Loss: 0.6584 || Learning rate: lr=1.25e-05.
===> Epoch[339](170/324): Loss: 0.7141 || Learning rate: lr=1.25e-05.
===> Epoch[339](180/324): Loss: 0.7413 || Learning rate: lr=1.25e-05.
===> Epoch[339](190/324): Loss: 0.5815 || Learning rate: lr=1.25e-05.
===> Epoch[339](200/324): Loss: 0.3876 || Learning rate: lr=1.25e-05.
===> Epoch[339](210/324): Loss: 0.5157 || Learning rate: lr=1.25e-05.
===> Epoch[339](220/324): Loss: 0.5762 || Learning rate: lr=1.25e-05.
===> Epoch[339](230/324): Loss: 0.6144 || Learning rate: lr=1.25e-05.
===> Epoch[339](240/324): Loss: 0.6124 || Learning rate: lr=1.25e-05.
===> Epoch[339](250/324): Loss: 0.5499 || Learning rate: lr=1.25e-05.
===> Epoch[339](260/324): Loss: 0.6896 || Learning rate: lr=1.25e-05.
===> Epoch[339](270/324): Loss: 0.6312 || Learning rate: lr=1.25e-05.
===> Epoch[339](280/324): Loss: 0.8915 || Learning rate: lr=1.25e-05.
===> Epoch[339](290/324): Loss: 0.6339 || Learning rate: lr=1.25e-05.
===> Epoch[339](300/324): Loss: 0.5019 || Learning rate: lr=1.25e-05.
===> Epoch[339](310/324): Loss: 0.5418 || Learning rate: lr=1.25e-05.
===> Epoch[339](320/324): Loss: 0.5203 || Learning rate: lr=1.25e-05.
===> Epoch[340](10/324): Loss: 0.6125 || Learning rate: lr=1.25e-05.
===> Epoch[340](20/324): Loss: 0.6246 || Learning rate: lr=1.25e-05.
===> Epoch[340](30/324): Loss: 0.7554 || Learning rate: lr=1.25e-05.
===> Epoch[340](40/324): Loss: 0.6607 || Learning rate: lr=1.25e-05.
===> Epoch[340](50/324): Loss: 0.6635 || Learning rate: lr=1.25e-05.
===> Epoch[340](60/324): Loss: 0.8341 || Learning rate: lr=1.25e-05.
===> Epoch[340](70/324): Loss: 0.6582 || Learning rate: lr=1.25e-05.
===> Epoch[340](80/324): Loss: 0.6557 || Learning rate: lr=1.25e-05.
===> Epoch[340](90/324): Loss: 0.5498 || Learning rate: lr=1.25e-05.
===> Epoch[340](100/324): Loss: 0.6142 || Learning rate: lr=1.25e-05.
===> Epoch[340](110/324): Loss: 0.9830 || Learning rate: lr=1.25e-05.
===> Epoch[340](120/324): Loss: 0.6157 || Learning rate: lr=1.25e-05.
===> Epoch[340](130/324): Loss: 0.4643 || Learning rate: lr=1.25e-05.
===> Epoch[340](140/324): Loss: 0.5971 || Learning rate: lr=1.25e-05.
===> Epoch[340](150/324): Loss: 0.5557 || Learning rate: lr=1.25e-05.
===> Epoch[340](160/324): Loss: 0.4915 || Learning rate: lr=1.25e-05.
===> Epoch[340](170/324): Loss: 0.6296 || Learning rate: lr=1.25e-05.
===> Epoch[340](180/324): Loss: 0.8448 || Learning rate: lr=1.25e-05.
===> Epoch[340](190/324): Loss: 0.4924 || Learning rate: lr=1.25e-05.
===> Epoch[340](200/324): Loss: 0.6696 || Learning rate: lr=1.25e-05.
===> Epoch[340](210/324): Loss: 0.7474 || Learning rate: lr=1.25e-05.
===> Epoch[340](220/324): Loss: 0.5311 || Learning rate: lr=1.25e-05.
===> Epoch[340](230/324): Loss: 0.7575 || Learning rate: lr=1.25e-05.
===> Epoch[340](240/324): Loss: 0.6367 || Learning rate: lr=1.25e-05.
===> Epoch[340](250/324): Loss: 0.6030 || Learning rate: lr=1.25e-05.
===> Epoch[340](260/324): Loss: 0.7765 || Learning rate: lr=1.25e-05.
===> Epoch[340](270/324): Loss: 0.4546 || Learning rate: lr=1.25e-05.
===> Epoch[340](280/324): Loss: 0.7432 || Learning rate: lr=1.25e-05.
===> Epoch[340](290/324): Loss: 0.7039 || Learning rate: lr=1.25e-05.
===> Epoch[340](300/324): Loss: 0.8653 || Learning rate: lr=1.25e-05.
===> Epoch[340](310/324): Loss: 0.4392 || Learning rate: lr=1.25e-05.
===> Epoch[340](320/324): Loss: 0.7982 || Learning rate: lr=1.25e-05.
Checkpoint saved to weights/epoch_v2_340.pth
===> Epoch[341](10/324): Loss: 0.6394 || Learning rate: lr=1.25e-05.
===> Epoch[341](20/324): Loss: 0.4763 || Learning rate: lr=1.25e-05.
===> Epoch[341](30/324): Loss: 0.6134 || Learning rate: lr=1.25e-05.
===> Epoch[341](40/324): Loss: 0.7723 || Learning rate: lr=1.25e-05.
===> Epoch[341](50/324): Loss: 0.5614 || Learning rate: lr=1.25e-05.
===> Epoch[341](60/324): Loss: 0.6829 || Learning rate: lr=1.25e-05.
===> Epoch[341](70/324): Loss: 0.8414 || Learning rate: lr=1.25e-05.
===> Epoch[341](80/324): Loss: 0.5380 || Learning rate: lr=1.25e-05.
===> Epoch[341](90/324): Loss: 0.4106 || Learning rate: lr=1.25e-05.
===> Epoch[341](100/324): Loss: 0.6574 || Learning rate: lr=1.25e-05.
===> Epoch[341](110/324): Loss: 0.7061 || Learning rate: lr=1.25e-05.
===> Epoch[341](120/324): Loss: 1.1130 || Learning rate: lr=1.25e-05.
===> Epoch[341](130/324): Loss: 1.0691 || Learning rate: lr=1.25e-05.
===> Epoch[341](140/324): Loss: 0.6204 || Learning rate: lr=1.25e-05.
===> Epoch[341](150/324): Loss: 0.6808 || Learning rate: lr=1.25e-05.
===> Epoch[341](160/324): Loss: 0.4929 || Learning rate: lr=1.25e-05.
===> Epoch[341](170/324): Loss: 0.3521 || Learning rate: lr=1.25e-05.
===> Epoch[341](180/324): Loss: 0.7803 || Learning rate: lr=1.25e-05.
===> Epoch[341](190/324): Loss: 0.5271 || Learning rate: lr=1.25e-05.
===> Epoch[341](200/324): Loss: 0.7506 || Learning rate: lr=1.25e-05.
===> Epoch[341](210/324): Loss: 0.6504 || Learning rate: lr=1.25e-05.
===> Epoch[341](220/324): Loss: 0.9997 || Learning rate: lr=1.25e-05.
===> Epoch[341](230/324): Loss: 0.7075 || Learning rate: lr=1.25e-05.
===> Epoch[341](240/324): Loss: 0.7957 || Learning rate: lr=1.25e-05.
===> Epoch[341](250/324): Loss: 0.6110 || Learning rate: lr=1.25e-05.
===> Epoch[341](260/324): Loss: 0.5827 || Learning rate: lr=1.25e-05.
===> Epoch[341](270/324): Loss: 0.5260 || Learning rate: lr=1.25e-05.
===> Epoch[341](280/324): Loss: 0.7098 || Learning rate: lr=1.25e-05.
===> Epoch[341](290/324): Loss: 0.5665 || Learning rate: lr=1.25e-05.
===> Epoch[341](300/324): Loss: 0.5044 || Learning rate: lr=1.25e-05.
===> Epoch[341](310/324): Loss: 0.7605 || Learning rate: lr=1.25e-05.
===> Epoch[341](320/324): Loss: 0.3901 || Learning rate: lr=1.25e-05.
===> Epoch[342](10/324): Loss: 0.6343 || Learning rate: lr=1.25e-05.
===> Epoch[342](20/324): Loss: 0.5279 || Learning rate: lr=1.25e-05.
===> Epoch[342](30/324): Loss: 0.8456 || Learning rate: lr=1.25e-05.
===> Epoch[342](40/324): Loss: 0.6999 || Learning rate: lr=1.25e-05.
===> Epoch[342](50/324): Loss: 0.6325 || Learning rate: lr=1.25e-05.
===> Epoch[342](60/324): Loss: 0.9626 || Learning rate: lr=1.25e-05.
===> Epoch[342](70/324): Loss: 0.7218 || Learning rate: lr=1.25e-05.
===> Epoch[342](80/324): Loss: 0.6885 || Learning rate: lr=1.25e-05.
===> Epoch[342](90/324): Loss: 0.5214 || Learning rate: lr=1.25e-05.
===> Epoch[342](100/324): Loss: 0.7213 || Learning rate: lr=1.25e-05.
===> Epoch[342](110/324): Loss: 0.5593 || Learning rate: lr=1.25e-05.
===> Epoch[342](120/324): Loss: 0.4506 || Learning rate: lr=1.25e-05.
===> Epoch[342](130/324): Loss: 0.6125 || Learning rate: lr=1.25e-05.
===> Epoch[342](140/324): Loss: 0.6790 || Learning rate: lr=1.25e-05.
===> Epoch[342](150/324): Loss: 0.6359 || Learning rate: lr=1.25e-05.
===> Epoch[342](160/324): Loss: 0.7022 || Learning rate: lr=1.25e-05.
===> Epoch[342](170/324): Loss: 0.8209 || Learning rate: lr=1.25e-05.
===> Epoch[342](180/324): Loss: 0.5959 || Learning rate: lr=1.25e-05.
===> Epoch[342](190/324): Loss: 0.5629 || Learning rate: lr=1.25e-05.
===> Epoch[342](200/324): Loss: 0.7561 || Learning rate: lr=1.25e-05.
===> Epoch[342](210/324): Loss: 0.5438 || Learning rate: lr=1.25e-05.
===> Epoch[342](220/324): Loss: 0.4571 || Learning rate: lr=1.25e-05.
===> Epoch[342](230/324): Loss: 0.5010 || Learning rate: lr=1.25e-05.
===> Epoch[342](240/324): Loss: 0.5959 || Learning rate: lr=1.25e-05.
===> Epoch[342](250/324): Loss: 0.8536 || Learning rate: lr=1.25e-05.
===> Epoch[342](260/324): Loss: 0.5073 || Learning rate: lr=1.25e-05.
===> Epoch[342](270/324): Loss: 0.7298 || Learning rate: lr=1.25e-05.
===> Epoch[342](280/324): Loss: 0.8041 || Learning rate: lr=1.25e-05.
===> Epoch[342](290/324): Loss: 0.5171 || Learning rate: lr=1.25e-05.
===> Epoch[342](300/324): Loss: 0.6206 || Learning rate: lr=1.25e-05.
===> Epoch[342](310/324): Loss: 0.5382 || Learning rate: lr=1.25e-05.
===> Epoch[342](320/324): Loss: 0.9613 || Learning rate: lr=1.25e-05.
===> Epoch[343](10/324): Loss: 0.5931 || Learning rate: lr=1.25e-05.
===> Epoch[343](20/324): Loss: 0.8868 || Learning rate: lr=1.25e-05.
===> Epoch[343](30/324): Loss: 0.6165 || Learning rate: lr=1.25e-05.
===> Epoch[343](40/324): Loss: 0.4521 || Learning rate: lr=1.25e-05.
===> Epoch[343](50/324): Loss: 0.5698 || Learning rate: lr=1.25e-05.
===> Epoch[343](60/324): Loss: 0.5884 || Learning rate: lr=1.25e-05.
===> Epoch[343](70/324): Loss: 0.9412 || Learning rate: lr=1.25e-05.
===> Epoch[343](80/324): Loss: 0.6614 || Learning rate: lr=1.25e-05.
===> Epoch[343](90/324): Loss: 0.6673 || Learning rate: lr=1.25e-05.
===> Epoch[343](100/324): Loss: 0.7358 || Learning rate: lr=1.25e-05.
===> Epoch[343](110/324): Loss: 0.7681 || Learning rate: lr=1.25e-05.
===> Epoch[343](120/324): Loss: 0.6779 || Learning rate: lr=1.25e-05.
===> Epoch[343](130/324): Loss: 0.6173 || Learning rate: lr=1.25e-05.
===> Epoch[343](140/324): Loss: 0.5887 || Learning rate: lr=1.25e-05.
===> Epoch[343](150/324): Loss: 0.4043 || Learning rate: lr=1.25e-05.
===> Epoch[343](160/324): Loss: 0.6455 || Learning rate: lr=1.25e-05.
===> Epoch[343](170/324): Loss: 0.5454 || Learning rate: lr=1.25e-05.
===> Epoch[343](180/324): Loss: 0.7543 || Learning rate: lr=1.25e-05.
===> Epoch[343](190/324): Loss: 0.6275 || Learning rate: lr=1.25e-05.
===> Epoch[343](200/324): Loss: 0.5761 || Learning rate: lr=1.25e-05.
===> Epoch[343](210/324): Loss: 0.5071 || Learning rate: lr=1.25e-05.
===> Epoch[343](220/324): Loss: 0.6368 || Learning rate: lr=1.25e-05.
===> Epoch[343](230/324): Loss: 0.5431 || Learning rate: lr=1.25e-05.
===> Epoch[343](240/324): Loss: 0.7747 || Learning rate: lr=1.25e-05.
===> Epoch[343](250/324): Loss: 0.8709 || Learning rate: lr=1.25e-05.
===> Epoch[343](260/324): Loss: 0.5114 || Learning rate: lr=1.25e-05.
===> Epoch[343](270/324): Loss: 0.7363 || Learning rate: lr=1.25e-05.
===> Epoch[343](280/324): Loss: 0.5493 || Learning rate: lr=1.25e-05.
===> Epoch[343](290/324): Loss: 0.5254 || Learning rate: lr=1.25e-05.
===> Epoch[343](300/324): Loss: 0.9149 || Learning rate: lr=1.25e-05.
===> Epoch[343](310/324): Loss: 0.6449 || Learning rate: lr=1.25e-05.
===> Epoch[343](320/324): Loss: 0.8320 || Learning rate: lr=1.25e-05.
===> Epoch[344](10/324): Loss: 0.4982 || Learning rate: lr=1.25e-05.
===> Epoch[344](20/324): Loss: 0.5898 || Learning rate: lr=1.25e-05.
===> Epoch[344](30/324): Loss: 0.5487 || Learning rate: lr=1.25e-05.
===> Epoch[344](40/324): Loss: 0.7001 || Learning rate: lr=1.25e-05.
===> Epoch[344](50/324): Loss: 0.6766 || Learning rate: lr=1.25e-05.
===> Epoch[344](60/324): Loss: 0.6333 || Learning rate: lr=1.25e-05.
===> Epoch[344](70/324): Loss: 0.6063 || Learning rate: lr=1.25e-05.
===> Epoch[344](80/324): Loss: 0.4649 || Learning rate: lr=1.25e-05.
===> Epoch[344](90/324): Loss: 0.6498 || Learning rate: lr=1.25e-05.
===> Epoch[344](100/324): Loss: 0.6360 || Learning rate: lr=1.25e-05.
===> Epoch[344](110/324): Loss: 0.6343 || Learning rate: lr=1.25e-05.
===> Epoch[344](120/324): Loss: 0.6716 || Learning rate: lr=1.25e-05.
===> Epoch[344](130/324): Loss: 0.6818 || Learning rate: lr=1.25e-05.
===> Epoch[344](140/324): Loss: 0.4902 || Learning rate: lr=1.25e-05.
===> Epoch[344](150/324): Loss: 0.4769 || Learning rate: lr=1.25e-05.
===> Epoch[344](160/324): Loss: 0.9744 || Learning rate: lr=1.25e-05.
===> Epoch[344](170/324): Loss: 0.6904 || Learning rate: lr=1.25e-05.
===> Epoch[344](180/324): Loss: 0.4604 || Learning rate: lr=1.25e-05.
===> Epoch[344](190/324): Loss: 0.7680 || Learning rate: lr=1.25e-05.
===> Epoch[344](200/324): Loss: 0.5122 || Learning rate: lr=1.25e-05.
===> Epoch[344](210/324): Loss: 0.5016 || Learning rate: lr=1.25e-05.
===> Epoch[344](220/324): Loss: 0.4398 || Learning rate: lr=1.25e-05.
===> Epoch[344](230/324): Loss: 0.4421 || Learning rate: lr=1.25e-05.
===> Epoch[344](240/324): Loss: 0.5703 || Learning rate: lr=1.25e-05.
===> Epoch[344](250/324): Loss: 0.7760 || Learning rate: lr=1.25e-05.
===> Epoch[344](260/324): Loss: 0.7490 || Learning rate: lr=1.25e-05.
===> Epoch[344](270/324): Loss: 0.8570 || Learning rate: lr=1.25e-05.
===> Epoch[344](280/324): Loss: 0.8037 || Learning rate: lr=1.25e-05.
===> Epoch[344](290/324): Loss: 0.8465 || Learning rate: lr=1.25e-05.
===> Epoch[344](300/324): Loss: 0.6172 || Learning rate: lr=1.25e-05.
===> Epoch[344](310/324): Loss: 0.6737 || Learning rate: lr=1.25e-05.
===> Epoch[344](320/324): Loss: 0.6383 || Learning rate: lr=1.25e-05.
===> Epoch[345](10/324): Loss: 0.4195 || Learning rate: lr=1.25e-05.
===> Epoch[345](20/324): Loss: 0.7295 || Learning rate: lr=1.25e-05.
===> Epoch[345](30/324): Loss: 0.6307 || Learning rate: lr=1.25e-05.
===> Epoch[345](40/324): Loss: 0.7799 || Learning rate: lr=1.25e-05.
===> Epoch[345](50/324): Loss: 0.6052 || Learning rate: lr=1.25e-05.
===> Epoch[345](60/324): Loss: 0.8042 || Learning rate: lr=1.25e-05.
===> Epoch[345](70/324): Loss: 0.5864 || Learning rate: lr=1.25e-05.
===> Epoch[345](80/324): Loss: 0.6604 || Learning rate: lr=1.25e-05.
===> Epoch[345](90/324): Loss: 0.5437 || Learning rate: lr=1.25e-05.
===> Epoch[345](100/324): Loss: 0.7752 || Learning rate: lr=1.25e-05.
===> Epoch[345](110/324): Loss: 0.6981 || Learning rate: lr=1.25e-05.
===> Epoch[345](120/324): Loss: 0.5732 || Learning rate: lr=1.25e-05.
===> Epoch[345](130/324): Loss: 0.4970 || Learning rate: lr=1.25e-05.
===> Epoch[345](140/324): Loss: 0.5246 || Learning rate: lr=1.25e-05.
===> Epoch[345](150/324): Loss: 0.6558 || Learning rate: lr=1.25e-05.
===> Epoch[345](160/324): Loss: 0.6435 || Learning rate: lr=1.25e-05.
===> Epoch[345](170/324): Loss: 0.6095 || Learning rate: lr=1.25e-05.
===> Epoch[345](180/324): Loss: 0.4449 || Learning rate: lr=1.25e-05.
===> Epoch[345](190/324): Loss: 0.3794 || Learning rate: lr=1.25e-05.
===> Epoch[345](200/324): Loss: 0.4910 || Learning rate: lr=1.25e-05.
===> Epoch[345](210/324): Loss: 0.5244 || Learning rate: lr=1.25e-05.
===> Epoch[345](220/324): Loss: 0.5893 || Learning rate: lr=1.25e-05.
===> Epoch[345](230/324): Loss: 0.7478 || Learning rate: lr=1.25e-05.
===> Epoch[345](240/324): Loss: 0.6700 || Learning rate: lr=1.25e-05.
===> Epoch[345](250/324): Loss: 0.6175 || Learning rate: lr=1.25e-05.
===> Epoch[345](260/324): Loss: 0.5165 || Learning rate: lr=1.25e-05.
===> Epoch[345](270/324): Loss: 0.5586 || Learning rate: lr=1.25e-05.
===> Epoch[345](280/324): Loss: 0.4723 || Learning rate: lr=1.25e-05.
===> Epoch[345](290/324): Loss: 0.9048 || Learning rate: lr=1.25e-05.
===> Epoch[345](300/324): Loss: 0.6712 || Learning rate: lr=1.25e-05.
===> Epoch[345](310/324): Loss: 0.4178 || Learning rate: lr=1.25e-05.
===> Epoch[345](320/324): Loss: 0.6533 || Learning rate: lr=1.25e-05.
===> Epoch[346](10/324): Loss: 0.6764 || Learning rate: lr=1.25e-05.
===> Epoch[346](20/324): Loss: 0.7216 || Learning rate: lr=1.25e-05.
===> Epoch[346](30/324): Loss: 0.7593 || Learning rate: lr=1.25e-05.
===> Epoch[346](40/324): Loss: 0.4617 || Learning rate: lr=1.25e-05.
===> Epoch[346](50/324): Loss: 0.5288 || Learning rate: lr=1.25e-05.
===> Epoch[346](60/324): Loss: 0.5917 || Learning rate: lr=1.25e-05.
===> Epoch[346](70/324): Loss: 0.4490 || Learning rate: lr=1.25e-05.
===> Epoch[346](80/324): Loss: 0.6024 || Learning rate: lr=1.25e-05.
===> Epoch[346](90/324): Loss: 0.8109 || Learning rate: lr=1.25e-05.
===> Epoch[346](100/324): Loss: 0.7512 || Learning rate: lr=1.25e-05.
===> Epoch[346](110/324): Loss: 0.6822 || Learning rate: lr=1.25e-05.
===> Epoch[346](120/324): Loss: 0.7786 || Learning rate: lr=1.25e-05.
===> Epoch[346](130/324): Loss: 0.6221 || Learning rate: lr=1.25e-05.
===> Epoch[346](140/324): Loss: 0.8631 || Learning rate: lr=1.25e-05.
===> Epoch[346](150/324): Loss: 0.4671 || Learning rate: lr=1.25e-05.
===> Epoch[346](160/324): Loss: 0.7876 || Learning rate: lr=1.25e-05.
===> Epoch[346](170/324): Loss: 0.5142 || Learning rate: lr=1.25e-05.
===> Epoch[346](180/324): Loss: 0.6165 || Learning rate: lr=1.25e-05.
===> Epoch[346](190/324): Loss: 0.7486 || Learning rate: lr=1.25e-05.
===> Epoch[346](200/324): Loss: 0.6409 || Learning rate: lr=1.25e-05.
===> Epoch[346](210/324): Loss: 0.4412 || Learning rate: lr=1.25e-05.
===> Epoch[346](220/324): Loss: 0.6136 || Learning rate: lr=1.25e-05.
===> Epoch[346](230/324): Loss: 0.7220 || Learning rate: lr=1.25e-05.
===> Epoch[346](240/324): Loss: 0.7090 || Learning rate: lr=1.25e-05.
===> Epoch[346](250/324): Loss: 0.5288 || Learning rate: lr=1.25e-05.
===> Epoch[346](260/324): Loss: 0.5484 || Learning rate: lr=1.25e-05.
===> Epoch[346](270/324): Loss: 0.6083 || Learning rate: lr=1.25e-05.
===> Epoch[346](280/324): Loss: 0.5523 || Learning rate: lr=1.25e-05.
===> Epoch[346](290/324): Loss: 0.5210 || Learning rate: lr=1.25e-05.
===> Epoch[346](300/324): Loss: 0.8236 || Learning rate: lr=1.25e-05.
===> Epoch[346](310/324): Loss: 0.7010 || Learning rate: lr=1.25e-05.
===> Epoch[346](320/324): Loss: 0.6089 || Learning rate: lr=1.25e-05.
===> Epoch[347](10/324): Loss: 0.5439 || Learning rate: lr=1.25e-05.
===> Epoch[347](20/324): Loss: 0.5492 || Learning rate: lr=1.25e-05.
===> Epoch[347](30/324): Loss: 0.6638 || Learning rate: lr=1.25e-05.
===> Epoch[347](40/324): Loss: 0.6865 || Learning rate: lr=1.25e-05.
===> Epoch[347](50/324): Loss: 0.7878 || Learning rate: lr=1.25e-05.
===> Epoch[347](60/324): Loss: 0.6370 || Learning rate: lr=1.25e-05.
===> Epoch[347](70/324): Loss: 0.6826 || Learning rate: lr=1.25e-05.
===> Epoch[347](80/324): Loss: 0.5804 || Learning rate: lr=1.25e-05.
===> Epoch[347](90/324): Loss: 0.8026 || Learning rate: lr=1.25e-05.
===> Epoch[347](100/324): Loss: 0.5614 || Learning rate: lr=1.25e-05.
===> Epoch[347](110/324): Loss: 0.7393 || Learning rate: lr=1.25e-05.
===> Epoch[347](120/324): Loss: 0.5545 || Learning rate: lr=1.25e-05.
===> Epoch[347](130/324): Loss: 0.6109 || Learning rate: lr=1.25e-05.
===> Epoch[347](140/324): Loss: 0.7628 || Learning rate: lr=1.25e-05.
===> Epoch[347](150/324): Loss: 1.0351 || Learning rate: lr=1.25e-05.
===> Epoch[347](160/324): Loss: 0.6865 || Learning rate: lr=1.25e-05.
===> Epoch[347](170/324): Loss: 0.5402 || Learning rate: lr=1.25e-05.
===> Epoch[347](180/324): Loss: 0.6057 || Learning rate: lr=1.25e-05.
===> Epoch[347](190/324): Loss: 0.5527 || Learning rate: lr=1.25e-05.
===> Epoch[347](200/324): Loss: 0.7837 || Learning rate: lr=1.25e-05.
===> Epoch[347](210/324): Loss: 0.4328 || Learning rate: lr=1.25e-05.
===> Epoch[347](220/324): Loss: 0.5158 || Learning rate: lr=1.25e-05.
===> Epoch[347](230/324): Loss: 0.4192 || Learning rate: lr=1.25e-05.
===> Epoch[347](240/324): Loss: 0.6682 || Learning rate: lr=1.25e-05.
===> Epoch[347](250/324): Loss: 0.6112 || Learning rate: lr=1.25e-05.
===> Epoch[347](260/324): Loss: 0.5291 || Learning rate: lr=1.25e-05.
===> Epoch[347](270/324): Loss: 1.0576 || Learning rate: lr=1.25e-05.
===> Epoch[347](280/324): Loss: 0.7291 || Learning rate: lr=1.25e-05.
===> Epoch[347](290/324): Loss: 0.6194 || Learning rate: lr=1.25e-05.
===> Epoch[347](300/324): Loss: 0.7366 || Learning rate: lr=1.25e-05.
===> Epoch[347](310/324): Loss: 0.7646 || Learning rate: lr=1.25e-05.
===> Epoch[347](320/324): Loss: 0.6034 || Learning rate: lr=1.25e-05.
===> Epoch[348](10/324): Loss: 0.7678 || Learning rate: lr=1.25e-05.
===> Epoch[348](20/324): Loss: 0.5643 || Learning rate: lr=1.25e-05.
===> Epoch[348](30/324): Loss: 0.4384 || Learning rate: lr=1.25e-05.
===> Epoch[348](40/324): Loss: 0.5583 || Learning rate: lr=1.25e-05.
===> Epoch[348](50/324): Loss: 0.6758 || Learning rate: lr=1.25e-05.
===> Epoch[348](60/324): Loss: 0.7312 || Learning rate: lr=1.25e-05.
===> Epoch[348](70/324): Loss: 0.7990 || Learning rate: lr=1.25e-05.
===> Epoch[348](80/324): Loss: 0.8179 || Learning rate: lr=1.25e-05.
===> Epoch[348](90/324): Loss: 0.7387 || Learning rate: lr=1.25e-05.
===> Epoch[348](100/324): Loss: 0.8628 || Learning rate: lr=1.25e-05.
===> Epoch[348](110/324): Loss: 0.6330 || Learning rate: lr=1.25e-05.
===> Epoch[348](120/324): Loss: 0.7513 || Learning rate: lr=1.25e-05.
===> Epoch[348](130/324): Loss: 0.7088 || Learning rate: lr=1.25e-05.
===> Epoch[348](140/324): Loss: 0.8401 || Learning rate: lr=1.25e-05.
===> Epoch[348](150/324): Loss: 0.6071 || Learning rate: lr=1.25e-05.
===> Epoch[348](160/324): Loss: 0.5142 || Learning rate: lr=1.25e-05.
===> Epoch[348](170/324): Loss: 0.7369 || Learning rate: lr=1.25e-05.
===> Epoch[348](180/324): Loss: 0.6193 || Learning rate: lr=1.25e-05.
===> Epoch[348](190/324): Loss: 0.6432 || Learning rate: lr=1.25e-05.
===> Epoch[348](200/324): Loss: 0.8629 || Learning rate: lr=1.25e-05.
===> Epoch[348](210/324): Loss: 0.6236 || Learning rate: lr=1.25e-05.
===> Epoch[348](220/324): Loss: 0.5923 || Learning rate: lr=1.25e-05.
===> Epoch[348](230/324): Loss: 0.4263 || Learning rate: lr=1.25e-05.
===> Epoch[348](240/324): Loss: 0.7024 || Learning rate: lr=1.25e-05.
===> Epoch[348](250/324): Loss: 0.6550 || Learning rate: lr=1.25e-05.
===> Epoch[348](260/324): Loss: 0.6376 || Learning rate: lr=1.25e-05.
===> Epoch[348](270/324): Loss: 0.5286 || Learning rate: lr=1.25e-05.
===> Epoch[348](280/324): Loss: 0.8375 || Learning rate: lr=1.25e-05.
===> Epoch[348](290/324): Loss: 0.7090 || Learning rate: lr=1.25e-05.
===> Epoch[348](300/324): Loss: 0.9241 || Learning rate: lr=1.25e-05.
===> Epoch[348](310/324): Loss: 0.5779 || Learning rate: lr=1.25e-05.
===> Epoch[348](320/324): Loss: 0.5151 || Learning rate: lr=1.25e-05.
===> Epoch[349](10/324): Loss: 0.7409 || Learning rate: lr=1.25e-05.
===> Epoch[349](20/324): Loss: 0.4955 || Learning rate: lr=1.25e-05.
===> Epoch[349](30/324): Loss: 0.7707 || Learning rate: lr=1.25e-05.
===> Epoch[349](40/324): Loss: 0.5415 || Learning rate: lr=1.25e-05.
===> Epoch[349](50/324): Loss: 0.6777 || Learning rate: lr=1.25e-05.
===> Epoch[349](60/324): Loss: 0.4218 || Learning rate: lr=1.25e-05.
===> Epoch[349](70/324): Loss: 0.4900 || Learning rate: lr=1.25e-05.
===> Epoch[349](80/324): Loss: 0.5651 || Learning rate: lr=1.25e-05.
===> Epoch[349](90/324): Loss: 0.5681 || Learning rate: lr=1.25e-05.
===> Epoch[349](100/324): Loss: 0.7734 || Learning rate: lr=1.25e-05.
===> Epoch[349](110/324): Loss: 0.5550 || Learning rate: lr=1.25e-05.
===> Epoch[349](120/324): Loss: 0.6073 || Learning rate: lr=1.25e-05.
===> Epoch[349](130/324): Loss: 0.6277 || Learning rate: lr=1.25e-05.
===> Epoch[349](140/324): Loss: 0.5988 || Learning rate: lr=1.25e-05.
===> Epoch[349](150/324): Loss: 0.5873 || Learning rate: lr=1.25e-05.
===> Epoch[349](160/324): Loss: 0.8128 || Learning rate: lr=1.25e-05.
===> Epoch[349](170/324): Loss: 0.7535 || Learning rate: lr=1.25e-05.
===> Epoch[349](180/324): Loss: 0.7985 || Learning rate: lr=1.25e-05.
===> Epoch[349](190/324): Loss: 0.7278 || Learning rate: lr=1.25e-05.
===> Epoch[349](200/324): Loss: 0.6140 || Learning rate: lr=1.25e-05.
===> Epoch[349](210/324): Loss: 0.5535 || Learning rate: lr=1.25e-05.
===> Epoch[349](220/324): Loss: 0.5241 || Learning rate: lr=1.25e-05.
===> Epoch[349](230/324): Loss: 0.6087 || Learning rate: lr=1.25e-05.
===> Epoch[349](240/324): Loss: 0.6707 || Learning rate: lr=1.25e-05.
===> Epoch[349](250/324): Loss: 0.7137 || Learning rate: lr=1.25e-05.
===> Epoch[349](260/324): Loss: 0.7253 || Learning rate: lr=1.25e-05.
===> Epoch[349](270/324): Loss: 0.5662 || Learning rate: lr=1.25e-05.
===> Epoch[349](280/324): Loss: 0.5116 || Learning rate: lr=1.25e-05.
===> Epoch[349](290/324): Loss: 0.5768 || Learning rate: lr=1.25e-05.
===> Epoch[349](300/324): Loss: 0.5863 || Learning rate: lr=1.25e-05.
===> Epoch[349](310/324): Loss: 0.6119 || Learning rate: lr=1.25e-05.
===> Epoch[349](320/324): Loss: 0.6195 || Learning rate: lr=1.25e-05.
===> Epoch[350](10/324): Loss: 0.5544 || Learning rate: lr=1.25e-05.
===> Epoch[350](20/324): Loss: 0.6038 || Learning rate: lr=1.25e-05.
===> Epoch[350](30/324): Loss: 0.6304 || Learning rate: lr=1.25e-05.
===> Epoch[350](40/324): Loss: 0.5321 || Learning rate: lr=1.25e-05.
===> Epoch[350](50/324): Loss: 0.7619 || Learning rate: lr=1.25e-05.
===> Epoch[350](60/324): Loss: 1.0285 || Learning rate: lr=1.25e-05.
===> Epoch[350](70/324): Loss: 0.7721 || Learning rate: lr=1.25e-05.
===> Epoch[350](80/324): Loss: 0.8382 || Learning rate: lr=1.25e-05.
===> Epoch[350](90/324): Loss: 0.8011 || Learning rate: lr=1.25e-05.
===> Epoch[350](100/324): Loss: 0.6355 || Learning rate: lr=1.25e-05.
===> Epoch[350](110/324): Loss: 0.6837 || Learning rate: lr=1.25e-05.
===> Epoch[350](120/324): Loss: 0.4183 || Learning rate: lr=1.25e-05.
===> Epoch[350](130/324): Loss: 0.5471 || Learning rate: lr=1.25e-05.
===> Epoch[350](140/324): Loss: 0.5249 || Learning rate: lr=1.25e-05.
===> Epoch[350](150/324): Loss: 0.5705 || Learning rate: lr=1.25e-05.
===> Epoch[350](160/324): Loss: 0.4989 || Learning rate: lr=1.25e-05.
===> Epoch[350](170/324): Loss: 0.6133 || Learning rate: lr=1.25e-05.
===> Epoch[350](180/324): Loss: 0.4923 || Learning rate: lr=1.25e-05.
===> Epoch[350](190/324): Loss: 0.5291 || Learning rate: lr=1.25e-05.
===> Epoch[350](200/324): Loss: 0.5229 || Learning rate: lr=1.25e-05.
===> Epoch[350](210/324): Loss: 1.0141 || Learning rate: lr=1.25e-05.
===> Epoch[350](220/324): Loss: 0.7582 || Learning rate: lr=1.25e-05.
===> Epoch[350](230/324): Loss: 0.6365 || Learning rate: lr=1.25e-05.
===> Epoch[350](240/324): Loss: 0.7217 || Learning rate: lr=1.25e-05.
===> Epoch[350](250/324): Loss: 0.6666 || Learning rate: lr=1.25e-05.
===> Epoch[350](260/324): Loss: 0.6567 || Learning rate: lr=1.25e-05.
===> Epoch[350](270/324): Loss: 0.6734 || Learning rate: lr=1.25e-05.
===> Epoch[350](280/324): Loss: 0.5873 || Learning rate: lr=1.25e-05.
===> Epoch[350](290/324): Loss: 0.7072 || Learning rate: lr=1.25e-05.
===> Epoch[350](300/324): Loss: 0.5548 || Learning rate: lr=1.25e-05.
===> Epoch[350](310/324): Loss: 0.6616 || Learning rate: lr=1.25e-05.
===> Epoch[350](320/324): Loss: 0.5915 || Learning rate: lr=1.25e-05.
===> Epoch[351](10/324): Loss: 0.6944 || Learning rate: lr=1.25e-05.
===> Epoch[351](20/324): Loss: 0.4853 || Learning rate: lr=1.25e-05.
===> Epoch[351](30/324): Loss: 0.5851 || Learning rate: lr=1.25e-05.
===> Epoch[351](40/324): Loss: 0.8432 || Learning rate: lr=1.25e-05.
===> Epoch[351](50/324): Loss: 0.5158 || Learning rate: lr=1.25e-05.
===> Epoch[351](60/324): Loss: 0.6059 || Learning rate: lr=1.25e-05.
===> Epoch[351](70/324): Loss: 0.8016 || Learning rate: lr=1.25e-05.
===> Epoch[351](80/324): Loss: 0.9408 || Learning rate: lr=1.25e-05.
===> Epoch[351](90/324): Loss: 0.9897 || Learning rate: lr=1.25e-05.
===> Epoch[351](100/324): Loss: 0.7933 || Learning rate: lr=1.25e-05.
===> Epoch[351](110/324): Loss: 0.6403 || Learning rate: lr=1.25e-05.
===> Epoch[351](120/324): Loss: 0.5855 || Learning rate: lr=1.25e-05.
===> Epoch[351](130/324): Loss: 0.4734 || Learning rate: lr=1.25e-05.
===> Epoch[351](140/324): Loss: 0.6690 || Learning rate: lr=1.25e-05.
===> Epoch[351](150/324): Loss: 0.5519 || Learning rate: lr=1.25e-05.
===> Epoch[351](160/324): Loss: 0.5429 || Learning rate: lr=1.25e-05.
===> Epoch[351](170/324): Loss: 0.5810 || Learning rate: lr=1.25e-05.
===> Epoch[351](180/324): Loss: 0.6894 || Learning rate: lr=1.25e-05.
===> Epoch[351](190/324): Loss: 0.5466 || Learning rate: lr=1.25e-05.
===> Epoch[351](200/324): Loss: 0.5944 || Learning rate: lr=1.25e-05.
===> Epoch[351](210/324): Loss: 1.0789 || Learning rate: lr=1.25e-05.
===> Epoch[351](220/324): Loss: 0.8911 || Learning rate: lr=1.25e-05.
===> Epoch[351](230/324): Loss: 0.3904 || Learning rate: lr=1.25e-05.
===> Epoch[351](240/324): Loss: 0.5822 || Learning rate: lr=1.25e-05.
===> Epoch[351](250/324): Loss: 0.6875 || Learning rate: lr=1.25e-05.
===> Epoch[351](260/324): Loss: 0.7847 || Learning rate: lr=1.25e-05.
===> Epoch[351](270/324): Loss: 0.7800 || Learning rate: lr=1.25e-05.
===> Epoch[351](280/324): Loss: 0.6728 || Learning rate: lr=1.25e-05.
===> Epoch[351](290/324): Loss: 0.4350 || Learning rate: lr=1.25e-05.
===> Epoch[351](300/324): Loss: 0.6197 || Learning rate: lr=1.25e-05.
===> Epoch[351](310/324): Loss: 0.6753 || Learning rate: lr=1.25e-05.
===> Epoch[351](320/324): Loss: 0.7181 || Learning rate: lr=1.25e-05.
===> Epoch[352](10/324): Loss: 0.7552 || Learning rate: lr=1.25e-05.
===> Epoch[352](20/324): Loss: 0.4266 || Learning rate: lr=1.25e-05.
===> Epoch[352](30/324): Loss: 0.5710 || Learning rate: lr=1.25e-05.
===> Epoch[352](40/324): Loss: 0.6287 || Learning rate: lr=1.25e-05.
===> Epoch[352](50/324): Loss: 0.7181 || Learning rate: lr=1.25e-05.
===> Epoch[352](60/324): Loss: 0.8064 || Learning rate: lr=1.25e-05.
===> Epoch[352](70/324): Loss: 0.5157 || Learning rate: lr=1.25e-05.
===> Epoch[352](80/324): Loss: 0.5767 || Learning rate: lr=1.25e-05.
===> Epoch[352](90/324): Loss: 0.7627 || Learning rate: lr=1.25e-05.
===> Epoch[352](100/324): Loss: 0.5122 || Learning rate: lr=1.25e-05.
===> Epoch[352](110/324): Loss: 0.7192 || Learning rate: lr=1.25e-05.
===> Epoch[352](120/324): Loss: 0.5264 || Learning rate: lr=1.25e-05.
===> Epoch[352](130/324): Loss: 0.5069 || Learning rate: lr=1.25e-05.
===> Epoch[352](140/324): Loss: 0.7219 || Learning rate: lr=1.25e-05.
===> Epoch[352](150/324): Loss: 0.9384 || Learning rate: lr=1.25e-05.
===> Epoch[352](160/324): Loss: 0.8960 || Learning rate: lr=1.25e-05.
===> Epoch[352](170/324): Loss: 0.7866 || Learning rate: lr=1.25e-05.
===> Epoch[352](180/324): Loss: 0.5381 || Learning rate: lr=1.25e-05.
===> Epoch[352](190/324): Loss: 0.6646 || Learning rate: lr=1.25e-05.
===> Epoch[352](200/324): Loss: 0.5285 || Learning rate: lr=1.25e-05.
===> Epoch[352](210/324): Loss: 0.6602 || Learning rate: lr=1.25e-05.
===> Epoch[352](220/324): Loss: 0.6578 || Learning rate: lr=1.25e-05.
===> Epoch[352](230/324): Loss: 0.7960 || Learning rate: lr=1.25e-05.
===> Epoch[352](240/324): Loss: 0.6042 || Learning rate: lr=1.25e-05.
===> Epoch[352](250/324): Loss: 0.6185 || Learning rate: lr=1.25e-05.
===> Epoch[352](260/324): Loss: 0.3930 || Learning rate: lr=1.25e-05.
===> Epoch[352](270/324): Loss: 0.8841 || Learning rate: lr=1.25e-05.
===> Epoch[352](280/324): Loss: 0.8606 || Learning rate: lr=1.25e-05.
===> Epoch[352](290/324): Loss: 0.6778 || Learning rate: lr=1.25e-05.
===> Epoch[352](300/324): Loss: 0.7616 || Learning rate: lr=1.25e-05.
===> Epoch[352](310/324): Loss: 0.6592 || Learning rate: lr=1.25e-05.
===> Epoch[352](320/324): Loss: 0.6707 || Learning rate: lr=1.25e-05.
===> Epoch[353](10/324): Loss: 0.5929 || Learning rate: lr=1.25e-05.
===> Epoch[353](20/324): Loss: 0.6288 || Learning rate: lr=1.25e-05.
===> Epoch[353](30/324): Loss: 0.6918 || Learning rate: lr=1.25e-05.
===> Epoch[353](40/324): Loss: 0.4019 || Learning rate: lr=1.25e-05.
===> Epoch[353](50/324): Loss: 0.3843 || Learning rate: lr=1.25e-05.
===> Epoch[353](60/324): Loss: 0.6085 || Learning rate: lr=1.25e-05.
===> Epoch[353](70/324): Loss: 0.5696 || Learning rate: lr=1.25e-05.
===> Epoch[353](80/324): Loss: 0.7259 || Learning rate: lr=1.25e-05.
===> Epoch[353](90/324): Loss: 0.7819 || Learning rate: lr=1.25e-05.
===> Epoch[353](100/324): Loss: 0.7541 || Learning rate: lr=1.25e-05.
===> Epoch[353](110/324): Loss: 0.8216 || Learning rate: lr=1.25e-05.
===> Epoch[353](120/324): Loss: 0.5553 || Learning rate: lr=1.25e-05.
===> Epoch[353](130/324): Loss: 0.7936 || Learning rate: lr=1.25e-05.
===> Epoch[353](140/324): Loss: 0.6042 || Learning rate: lr=1.25e-05.
===> Epoch[353](150/324): Loss: 0.5741 || Learning rate: lr=1.25e-05.
===> Epoch[353](160/324): Loss: 0.9046 || Learning rate: lr=1.25e-05.
===> Epoch[353](170/324): Loss: 0.4986 || Learning rate: lr=1.25e-05.
===> Epoch[353](180/324): Loss: 0.4670 || Learning rate: lr=1.25e-05.
===> Epoch[353](190/324): Loss: 0.4933 || Learning rate: lr=1.25e-05.
===> Epoch[353](200/324): Loss: 0.6137 || Learning rate: lr=1.25e-05.
===> Epoch[353](210/324): Loss: 0.6711 || Learning rate: lr=1.25e-05.
===> Epoch[353](220/324): Loss: 0.7248 || Learning rate: lr=1.25e-05.
===> Epoch[353](230/324): Loss: 0.7056 || Learning rate: lr=1.25e-05.
===> Epoch[353](240/324): Loss: 0.5731 || Learning rate: lr=1.25e-05.
===> Epoch[353](250/324): Loss: 0.7310 || Learning rate: lr=1.25e-05.
===> Epoch[353](260/324): Loss: 1.0781 || Learning rate: lr=1.25e-05.
===> Epoch[353](270/324): Loss: 0.6146 || Learning rate: lr=1.25e-05.
===> Epoch[353](280/324): Loss: 0.5226 || Learning rate: lr=1.25e-05.
===> Epoch[353](290/324): Loss: 0.7868 || Learning rate: lr=1.25e-05.
===> Epoch[353](300/324): Loss: 0.5023 || Learning rate: lr=1.25e-05.
===> Epoch[353](310/324): Loss: 0.5758 || Learning rate: lr=1.25e-05.
===> Epoch[353](320/324): Loss: 0.6302 || Learning rate: lr=1.25e-05.
===> Epoch[354](10/324): Loss: 0.8218 || Learning rate: lr=1.25e-05.
===> Epoch[354](20/324): Loss: 0.4581 || Learning rate: lr=1.25e-05.
===> Epoch[354](30/324): Loss: 0.5750 || Learning rate: lr=1.25e-05.
===> Epoch[354](40/324): Loss: 0.6270 || Learning rate: lr=1.25e-05.
===> Epoch[354](50/324): Loss: 0.6364 || Learning rate: lr=1.25e-05.
===> Epoch[354](60/324): Loss: 0.7907 || Learning rate: lr=1.25e-05.
===> Epoch[354](70/324): Loss: 0.7463 || Learning rate: lr=1.25e-05.
===> Epoch[354](80/324): Loss: 0.4723 || Learning rate: lr=1.25e-05.
===> Epoch[354](90/324): Loss: 0.5402 || Learning rate: lr=1.25e-05.
===> Epoch[354](100/324): Loss: 0.5999 || Learning rate: lr=1.25e-05.
===> Epoch[354](110/324): Loss: 0.5945 || Learning rate: lr=1.25e-05.
===> Epoch[354](120/324): Loss: 0.6236 || Learning rate: lr=1.25e-05.
===> Epoch[354](130/324): Loss: 0.7023 || Learning rate: lr=1.25e-05.
===> Epoch[354](140/324): Loss: 0.6387 || Learning rate: lr=1.25e-05.
===> Epoch[354](150/324): Loss: 0.6534 || Learning rate: lr=1.25e-05.
===> Epoch[354](160/324): Loss: 0.6647 || Learning rate: lr=1.25e-05.
===> Epoch[354](170/324): Loss: 0.8630 || Learning rate: lr=1.25e-05.
===> Epoch[354](180/324): Loss: 0.6716 || Learning rate: lr=1.25e-05.
===> Epoch[354](190/324): Loss: 0.6102 || Learning rate: lr=1.25e-05.
===> Epoch[354](200/324): Loss: 0.6649 || Learning rate: lr=1.25e-05.
===> Epoch[354](210/324): Loss: 0.4668 || Learning rate: lr=1.25e-05.
===> Epoch[354](220/324): Loss: 0.5590 || Learning rate: lr=1.25e-05.
===> Epoch[354](230/324): Loss: 0.4311 || Learning rate: lr=1.25e-05.
===> Epoch[354](240/324): Loss: 0.8477 || Learning rate: lr=1.25e-05.
===> Epoch[354](250/324): Loss: 0.8072 || Learning rate: lr=1.25e-05.
===> Epoch[354](260/324): Loss: 0.7094 || Learning rate: lr=1.25e-05.
===> Epoch[354](270/324): Loss: 0.8079 || Learning rate: lr=1.25e-05.
===> Epoch[354](280/324): Loss: 0.3903 || Learning rate: lr=1.25e-05.
===> Epoch[354](290/324): Loss: 0.7668 || Learning rate: lr=1.25e-05.
===> Epoch[354](300/324): Loss: 0.6569 || Learning rate: lr=1.25e-05.
===> Epoch[354](310/324): Loss: 0.3947 || Learning rate: lr=1.25e-05.
===> Epoch[354](320/324): Loss: 0.5286 || Learning rate: lr=1.25e-05.
===> Epoch[355](10/324): Loss: 0.6866 || Learning rate: lr=1.25e-05.
===> Epoch[355](20/324): Loss: 0.5754 || Learning rate: lr=1.25e-05.
===> Epoch[355](30/324): Loss: 0.8214 || Learning rate: lr=1.25e-05.
===> Epoch[355](40/324): Loss: 0.6773 || Learning rate: lr=1.25e-05.
===> Epoch[355](50/324): Loss: 0.7066 || Learning rate: lr=1.25e-05.
===> Epoch[355](60/324): Loss: 0.5510 || Learning rate: lr=1.25e-05.
===> Epoch[355](70/324): Loss: 0.5665 || Learning rate: lr=1.25e-05.
===> Epoch[355](80/324): Loss: 0.8961 || Learning rate: lr=1.25e-05.
===> Epoch[355](90/324): Loss: 0.9257 || Learning rate: lr=1.25e-05.
===> Epoch[355](100/324): Loss: 0.5769 || Learning rate: lr=1.25e-05.
===> Epoch[355](110/324): Loss: 0.7049 || Learning rate: lr=1.25e-05.
===> Epoch[355](120/324): Loss: 0.5257 || Learning rate: lr=1.25e-05.
===> Epoch[355](130/324): Loss: 0.4122 || Learning rate: lr=1.25e-05.
===> Epoch[355](140/324): Loss: 0.7269 || Learning rate: lr=1.25e-05.
===> Epoch[355](150/324): Loss: 0.4313 || Learning rate: lr=1.25e-05.
===> Epoch[355](160/324): Loss: 0.8400 || Learning rate: lr=1.25e-05.
===> Epoch[355](170/324): Loss: 0.6812 || Learning rate: lr=1.25e-05.
===> Epoch[355](180/324): Loss: 0.6483 || Learning rate: lr=1.25e-05.
===> Epoch[355](190/324): Loss: 0.7937 || Learning rate: lr=1.25e-05.
===> Epoch[355](200/324): Loss: 0.6414 || Learning rate: lr=1.25e-05.
===> Epoch[355](210/324): Loss: 0.5325 || Learning rate: lr=1.25e-05.
===> Epoch[355](220/324): Loss: 0.7057 || Learning rate: lr=1.25e-05.
===> Epoch[355](230/324): Loss: 0.6280 || Learning rate: lr=1.25e-05.
===> Epoch[355](240/324): Loss: 0.6588 || Learning rate: lr=1.25e-05.
===> Epoch[355](250/324): Loss: 0.7830 || Learning rate: lr=1.25e-05.
===> Epoch[355](260/324): Loss: 0.8080 || Learning rate: lr=1.25e-05.
===> Epoch[355](270/324): Loss: 0.8128 || Learning rate: lr=1.25e-05.
===> Epoch[355](280/324): Loss: 0.5434 || Learning rate: lr=1.25e-05.
===> Epoch[355](290/324): Loss: 0.5481 || Learning rate: lr=1.25e-05.
===> Epoch[355](300/324): Loss: 0.8007 || Learning rate: lr=1.25e-05.
===> Epoch[355](310/324): Loss: 0.9245 || Learning rate: lr=1.25e-05.
===> Epoch[355](320/324): Loss: 0.8134 || Learning rate: lr=1.25e-05.
===> Epoch[356](10/324): Loss: 0.6557 || Learning rate: lr=1.25e-05.
===> Epoch[356](20/324): Loss: 0.6356 || Learning rate: lr=1.25e-05.
===> Epoch[356](30/324): Loss: 1.0100 || Learning rate: lr=1.25e-05.
===> Epoch[356](40/324): Loss: 0.4650 || Learning rate: lr=1.25e-05.
===> Epoch[356](50/324): Loss: 0.4942 || Learning rate: lr=1.25e-05.
===> Epoch[356](60/324): Loss: 0.6956 || Learning rate: lr=1.25e-05.
===> Epoch[356](70/324): Loss: 0.6640 || Learning rate: lr=1.25e-05.
===> Epoch[356](80/324): Loss: 0.6270 || Learning rate: lr=1.25e-05.
===> Epoch[356](90/324): Loss: 0.5820 || Learning rate: lr=1.25e-05.
===> Epoch[356](100/324): Loss: 0.5318 || Learning rate: lr=1.25e-05.
===> Epoch[356](110/324): Loss: 0.5066 || Learning rate: lr=1.25e-05.
===> Epoch[356](120/324): Loss: 0.7018 || Learning rate: lr=1.25e-05.
===> Epoch[356](130/324): Loss: 0.8531 || Learning rate: lr=1.25e-05.
===> Epoch[356](140/324): Loss: 0.7893 || Learning rate: lr=1.25e-05.
===> Epoch[356](150/324): Loss: 0.5125 || Learning rate: lr=1.25e-05.
===> Epoch[356](160/324): Loss: 0.6363 || Learning rate: lr=1.25e-05.
===> Epoch[356](170/324): Loss: 0.4907 || Learning rate: lr=1.25e-05.
===> Epoch[356](180/324): Loss: 0.4891 || Learning rate: lr=1.25e-05.
===> Epoch[356](190/324): Loss: 0.4101 || Learning rate: lr=1.25e-05.
===> Epoch[356](200/324): Loss: 0.5798 || Learning rate: lr=1.25e-05.
===> Epoch[356](210/324): Loss: 0.8267 || Learning rate: lr=1.25e-05.
===> Epoch[356](220/324): Loss: 0.6149 || Learning rate: lr=1.25e-05.
===> Epoch[356](230/324): Loss: 0.6915 || Learning rate: lr=1.25e-05.
===> Epoch[356](240/324): Loss: 0.6983 || Learning rate: lr=1.25e-05.
===> Epoch[356](250/324): Loss: 0.6381 || Learning rate: lr=1.25e-05.
===> Epoch[356](260/324): Loss: 0.6189 || Learning rate: lr=1.25e-05.
===> Epoch[356](270/324): Loss: 0.6645 || Learning rate: lr=1.25e-05.
===> Epoch[356](280/324): Loss: 0.5899 || Learning rate: lr=1.25e-05.
===> Epoch[356](290/324): Loss: 0.6040 || Learning rate: lr=1.25e-05.
===> Epoch[356](300/324): Loss: 0.6039 || Learning rate: lr=1.25e-05.
===> Epoch[356](310/324): Loss: 0.5937 || Learning rate: lr=1.25e-05.
===> Epoch[356](320/324): Loss: 0.6642 || Learning rate: lr=1.25e-05.
===> Epoch[357](10/324): Loss: 0.6358 || Learning rate: lr=1.25e-05.
===> Epoch[357](20/324): Loss: 0.5273 || Learning rate: lr=1.25e-05.
===> Epoch[357](30/324): Loss: 0.6924 || Learning rate: lr=1.25e-05.
===> Epoch[357](40/324): Loss: 0.5477 || Learning rate: lr=1.25e-05.
===> Epoch[357](50/324): Loss: 0.6933 || Learning rate: lr=1.25e-05.
===> Epoch[357](60/324): Loss: 0.8219 || Learning rate: lr=1.25e-05.
===> Epoch[357](70/324): Loss: 0.5731 || Learning rate: lr=1.25e-05.
===> Epoch[357](80/324): Loss: 0.5960 || Learning rate: lr=1.25e-05.
===> Epoch[357](90/324): Loss: 0.5850 || Learning rate: lr=1.25e-05.
===> Epoch[357](100/324): Loss: 0.8537 || Learning rate: lr=1.25e-05.
===> Epoch[357](110/324): Loss: 0.4059 || Learning rate: lr=1.25e-05.
===> Epoch[357](120/324): Loss: 0.5731 || Learning rate: lr=1.25e-05.
===> Epoch[357](130/324): Loss: 0.5562 || Learning rate: lr=1.25e-05.
===> Epoch[357](140/324): Loss: 0.7108 || Learning rate: lr=1.25e-05.
===> Epoch[357](150/324): Loss: 0.4559 || Learning rate: lr=1.25e-05.
===> Epoch[357](160/324): Loss: 0.9678 || Learning rate: lr=1.25e-05.
===> Epoch[357](170/324): Loss: 0.7804 || Learning rate: lr=1.25e-05.
===> Epoch[357](180/324): Loss: 0.6295 || Learning rate: lr=1.25e-05.
===> Epoch[357](190/324): Loss: 0.6668 || Learning rate: lr=1.25e-05.
===> Epoch[357](200/324): Loss: 0.6578 || Learning rate: lr=1.25e-05.
===> Epoch[357](210/324): Loss: 0.7861 || Learning rate: lr=1.25e-05.
===> Epoch[357](220/324): Loss: 0.5564 || Learning rate: lr=1.25e-05.
===> Epoch[357](230/324): Loss: 0.4940 || Learning rate: lr=1.25e-05.
===> Epoch[357](240/324): Loss: 0.9521 || Learning rate: lr=1.25e-05.
===> Epoch[357](250/324): Loss: 0.7342 || Learning rate: lr=1.25e-05.
===> Epoch[357](260/324): Loss: 0.9705 || Learning rate: lr=1.25e-05.
===> Epoch[357](270/324): Loss: 0.4959 || Learning rate: lr=1.25e-05.
===> Epoch[357](280/324): Loss: 0.4909 || Learning rate: lr=1.25e-05.
===> Epoch[357](290/324): Loss: 0.8145 || Learning rate: lr=1.25e-05.
===> Epoch[357](300/324): Loss: 0.6710 || Learning rate: lr=1.25e-05.
===> Epoch[357](310/324): Loss: 0.4960 || Learning rate: lr=1.25e-05.
===> Epoch[357](320/324): Loss: 0.7842 || Learning rate: lr=1.25e-05.
===> Epoch[358](10/324): Loss: 0.5154 || Learning rate: lr=1.25e-05.
===> Epoch[358](20/324): Loss: 0.5787 || Learning rate: lr=1.25e-05.
===> Epoch[358](30/324): Loss: 0.7309 || Learning rate: lr=1.25e-05.
===> Epoch[358](40/324): Loss: 0.8693 || Learning rate: lr=1.25e-05.
===> Epoch[358](50/324): Loss: 0.5668 || Learning rate: lr=1.25e-05.
===> Epoch[358](60/324): Loss: 0.5031 || Learning rate: lr=1.25e-05.
===> Epoch[358](70/324): Loss: 0.6614 || Learning rate: lr=1.25e-05.
===> Epoch[358](80/324): Loss: 0.7319 || Learning rate: lr=1.25e-05.
===> Epoch[358](90/324): Loss: 0.6962 || Learning rate: lr=1.25e-05.
===> Epoch[358](100/324): Loss: 0.9590 || Learning rate: lr=1.25e-05.
===> Epoch[358](110/324): Loss: 0.4419 || Learning rate: lr=1.25e-05.
===> Epoch[358](120/324): Loss: 0.4952 || Learning rate: lr=1.25e-05.
===> Epoch[358](130/324): Loss: 0.8650 || Learning rate: lr=1.25e-05.
===> Epoch[358](140/324): Loss: 0.6463 || Learning rate: lr=1.25e-05.
===> Epoch[358](150/324): Loss: 0.5266 || Learning rate: lr=1.25e-05.
===> Epoch[358](160/324): Loss: 0.6003 || Learning rate: lr=1.25e-05.
===> Epoch[358](170/324): Loss: 0.7132 || Learning rate: lr=1.25e-05.
===> Epoch[358](180/324): Loss: 0.7255 || Learning rate: lr=1.25e-05.
===> Epoch[358](190/324): Loss: 0.6519 || Learning rate: lr=1.25e-05.
===> Epoch[358](200/324): Loss: 0.6982 || Learning rate: lr=1.25e-05.
===> Epoch[358](210/324): Loss: 0.4389 || Learning rate: lr=1.25e-05.
===> Epoch[358](220/324): Loss: 0.6558 || Learning rate: lr=1.25e-05.
===> Epoch[358](230/324): Loss: 0.6287 || Learning rate: lr=1.25e-05.
===> Epoch[358](240/324): Loss: 0.5343 || Learning rate: lr=1.25e-05.
===> Epoch[358](250/324): Loss: 0.4896 || Learning rate: lr=1.25e-05.
===> Epoch[358](260/324): Loss: 0.7854 || Learning rate: lr=1.25e-05.
===> Epoch[358](270/324): Loss: 0.6360 || Learning rate: lr=1.25e-05.
===> Epoch[358](280/324): Loss: 0.5435 || Learning rate: lr=1.25e-05.
===> Epoch[358](290/324): Loss: 0.6373 || Learning rate: lr=1.25e-05.
===> Epoch[358](300/324): Loss: 0.7901 || Learning rate: lr=1.25e-05.
===> Epoch[358](310/324): Loss: 0.5371 || Learning rate: lr=1.25e-05.
===> Epoch[358](320/324): Loss: 0.5138 || Learning rate: lr=1.25e-05.
===> Epoch[359](10/324): Loss: 0.5473 || Learning rate: lr=1.25e-05.
===> Epoch[359](20/324): Loss: 0.9831 || Learning rate: lr=1.25e-05.
===> Epoch[359](30/324): Loss: 0.8075 || Learning rate: lr=1.25e-05.
===> Epoch[359](40/324): Loss: 0.7227 || Learning rate: lr=1.25e-05.
===> Epoch[359](50/324): Loss: 0.5550 || Learning rate: lr=1.25e-05.
===> Epoch[359](60/324): Loss: 0.6339 || Learning rate: lr=1.25e-05.
===> Epoch[359](70/324): Loss: 0.5141 || Learning rate: lr=1.25e-05.
===> Epoch[359](80/324): Loss: 0.7448 || Learning rate: lr=1.25e-05.
===> Epoch[359](90/324): Loss: 0.8979 || Learning rate: lr=1.25e-05.
===> Epoch[359](100/324): Loss: 0.5014 || Learning rate: lr=1.25e-05.
===> Epoch[359](110/324): Loss: 0.4472 || Learning rate: lr=1.25e-05.
===> Epoch[359](120/324): Loss: 0.5714 || Learning rate: lr=1.25e-05.
===> Epoch[359](130/324): Loss: 0.6607 || Learning rate: lr=1.25e-05.
===> Epoch[359](140/324): Loss: 0.4433 || Learning rate: lr=1.25e-05.
===> Epoch[359](150/324): Loss: 0.4619 || Learning rate: lr=1.25e-05.
===> Epoch[359](160/324): Loss: 0.5684 || Learning rate: lr=1.25e-05.
===> Epoch[359](170/324): Loss: 0.5938 || Learning rate: lr=1.25e-05.
===> Epoch[359](180/324): Loss: 0.9209 || Learning rate: lr=1.25e-05.
===> Epoch[359](190/324): Loss: 0.5790 || Learning rate: lr=1.25e-05.
===> Epoch[359](200/324): Loss: 0.8286 || Learning rate: lr=1.25e-05.
===> Epoch[359](210/324): Loss: 0.5140 || Learning rate: lr=1.25e-05.
===> Epoch[359](220/324): Loss: 0.5560 || Learning rate: lr=1.25e-05.
===> Epoch[359](230/324): Loss: 0.6803 || Learning rate: lr=1.25e-05.
===> Epoch[359](240/324): Loss: 0.8883 || Learning rate: lr=1.25e-05.
===> Epoch[359](250/324): Loss: 0.5406 || Learning rate: lr=1.25e-05.
===> Epoch[359](260/324): Loss: 0.6517 || Learning rate: lr=1.25e-05.
===> Epoch[359](270/324): Loss: 0.7251 || Learning rate: lr=1.25e-05.
===> Epoch[359](280/324): Loss: 0.6374 || Learning rate: lr=1.25e-05.
===> Epoch[359](290/324): Loss: 0.5111 || Learning rate: lr=1.25e-05.
===> Epoch[359](300/324): Loss: 0.5680 || Learning rate: lr=1.25e-05.
===> Epoch[359](310/324): Loss: 0.6398 || Learning rate: lr=1.25e-05.
===> Epoch[359](320/324): Loss: 0.6694 || Learning rate: lr=1.25e-05.
===> Epoch[360](10/324): Loss: 0.6338 || Learning rate: lr=1.25e-05.
===> Epoch[360](20/324): Loss: 0.7275 || Learning rate: lr=1.25e-05.
===> Epoch[360](30/324): Loss: 0.5588 || Learning rate: lr=1.25e-05.
===> Epoch[360](40/324): Loss: 0.6775 || Learning rate: lr=1.25e-05.
===> Epoch[360](50/324): Loss: 0.6726 || Learning rate: lr=1.25e-05.
===> Epoch[360](60/324): Loss: 0.4683 || Learning rate: lr=1.25e-05.
===> Epoch[360](70/324): Loss: 0.5542 || Learning rate: lr=1.25e-05.
===> Epoch[360](80/324): Loss: 0.7904 || Learning rate: lr=1.25e-05.
===> Epoch[360](90/324): Loss: 0.5564 || Learning rate: lr=1.25e-05.
===> Epoch[360](100/324): Loss: 0.5641 || Learning rate: lr=1.25e-05.
===> Epoch[360](110/324): Loss: 0.5903 || Learning rate: lr=1.25e-05.
===> Epoch[360](120/324): Loss: 0.4670 || Learning rate: lr=1.25e-05.
===> Epoch[360](130/324): Loss: 1.1063 || Learning rate: lr=1.25e-05.
===> Epoch[360](140/324): Loss: 0.6724 || Learning rate: lr=1.25e-05.
===> Epoch[360](150/324): Loss: 0.9677 || Learning rate: lr=1.25e-05.
===> Epoch[360](160/324): Loss: 0.5814 || Learning rate: lr=1.25e-05.
===> Epoch[360](170/324): Loss: 0.5499 || Learning rate: lr=1.25e-05.
===> Epoch[360](180/324): Loss: 0.5258 || Learning rate: lr=1.25e-05.
===> Epoch[360](190/324): Loss: 0.7439 || Learning rate: lr=1.25e-05.
===> Epoch[360](200/324): Loss: 0.7133 || Learning rate: lr=1.25e-05.
===> Epoch[360](210/324): Loss: 0.5411 || Learning rate: lr=1.25e-05.
===> Epoch[360](220/324): Loss: 0.5757 || Learning rate: lr=1.25e-05.
===> Epoch[360](230/324): Loss: 0.7898 || Learning rate: lr=1.25e-05.
===> Epoch[360](240/324): Loss: 0.4637 || Learning rate: lr=1.25e-05.
===> Epoch[360](250/324): Loss: 0.7707 || Learning rate: lr=1.25e-05.
===> Epoch[360](260/324): Loss: 0.5573 || Learning rate: lr=1.25e-05.
===> Epoch[360](270/324): Loss: 0.6063 || Learning rate: lr=1.25e-05.
===> Epoch[360](280/324): Loss: 0.7105 || Learning rate: lr=1.25e-05.
===> Epoch[360](290/324): Loss: 0.7473 || Learning rate: lr=1.25e-05.
===> Epoch[360](300/324): Loss: 0.5276 || Learning rate: lr=1.25e-05.
===> Epoch[360](310/324): Loss: 0.8383 || Learning rate: lr=1.25e-05.
===> Epoch[360](320/324): Loss: 0.7735 || Learning rate: lr=1.25e-05.
Checkpoint saved to weights/epoch_v2_360.pth
===> Epoch[361](10/324): Loss: 0.7781 || Learning rate: lr=1.25e-05.
===> Epoch[361](20/324): Loss: 0.6655 || Learning rate: lr=1.25e-05.
===> Epoch[361](30/324): Loss: 0.6095 || Learning rate: lr=1.25e-05.
===> Epoch[361](40/324): Loss: 0.5464 || Learning rate: lr=1.25e-05.
===> Epoch[361](50/324): Loss: 0.6666 || Learning rate: lr=1.25e-05.
===> Epoch[361](60/324): Loss: 0.7585 || Learning rate: lr=1.25e-05.
===> Epoch[361](70/324): Loss: 0.7190 || Learning rate: lr=1.25e-05.
===> Epoch[361](80/324): Loss: 0.7428 || Learning rate: lr=1.25e-05.
===> Epoch[361](90/324): Loss: 0.5333 || Learning rate: lr=1.25e-05.
===> Epoch[361](100/324): Loss: 0.3976 || Learning rate: lr=1.25e-05.
===> Epoch[361](110/324): Loss: 0.7050 || Learning rate: lr=1.25e-05.
===> Epoch[361](120/324): Loss: 0.6666 || Learning rate: lr=1.25e-05.
===> Epoch[361](130/324): Loss: 0.7222 || Learning rate: lr=1.25e-05.
===> Epoch[361](140/324): Loss: 0.7689 || Learning rate: lr=1.25e-05.
===> Epoch[361](150/324): Loss: 0.6053 || Learning rate: lr=1.25e-05.
===> Epoch[361](160/324): Loss: 0.4349 || Learning rate: lr=1.25e-05.
===> Epoch[361](170/324): Loss: 0.6545 || Learning rate: lr=1.25e-05.
===> Epoch[361](180/324): Loss: 0.4419 || Learning rate: lr=1.25e-05.
===> Epoch[361](190/324): Loss: 0.5575 || Learning rate: lr=1.25e-05.
===> Epoch[361](200/324): Loss: 0.5764 || Learning rate: lr=1.25e-05.
===> Epoch[361](210/324): Loss: 0.6844 || Learning rate: lr=1.25e-05.
===> Epoch[361](220/324): Loss: 0.8523 || Learning rate: lr=1.25e-05.
===> Epoch[361](230/324): Loss: 0.7198 || Learning rate: lr=1.25e-05.
===> Epoch[361](240/324): Loss: 0.6197 || Learning rate: lr=1.25e-05.
===> Epoch[361](250/324): Loss: 0.5907 || Learning rate: lr=1.25e-05.
===> Epoch[361](260/324): Loss: 0.6389 || Learning rate: lr=1.25e-05.
===> Epoch[361](270/324): Loss: 0.8042 || Learning rate: lr=1.25e-05.
===> Epoch[361](280/324): Loss: 0.5738 || Learning rate: lr=1.25e-05.
===> Epoch[361](290/324): Loss: 0.9320 || Learning rate: lr=1.25e-05.
===> Epoch[361](300/324): Loss: 0.4692 || Learning rate: lr=1.25e-05.
===> Epoch[361](310/324): Loss: 0.8374 || Learning rate: lr=1.25e-05.
===> Epoch[361](320/324): Loss: 0.7136 || Learning rate: lr=1.25e-05.
===> Epoch[362](10/324): Loss: 0.5774 || Learning rate: lr=1.25e-05.
===> Epoch[362](20/324): Loss: 0.5904 || Learning rate: lr=1.25e-05.
===> Epoch[362](30/324): Loss: 0.7431 || Learning rate: lr=1.25e-05.
===> Epoch[362](40/324): Loss: 0.3744 || Learning rate: lr=1.25e-05.
===> Epoch[362](50/324): Loss: 0.4141 || Learning rate: lr=1.25e-05.
===> Epoch[362](60/324): Loss: 0.4544 || Learning rate: lr=1.25e-05.
===> Epoch[362](70/324): Loss: 0.6134 || Learning rate: lr=1.25e-05.
===> Epoch[362](80/324): Loss: 0.9951 || Learning rate: lr=1.25e-05.
===> Epoch[362](90/324): Loss: 0.6585 || Learning rate: lr=1.25e-05.
===> Epoch[362](100/324): Loss: 0.8839 || Learning rate: lr=1.25e-05.
===> Epoch[362](110/324): Loss: 0.6886 || Learning rate: lr=1.25e-05.
===> Epoch[362](120/324): Loss: 0.6720 || Learning rate: lr=1.25e-05.
===> Epoch[362](130/324): Loss: 0.5820 || Learning rate: lr=1.25e-05.
===> Epoch[362](140/324): Loss: 0.6325 || Learning rate: lr=1.25e-05.
===> Epoch[362](150/324): Loss: 0.6483 || Learning rate: lr=1.25e-05.
===> Epoch[362](160/324): Loss: 0.5768 || Learning rate: lr=1.25e-05.
===> Epoch[362](170/324): Loss: 0.7823 || Learning rate: lr=1.25e-05.
===> Epoch[362](180/324): Loss: 0.7184 || Learning rate: lr=1.25e-05.
===> Epoch[362](190/324): Loss: 0.7421 || Learning rate: lr=1.25e-05.
===> Epoch[362](200/324): Loss: 0.6329 || Learning rate: lr=1.25e-05.
===> Epoch[362](210/324): Loss: 0.5178 || Learning rate: lr=1.25e-05.
===> Epoch[362](220/324): Loss: 0.6143 || Learning rate: lr=1.25e-05.
===> Epoch[362](230/324): Loss: 0.4571 || Learning rate: lr=1.25e-05.
===> Epoch[362](240/324): Loss: 0.5405 || Learning rate: lr=1.25e-05.
===> Epoch[362](250/324): Loss: 0.6095 || Learning rate: lr=1.25e-05.
===> Epoch[362](260/324): Loss: 0.7623 || Learning rate: lr=1.25e-05.
===> Epoch[362](270/324): Loss: 0.5146 || Learning rate: lr=1.25e-05.
===> Epoch[362](280/324): Loss: 0.7121 || Learning rate: lr=1.25e-05.
===> Epoch[362](290/324): Loss: 0.6256 || Learning rate: lr=1.25e-05.
===> Epoch[362](300/324): Loss: 0.6256 || Learning rate: lr=1.25e-05.
===> Epoch[362](310/324): Loss: 0.6451 || Learning rate: lr=1.25e-05.
===> Epoch[362](320/324): Loss: 0.8460 || Learning rate: lr=1.25e-05.
===> Epoch[363](10/324): Loss: 0.6930 || Learning rate: lr=1.25e-05.
===> Epoch[363](20/324): Loss: 0.6787 || Learning rate: lr=1.25e-05.
===> Epoch[363](30/324): Loss: 0.7586 || Learning rate: lr=1.25e-05.
===> Epoch[363](40/324): Loss: 0.7575 || Learning rate: lr=1.25e-05.
===> Epoch[363](50/324): Loss: 0.7170 || Learning rate: lr=1.25e-05.
===> Epoch[363](60/324): Loss: 0.5761 || Learning rate: lr=1.25e-05.
===> Epoch[363](70/324): Loss: 0.7048 || Learning rate: lr=1.25e-05.
===> Epoch[363](80/324): Loss: 0.6619 || Learning rate: lr=1.25e-05.
===> Epoch[363](90/324): Loss: 0.8108 || Learning rate: lr=1.25e-05.
===> Epoch[363](100/324): Loss: 0.5658 || Learning rate: lr=1.25e-05.
===> Epoch[363](110/324): Loss: 0.4100 || Learning rate: lr=1.25e-05.
===> Epoch[363](120/324): Loss: 0.5017 || Learning rate: lr=1.25e-05.
===> Epoch[363](130/324): Loss: 0.7713 || Learning rate: lr=1.25e-05.
===> Epoch[363](140/324): Loss: 0.6650 || Learning rate: lr=1.25e-05.
===> Epoch[363](150/324): Loss: 0.6422 || Learning rate: lr=1.25e-05.
===> Epoch[363](160/324): Loss: 0.7543 || Learning rate: lr=1.25e-05.
===> Epoch[363](170/324): Loss: 1.0385 || Learning rate: lr=1.25e-05.
===> Epoch[363](180/324): Loss: 0.8237 || Learning rate: lr=1.25e-05.
===> Epoch[363](190/324): Loss: 0.5771 || Learning rate: lr=1.25e-05.
===> Epoch[363](200/324): Loss: 0.4079 || Learning rate: lr=1.25e-05.
===> Epoch[363](210/324): Loss: 0.3499 || Learning rate: lr=1.25e-05.
===> Epoch[363](220/324): Loss: 0.6953 || Learning rate: lr=1.25e-05.
===> Epoch[363](230/324): Loss: 0.7469 || Learning rate: lr=1.25e-05.
===> Epoch[363](240/324): Loss: 0.7792 || Learning rate: lr=1.25e-05.
===> Epoch[363](250/324): Loss: 0.6356 || Learning rate: lr=1.25e-05.
===> Epoch[363](260/324): Loss: 0.5567 || Learning rate: lr=1.25e-05.
===> Epoch[363](270/324): Loss: 0.7427 || Learning rate: lr=1.25e-05.
===> Epoch[363](280/324): Loss: 0.5700 || Learning rate: lr=1.25e-05.
===> Epoch[363](290/324): Loss: 0.3969 || Learning rate: lr=1.25e-05.
===> Epoch[363](300/324): Loss: 0.4933 || Learning rate: lr=1.25e-05.
===> Epoch[363](310/324): Loss: 0.7021 || Learning rate: lr=1.25e-05.
===> Epoch[363](320/324): Loss: 0.8542 || Learning rate: lr=1.25e-05.
===> Epoch[364](10/324): Loss: 0.4823 || Learning rate: lr=1.25e-05.
===> Epoch[364](20/324): Loss: 0.5943 || Learning rate: lr=1.25e-05.
===> Epoch[364](30/324): Loss: 0.5994 || Learning rate: lr=1.25e-05.
===> Epoch[364](40/324): Loss: 0.6216 || Learning rate: lr=1.25e-05.
===> Epoch[364](50/324): Loss: 0.5290 || Learning rate: lr=1.25e-05.
===> Epoch[364](60/324): Loss: 0.5122 || Learning rate: lr=1.25e-05.
===> Epoch[364](70/324): Loss: 0.7694 || Learning rate: lr=1.25e-05.
===> Epoch[364](80/324): Loss: 0.7253 || Learning rate: lr=1.25e-05.
===> Epoch[364](90/324): Loss: 0.7806 || Learning rate: lr=1.25e-05.
===> Epoch[364](100/324): Loss: 0.5623 || Learning rate: lr=1.25e-05.
===> Epoch[364](110/324): Loss: 0.8911 || Learning rate: lr=1.25e-05.
===> Epoch[364](120/324): Loss: 0.5614 || Learning rate: lr=1.25e-05.
===> Epoch[364](130/324): Loss: 1.0843 || Learning rate: lr=1.25e-05.
===> Epoch[364](140/324): Loss: 0.5449 || Learning rate: lr=1.25e-05.
===> Epoch[364](150/324): Loss: 0.8192 || Learning rate: lr=1.25e-05.
===> Epoch[364](160/324): Loss: 0.7932 || Learning rate: lr=1.25e-05.
===> Epoch[364](170/324): Loss: 0.9627 || Learning rate: lr=1.25e-05.
===> Epoch[364](180/324): Loss: 0.7332 || Learning rate: lr=1.25e-05.
===> Epoch[364](190/324): Loss: 0.6047 || Learning rate: lr=1.25e-05.
===> Epoch[364](200/324): Loss: 0.4852 || Learning rate: lr=1.25e-05.
===> Epoch[364](210/324): Loss: 0.7767 || Learning rate: lr=1.25e-05.
===> Epoch[364](220/324): Loss: 0.5515 || Learning rate: lr=1.25e-05.
===> Epoch[364](230/324): Loss: 0.5333 || Learning rate: lr=1.25e-05.
===> Epoch[364](240/324): Loss: 0.5821 || Learning rate: lr=1.25e-05.
===> Epoch[364](250/324): Loss: 0.6487 || Learning rate: lr=1.25e-05.
===> Epoch[364](260/324): Loss: 0.4845 || Learning rate: lr=1.25e-05.
===> Epoch[364](270/324): Loss: 0.6188 || Learning rate: lr=1.25e-05.
===> Epoch[364](280/324): Loss: 0.7973 || Learning rate: lr=1.25e-05.
===> Epoch[364](290/324): Loss: 0.6656 || Learning rate: lr=1.25e-05.
===> Epoch[364](300/324): Loss: 0.6286 || Learning rate: lr=1.25e-05.
===> Epoch[364](310/324): Loss: 0.6561 || Learning rate: lr=1.25e-05.
===> Epoch[364](320/324): Loss: 0.6756 || Learning rate: lr=1.25e-05.
===> Epoch[365](10/324): Loss: 0.5490 || Learning rate: lr=1.25e-05.
===> Epoch[365](20/324): Loss: 0.5877 || Learning rate: lr=1.25e-05.
===> Epoch[365](30/324): Loss: 0.4557 || Learning rate: lr=1.25e-05.
===> Epoch[365](40/324): Loss: 0.5255 || Learning rate: lr=1.25e-05.
===> Epoch[365](50/324): Loss: 0.4807 || Learning rate: lr=1.25e-05.
===> Epoch[365](60/324): Loss: 0.8511 || Learning rate: lr=1.25e-05.
===> Epoch[365](70/324): Loss: 0.6517 || Learning rate: lr=1.25e-05.
===> Epoch[365](80/324): Loss: 0.5206 || Learning rate: lr=1.25e-05.
===> Epoch[365](90/324): Loss: 0.5707 || Learning rate: lr=1.25e-05.
===> Epoch[365](100/324): Loss: 0.7351 || Learning rate: lr=1.25e-05.
===> Epoch[365](110/324): Loss: 0.4268 || Learning rate: lr=1.25e-05.
===> Epoch[365](120/324): Loss: 0.6275 || Learning rate: lr=1.25e-05.
===> Epoch[365](130/324): Loss: 0.7258 || Learning rate: lr=1.25e-05.
===> Epoch[365](140/324): Loss: 0.8505 || Learning rate: lr=1.25e-05.
===> Epoch[365](150/324): Loss: 0.5771 || Learning rate: lr=1.25e-05.
===> Epoch[365](160/324): Loss: 0.6062 || Learning rate: lr=1.25e-05.
===> Epoch[365](170/324): Loss: 0.6453 || Learning rate: lr=1.25e-05.
===> Epoch[365](180/324): Loss: 0.8545 || Learning rate: lr=1.25e-05.
===> Epoch[365](190/324): Loss: 0.7925 || Learning rate: lr=1.25e-05.
===> Epoch[365](200/324): Loss: 0.5711 || Learning rate: lr=1.25e-05.
===> Epoch[365](210/324): Loss: 0.6728 || Learning rate: lr=1.25e-05.
===> Epoch[365](220/324): Loss: 0.7964 || Learning rate: lr=1.25e-05.
===> Epoch[365](230/324): Loss: 0.5000 || Learning rate: lr=1.25e-05.
===> Epoch[365](240/324): Loss: 0.6030 || Learning rate: lr=1.25e-05.
===> Epoch[365](250/324): Loss: 0.3881 || Learning rate: lr=1.25e-05.
===> Epoch[365](260/324): Loss: 1.1730 || Learning rate: lr=1.25e-05.
===> Epoch[365](270/324): Loss: 0.6304 || Learning rate: lr=1.25e-05.
===> Epoch[365](280/324): Loss: 0.6852 || Learning rate: lr=1.25e-05.
===> Epoch[365](290/324): Loss: 0.5703 || Learning rate: lr=1.25e-05.
===> Epoch[365](300/324): Loss: 0.7939 || Learning rate: lr=1.25e-05.
===> Epoch[365](310/324): Loss: 1.0579 || Learning rate: lr=1.25e-05.
===> Epoch[365](320/324): Loss: 0.6203 || Learning rate: lr=1.25e-05.
===> Epoch[366](10/324): Loss: 0.5267 || Learning rate: lr=1.25e-05.
===> Epoch[366](20/324): Loss: 0.8219 || Learning rate: lr=1.25e-05.
===> Epoch[366](30/324): Loss: 0.6948 || Learning rate: lr=1.25e-05.
===> Epoch[366](40/324): Loss: 0.4354 || Learning rate: lr=1.25e-05.
===> Epoch[366](50/324): Loss: 0.6984 || Learning rate: lr=1.25e-05.
===> Epoch[366](60/324): Loss: 0.4184 || Learning rate: lr=1.25e-05.
===> Epoch[366](70/324): Loss: 0.7412 || Learning rate: lr=1.25e-05.
===> Epoch[366](80/324): Loss: 0.6991 || Learning rate: lr=1.25e-05.
===> Epoch[366](90/324): Loss: 0.8566 || Learning rate: lr=1.25e-05.
===> Epoch[366](100/324): Loss: 0.5792 || Learning rate: lr=1.25e-05.
===> Epoch[366](110/324): Loss: 0.5913 || Learning rate: lr=1.25e-05.
===> Epoch[366](120/324): Loss: 0.6613 || Learning rate: lr=1.25e-05.
===> Epoch[366](130/324): Loss: 0.6793 || Learning rate: lr=1.25e-05.
===> Epoch[366](140/324): Loss: 0.6249 || Learning rate: lr=1.25e-05.
===> Epoch[366](150/324): Loss: 0.5730 || Learning rate: lr=1.25e-05.
===> Epoch[366](160/324): Loss: 1.0555 || Learning rate: lr=1.25e-05.
===> Epoch[366](170/324): Loss: 0.8130 || Learning rate: lr=1.25e-05.
===> Epoch[366](180/324): Loss: 0.5404 || Learning rate: lr=1.25e-05.
===> Epoch[366](190/324): Loss: 0.5550 || Learning rate: lr=1.25e-05.
===> Epoch[366](200/324): Loss: 0.5620 || Learning rate: lr=1.25e-05.
===> Epoch[366](210/324): Loss: 0.9654 || Learning rate: lr=1.25e-05.
===> Epoch[366](220/324): Loss: 0.5116 || Learning rate: lr=1.25e-05.
===> Epoch[366](230/324): Loss: 0.6652 || Learning rate: lr=1.25e-05.
===> Epoch[366](240/324): Loss: 0.6298 || Learning rate: lr=1.25e-05.
===> Epoch[366](250/324): Loss: 0.6405 || Learning rate: lr=1.25e-05.
===> Epoch[366](260/324): Loss: 0.5612 || Learning rate: lr=1.25e-05.
===> Epoch[366](270/324): Loss: 0.6305 || Learning rate: lr=1.25e-05.
===> Epoch[366](280/324): Loss: 0.7304 || Learning rate: lr=1.25e-05.
===> Epoch[366](290/324): Loss: 0.8121 || Learning rate: lr=1.25e-05.
===> Epoch[366](300/324): Loss: 0.5153 || Learning rate: lr=1.25e-05.
===> Epoch[366](310/324): Loss: 0.5202 || Learning rate: lr=1.25e-05.
===> Epoch[366](320/324): Loss: 0.7540 || Learning rate: lr=1.25e-05.
===> Epoch[367](10/324): Loss: 0.6473 || Learning rate: lr=1.25e-05.
===> Epoch[367](20/324): Loss: 0.4700 || Learning rate: lr=1.25e-05.
===> Epoch[367](30/324): Loss: 0.5463 || Learning rate: lr=1.25e-05.
===> Epoch[367](40/324): Loss: 0.4993 || Learning rate: lr=1.25e-05.
===> Epoch[367](50/324): Loss: 0.5309 || Learning rate: lr=1.25e-05.
===> Epoch[367](60/324): Loss: 0.6767 || Learning rate: lr=1.25e-05.
===> Epoch[367](70/324): Loss: 0.4579 || Learning rate: lr=1.25e-05.
===> Epoch[367](80/324): Loss: 0.6434 || Learning rate: lr=1.25e-05.
===> Epoch[367](90/324): Loss: 0.7186 || Learning rate: lr=1.25e-05.
===> Epoch[367](100/324): Loss: 0.8419 || Learning rate: lr=1.25e-05.
===> Epoch[367](110/324): Loss: 0.9703 || Learning rate: lr=1.25e-05.
===> Epoch[367](120/324): Loss: 0.5293 || Learning rate: lr=1.25e-05.
===> Epoch[367](130/324): Loss: 0.6212 || Learning rate: lr=1.25e-05.
===> Epoch[367](140/324): Loss: 0.7887 || Learning rate: lr=1.25e-05.
===> Epoch[367](150/324): Loss: 0.4984 || Learning rate: lr=1.25e-05.
===> Epoch[367](160/324): Loss: 0.8221 || Learning rate: lr=1.25e-05.
===> Epoch[367](170/324): Loss: 0.8309 || Learning rate: lr=1.25e-05.
===> Epoch[367](180/324): Loss: 0.6527 || Learning rate: lr=1.25e-05.
===> Epoch[367](190/324): Loss: 0.5763 || Learning rate: lr=1.25e-05.
===> Epoch[367](200/324): Loss: 0.4846 || Learning rate: lr=1.25e-05.
===> Epoch[367](210/324): Loss: 0.8140 || Learning rate: lr=1.25e-05.
===> Epoch[367](220/324): Loss: 0.6923 || Learning rate: lr=1.25e-05.
===> Epoch[367](230/324): Loss: 0.6926 || Learning rate: lr=1.25e-05.
===> Epoch[367](240/324): Loss: 0.6336 || Learning rate: lr=1.25e-05.
===> Epoch[367](250/324): Loss: 0.8054 || Learning rate: lr=1.25e-05.
===> Epoch[367](260/324): Loss: 0.5602 || Learning rate: lr=1.25e-05.
===> Epoch[367](270/324): Loss: 0.8644 || Learning rate: lr=1.25e-05.
===> Epoch[367](280/324): Loss: 0.5703 || Learning rate: lr=1.25e-05.
===> Epoch[367](290/324): Loss: 0.4793 || Learning rate: lr=1.25e-05.
===> Epoch[367](300/324): Loss: 0.5109 || Learning rate: lr=1.25e-05.
===> Epoch[367](310/324): Loss: 1.2150 || Learning rate: lr=1.25e-05.
===> Epoch[367](320/324): Loss: 0.5644 || Learning rate: lr=1.25e-05.
===> Epoch[368](10/324): Loss: 0.6581 || Learning rate: lr=1.25e-05.
===> Epoch[368](20/324): Loss: 0.7585 || Learning rate: lr=1.25e-05.
===> Epoch[368](30/324): Loss: 0.8668 || Learning rate: lr=1.25e-05.
===> Epoch[368](40/324): Loss: 0.6959 || Learning rate: lr=1.25e-05.
===> Epoch[368](50/324): Loss: 0.5641 || Learning rate: lr=1.25e-05.
===> Epoch[368](60/324): Loss: 0.7128 || Learning rate: lr=1.25e-05.
===> Epoch[368](70/324): Loss: 0.5879 || Learning rate: lr=1.25e-05.
===> Epoch[368](80/324): Loss: 0.5791 || Learning rate: lr=1.25e-05.
===> Epoch[368](90/324): Loss: 0.7739 || Learning rate: lr=1.25e-05.
===> Epoch[368](100/324): Loss: 0.6476 || Learning rate: lr=1.25e-05.
===> Epoch[368](110/324): Loss: 1.1454 || Learning rate: lr=1.25e-05.
===> Epoch[368](120/324): Loss: 0.8865 || Learning rate: lr=1.25e-05.
===> Epoch[368](130/324): Loss: 0.7354 || Learning rate: lr=1.25e-05.
===> Epoch[368](140/324): Loss: 0.4831 || Learning rate: lr=1.25e-05.
===> Epoch[368](150/324): Loss: 0.9079 || Learning rate: lr=1.25e-05.
===> Epoch[368](160/324): Loss: 0.5695 || Learning rate: lr=1.25e-05.
===> Epoch[368](170/324): Loss: 0.6160 || Learning rate: lr=1.25e-05.
===> Epoch[368](180/324): Loss: 0.5775 || Learning rate: lr=1.25e-05.
===> Epoch[368](190/324): Loss: 0.5113 || Learning rate: lr=1.25e-05.
===> Epoch[368](200/324): Loss: 0.5258 || Learning rate: lr=1.25e-05.
===> Epoch[368](210/324): Loss: 0.6647 || Learning rate: lr=1.25e-05.
===> Epoch[368](220/324): Loss: 0.5416 || Learning rate: lr=1.25e-05.
===> Epoch[368](230/324): Loss: 0.5954 || Learning rate: lr=1.25e-05.
===> Epoch[368](240/324): Loss: 0.4511 || Learning rate: lr=1.25e-05.
===> Epoch[368](250/324): Loss: 0.7207 || Learning rate: lr=1.25e-05.
===> Epoch[368](260/324): Loss: 0.6461 || Learning rate: lr=1.25e-05.
===> Epoch[368](270/324): Loss: 0.3737 || Learning rate: lr=1.25e-05.
===> Epoch[368](280/324): Loss: 0.5652 || Learning rate: lr=1.25e-05.
===> Epoch[368](290/324): Loss: 0.8609 || Learning rate: lr=1.25e-05.
===> Epoch[368](300/324): Loss: 0.4726 || Learning rate: lr=1.25e-05.
===> Epoch[368](310/324): Loss: 0.5190 || Learning rate: lr=1.25e-05.
===> Epoch[368](320/324): Loss: 0.6373 || Learning rate: lr=1.25e-05.
===> Epoch[369](10/324): Loss: 0.7151 || Learning rate: lr=1.25e-05.
===> Epoch[369](20/324): Loss: 0.4709 || Learning rate: lr=1.25e-05.
===> Epoch[369](30/324): Loss: 0.6899 || Learning rate: lr=1.25e-05.
===> Epoch[369](40/324): Loss: 0.8699 || Learning rate: lr=1.25e-05.
===> Epoch[369](50/324): Loss: 0.7377 || Learning rate: lr=1.25e-05.
===> Epoch[369](60/324): Loss: 0.6060 || Learning rate: lr=1.25e-05.
===> Epoch[369](70/324): Loss: 0.6013 || Learning rate: lr=1.25e-05.
===> Epoch[369](80/324): Loss: 0.8144 || Learning rate: lr=1.25e-05.
===> Epoch[369](90/324): Loss: 0.4859 || Learning rate: lr=1.25e-05.
===> Epoch[369](100/324): Loss: 0.6771 || Learning rate: lr=1.25e-05.
===> Epoch[369](110/324): Loss: 0.6434 || Learning rate: lr=1.25e-05.
===> Epoch[369](120/324): Loss: 0.6055 || Learning rate: lr=1.25e-05.
===> Epoch[369](130/324): Loss: 0.6880 || Learning rate: lr=1.25e-05.
===> Epoch[369](140/324): Loss: 0.6436 || Learning rate: lr=1.25e-05.
===> Epoch[369](150/324): Loss: 0.5942 || Learning rate: lr=1.25e-05.
===> Epoch[369](160/324): Loss: 0.7519 || Learning rate: lr=1.25e-05.
===> Epoch[369](170/324): Loss: 0.4489 || Learning rate: lr=1.25e-05.
===> Epoch[369](180/324): Loss: 0.5837 || Learning rate: lr=1.25e-05.
===> Epoch[369](190/324): Loss: 0.5443 || Learning rate: lr=1.25e-05.
===> Epoch[369](200/324): Loss: 0.7742 || Learning rate: lr=1.25e-05.
===> Epoch[369](210/324): Loss: 0.5008 || Learning rate: lr=1.25e-05.
===> Epoch[369](220/324): Loss: 0.5401 || Learning rate: lr=1.25e-05.
===> Epoch[369](230/324): Loss: 0.7649 || Learning rate: lr=1.25e-05.
===> Epoch[369](240/324): Loss: 0.4861 || Learning rate: lr=1.25e-05.
===> Epoch[369](250/324): Loss: 0.5765 || Learning rate: lr=1.25e-05.
===> Epoch[369](260/324): Loss: 0.6494 || Learning rate: lr=1.25e-05.
===> Epoch[369](270/324): Loss: 0.7032 || Learning rate: lr=1.25e-05.
===> Epoch[369](280/324): Loss: 0.5089 || Learning rate: lr=1.25e-05.
===> Epoch[369](290/324): Loss: 0.5710 || Learning rate: lr=1.25e-05.
===> Epoch[369](300/324): Loss: 0.5665 || Learning rate: lr=1.25e-05.
===> Epoch[369](310/324): Loss: 0.8017 || Learning rate: lr=1.25e-05.
===> Epoch[369](320/324): Loss: 0.6373 || Learning rate: lr=1.25e-05.
===> Epoch[370](10/324): Loss: 0.4636 || Learning rate: lr=1.25e-05.
===> Epoch[370](20/324): Loss: 0.9328 || Learning rate: lr=1.25e-05.
===> Epoch[370](30/324): Loss: 0.6079 || Learning rate: lr=1.25e-05.
===> Epoch[370](40/324): Loss: 0.5960 || Learning rate: lr=1.25e-05.
===> Epoch[370](50/324): Loss: 0.7038 || Learning rate: lr=1.25e-05.
===> Epoch[370](60/324): Loss: 0.6174 || Learning rate: lr=1.25e-05.
===> Epoch[370](70/324): Loss: 0.5446 || Learning rate: lr=1.25e-05.
===> Epoch[370](80/324): Loss: 0.6695 || Learning rate: lr=1.25e-05.
===> Epoch[370](90/324): Loss: 0.6968 || Learning rate: lr=1.25e-05.
===> Epoch[370](100/324): Loss: 0.5338 || Learning rate: lr=1.25e-05.
===> Epoch[370](110/324): Loss: 0.6602 || Learning rate: lr=1.25e-05.
===> Epoch[370](120/324): Loss: 0.6311 || Learning rate: lr=1.25e-05.
===> Epoch[370](130/324): Loss: 0.7449 || Learning rate: lr=1.25e-05.
===> Epoch[370](140/324): Loss: 0.5245 || Learning rate: lr=1.25e-05.
===> Epoch[370](150/324): Loss: 0.6066 || Learning rate: lr=1.25e-05.
===> Epoch[370](160/324): Loss: 0.4630 || Learning rate: lr=1.25e-05.
===> Epoch[370](170/324): Loss: 0.5724 || Learning rate: lr=1.25e-05.
===> Epoch[370](180/324): Loss: 0.5366 || Learning rate: lr=1.25e-05.
===> Epoch[370](190/324): Loss: 0.6271 || Learning rate: lr=1.25e-05.
===> Epoch[370](200/324): Loss: 0.9212 || Learning rate: lr=1.25e-05.
===> Epoch[370](210/324): Loss: 0.7908 || Learning rate: lr=1.25e-05.
===> Epoch[370](220/324): Loss: 0.6336 || Learning rate: lr=1.25e-05.
===> Epoch[370](230/324): Loss: 1.0312 || Learning rate: lr=1.25e-05.
===> Epoch[370](240/324): Loss: 0.7831 || Learning rate: lr=1.25e-05.
===> Epoch[370](250/324): Loss: 0.5239 || Learning rate: lr=1.25e-05.
===> Epoch[370](260/324): Loss: 0.6562 || Learning rate: lr=1.25e-05.
===> Epoch[370](270/324): Loss: 0.6683 || Learning rate: lr=1.25e-05.
===> Epoch[370](280/324): Loss: 0.4978 || Learning rate: lr=1.25e-05.
===> Epoch[370](290/324): Loss: 0.5968 || Learning rate: lr=1.25e-05.
===> Epoch[370](300/324): Loss: 0.6766 || Learning rate: lr=1.25e-05.
===> Epoch[370](310/324): Loss: 0.5378 || Learning rate: lr=1.25e-05.
===> Epoch[370](320/324): Loss: 0.5635 || Learning rate: lr=1.25e-05.
===> Epoch[371](10/324): Loss: 0.4606 || Learning rate: lr=1.25e-05.
===> Epoch[371](20/324): Loss: 0.9129 || Learning rate: lr=1.25e-05.
===> Epoch[371](30/324): Loss: 0.6695 || Learning rate: lr=1.25e-05.
===> Epoch[371](40/324): Loss: 0.8443 || Learning rate: lr=1.25e-05.
===> Epoch[371](50/324): Loss: 0.5947 || Learning rate: lr=1.25e-05.
===> Epoch[371](60/324): Loss: 0.6596 || Learning rate: lr=1.25e-05.
===> Epoch[371](70/324): Loss: 0.6751 || Learning rate: lr=1.25e-05.
===> Epoch[371](80/324): Loss: 0.6720 || Learning rate: lr=1.25e-05.
===> Epoch[371](90/324): Loss: 0.5695 || Learning rate: lr=1.25e-05.
===> Epoch[371](100/324): Loss: 0.6857 || Learning rate: lr=1.25e-05.
===> Epoch[371](110/324): Loss: 1.0166 || Learning rate: lr=1.25e-05.
===> Epoch[371](120/324): Loss: 0.6772 || Learning rate: lr=1.25e-05.
===> Epoch[371](130/324): Loss: 0.4059 || Learning rate: lr=1.25e-05.
===> Epoch[371](140/324): Loss: 0.5349 || Learning rate: lr=1.25e-05.
===> Epoch[371](150/324): Loss: 0.7567 || Learning rate: lr=1.25e-05.
===> Epoch[371](160/324): Loss: 0.5630 || Learning rate: lr=1.25e-05.
===> Epoch[371](170/324): Loss: 0.6759 || Learning rate: lr=1.25e-05.
===> Epoch[371](180/324): Loss: 0.7393 || Learning rate: lr=1.25e-05.
===> Epoch[371](190/324): Loss: 0.8151 || Learning rate: lr=1.25e-05.
===> Epoch[371](200/324): Loss: 0.5404 || Learning rate: lr=1.25e-05.
===> Epoch[371](210/324): Loss: 0.5283 || Learning rate: lr=1.25e-05.
===> Epoch[371](220/324): Loss: 0.6515 || Learning rate: lr=1.25e-05.
===> Epoch[371](230/324): Loss: 0.5327 || Learning rate: lr=1.25e-05.
===> Epoch[371](240/324): Loss: 0.6255 || Learning rate: lr=1.25e-05.
===> Epoch[371](250/324): Loss: 0.5212 || Learning rate: lr=1.25e-05.
===> Epoch[371](260/324): Loss: 0.6601 || Learning rate: lr=1.25e-05.
===> Epoch[371](270/324): Loss: 0.5323 || Learning rate: lr=1.25e-05.
===> Epoch[371](280/324): Loss: 0.8347 || Learning rate: lr=1.25e-05.
===> Epoch[371](290/324): Loss: 0.5949 || Learning rate: lr=1.25e-05.
===> Epoch[371](300/324): Loss: 0.6776 || Learning rate: lr=1.25e-05.
===> Epoch[371](310/324): Loss: 0.5719 || Learning rate: lr=1.25e-05.
===> Epoch[371](320/324): Loss: 0.5929 || Learning rate: lr=1.25e-05.
===> Epoch[372](10/324): Loss: 0.8380 || Learning rate: lr=1.25e-05.
===> Epoch[372](20/324): Loss: 0.4753 || Learning rate: lr=1.25e-05.
===> Epoch[372](30/324): Loss: 0.6197 || Learning rate: lr=1.25e-05.
===> Epoch[372](40/324): Loss: 0.6958 || Learning rate: lr=1.25e-05.
===> Epoch[372](50/324): Loss: 0.6172 || Learning rate: lr=1.25e-05.
===> Epoch[372](60/324): Loss: 0.6680 || Learning rate: lr=1.25e-05.
===> Epoch[372](70/324): Loss: 0.6350 || Learning rate: lr=1.25e-05.
===> Epoch[372](80/324): Loss: 0.6737 || Learning rate: lr=1.25e-05.
===> Epoch[372](90/324): Loss: 0.5007 || Learning rate: lr=1.25e-05.
===> Epoch[372](100/324): Loss: 0.5655 || Learning rate: lr=1.25e-05.
===> Epoch[372](110/324): Loss: 0.6640 || Learning rate: lr=1.25e-05.
===> Epoch[372](120/324): Loss: 0.6322 || Learning rate: lr=1.25e-05.
===> Epoch[372](130/324): Loss: 0.5320 || Learning rate: lr=1.25e-05.
===> Epoch[372](140/324): Loss: 0.5399 || Learning rate: lr=1.25e-05.
===> Epoch[372](150/324): Loss: 0.4618 || Learning rate: lr=1.25e-05.
===> Epoch[372](160/324): Loss: 0.7358 || Learning rate: lr=1.25e-05.
===> Epoch[372](170/324): Loss: 0.8936 || Learning rate: lr=1.25e-05.
===> Epoch[372](180/324): Loss: 0.8020 || Learning rate: lr=1.25e-05.
===> Epoch[372](190/324): Loss: 0.6766 || Learning rate: lr=1.25e-05.
===> Epoch[372](200/324): Loss: 0.4747 || Learning rate: lr=1.25e-05.
===> Epoch[372](210/324): Loss: 0.6773 || Learning rate: lr=1.25e-05.
===> Epoch[372](220/324): Loss: 0.5511 || Learning rate: lr=1.25e-05.
===> Epoch[372](230/324): Loss: 0.7729 || Learning rate: lr=1.25e-05.
===> Epoch[372](240/324): Loss: 0.4553 || Learning rate: lr=1.25e-05.
===> Epoch[372](250/324): Loss: 0.6769 || Learning rate: lr=1.25e-05.
===> Epoch[372](260/324): Loss: 0.6913 || Learning rate: lr=1.25e-05.
===> Epoch[372](270/324): Loss: 0.6963 || Learning rate: lr=1.25e-05.
===> Epoch[372](280/324): Loss: 0.6815 || Learning rate: lr=1.25e-05.
===> Epoch[372](290/324): Loss: 0.4979 || Learning rate: lr=1.25e-05.
===> Epoch[372](300/324): Loss: 0.6105 || Learning rate: lr=1.25e-05.
===> Epoch[372](310/324): Loss: 0.5221 || Learning rate: lr=1.25e-05.
===> Epoch[372](320/324): Loss: 0.8788 || Learning rate: lr=1.25e-05.
===> Epoch[373](10/324): Loss: 0.5968 || Learning rate: lr=1.25e-05.
===> Epoch[373](20/324): Loss: 0.9499 || Learning rate: lr=1.25e-05.
===> Epoch[373](30/324): Loss: 0.7031 || Learning rate: lr=1.25e-05.
===> Epoch[373](40/324): Loss: 0.7689 || Learning rate: lr=1.25e-05.
===> Epoch[373](50/324): Loss: 0.7200 || Learning rate: lr=1.25e-05.
===> Epoch[373](60/324): Loss: 0.3914 || Learning rate: lr=1.25e-05.
===> Epoch[373](70/324): Loss: 0.7122 || Learning rate: lr=1.25e-05.
===> Epoch[373](80/324): Loss: 0.4609 || Learning rate: lr=1.25e-05.
===> Epoch[373](90/324): Loss: 0.8387 || Learning rate: lr=1.25e-05.
===> Epoch[373](100/324): Loss: 0.4607 || Learning rate: lr=1.25e-05.
===> Epoch[373](110/324): Loss: 0.5500 || Learning rate: lr=1.25e-05.
===> Epoch[373](120/324): Loss: 0.5989 || Learning rate: lr=1.25e-05.
===> Epoch[373](130/324): Loss: 0.5602 || Learning rate: lr=1.25e-05.
===> Epoch[373](140/324): Loss: 0.5172 || Learning rate: lr=1.25e-05.
===> Epoch[373](150/324): Loss: 0.5475 || Learning rate: lr=1.25e-05.
===> Epoch[373](160/324): Loss: 0.7591 || Learning rate: lr=1.25e-05.
===> Epoch[373](170/324): Loss: 0.4178 || Learning rate: lr=1.25e-05.
===> Epoch[373](180/324): Loss: 0.7243 || Learning rate: lr=1.25e-05.
===> Epoch[373](190/324): Loss: 0.5768 || Learning rate: lr=1.25e-05.
===> Epoch[373](200/324): Loss: 0.8268 || Learning rate: lr=1.25e-05.
===> Epoch[373](210/324): Loss: 0.6210 || Learning rate: lr=1.25e-05.
===> Epoch[373](220/324): Loss: 0.6278 || Learning rate: lr=1.25e-05.
===> Epoch[373](230/324): Loss: 0.5443 || Learning rate: lr=1.25e-05.
===> Epoch[373](240/324): Loss: 0.4731 || Learning rate: lr=1.25e-05.
===> Epoch[373](250/324): Loss: 0.4233 || Learning rate: lr=1.25e-05.
===> Epoch[373](260/324): Loss: 0.5356 || Learning rate: lr=1.25e-05.
===> Epoch[373](270/324): Loss: 0.8287 || Learning rate: lr=1.25e-05.
===> Epoch[373](280/324): Loss: 1.0766 || Learning rate: lr=1.25e-05.
===> Epoch[373](290/324): Loss: 0.6101 || Learning rate: lr=1.25e-05.
===> Epoch[373](300/324): Loss: 0.8062 || Learning rate: lr=1.25e-05.
===> Epoch[373](310/324): Loss: 0.4711 || Learning rate: lr=1.25e-05.
===> Epoch[373](320/324): Loss: 0.9820 || Learning rate: lr=1.25e-05.
===> Epoch[374](10/324): Loss: 0.7333 || Learning rate: lr=1.25e-05.
===> Epoch[374](20/324): Loss: 0.6094 || Learning rate: lr=1.25e-05.
===> Epoch[374](30/324): Loss: 0.5658 || Learning rate: lr=1.25e-05.
===> Epoch[374](40/324): Loss: 0.7888 || Learning rate: lr=1.25e-05.
===> Epoch[374](50/324): Loss: 0.8002 || Learning rate: lr=1.25e-05.
===> Epoch[374](60/324): Loss: 0.5946 || Learning rate: lr=1.25e-05.
===> Epoch[374](70/324): Loss: 0.7806 || Learning rate: lr=1.25e-05.
===> Epoch[374](80/324): Loss: 0.6597 || Learning rate: lr=1.25e-05.
===> Epoch[374](90/324): Loss: 0.5849 || Learning rate: lr=1.25e-05.
===> Epoch[374](100/324): Loss: 0.7131 || Learning rate: lr=1.25e-05.
===> Epoch[374](110/324): Loss: 0.4173 || Learning rate: lr=1.25e-05.
===> Epoch[374](120/324): Loss: 0.4825 || Learning rate: lr=1.25e-05.
===> Epoch[374](130/324): Loss: 0.6049 || Learning rate: lr=1.25e-05.
===> Epoch[374](140/324): Loss: 0.7240 || Learning rate: lr=1.25e-05.
===> Epoch[374](150/324): Loss: 0.5102 || Learning rate: lr=1.25e-05.
===> Epoch[374](160/324): Loss: 0.4210 || Learning rate: lr=1.25e-05.
===> Epoch[374](170/324): Loss: 0.4643 || Learning rate: lr=1.25e-05.
===> Epoch[374](180/324): Loss: 0.6088 || Learning rate: lr=1.25e-05.
===> Epoch[374](190/324): Loss: 0.8021 || Learning rate: lr=1.25e-05.
===> Epoch[374](200/324): Loss: 0.5202 || Learning rate: lr=1.25e-05.
===> Epoch[374](210/324): Loss: 0.9354 || Learning rate: lr=1.25e-05.
===> Epoch[374](220/324): Loss: 0.5989 || Learning rate: lr=1.25e-05.
===> Epoch[374](230/324): Loss: 0.6072 || Learning rate: lr=1.25e-05.
===> Epoch[374](240/324): Loss: 0.6131 || Learning rate: lr=1.25e-05.
===> Epoch[374](250/324): Loss: 0.6941 || Learning rate: lr=1.25e-05.
===> Epoch[374](260/324): Loss: 0.5387 || Learning rate: lr=1.25e-05.
===> Epoch[374](270/324): Loss: 0.6987 || Learning rate: lr=1.25e-05.
===> Epoch[374](280/324): Loss: 0.6471 || Learning rate: lr=1.25e-05.
===> Epoch[374](290/324): Loss: 0.5915 || Learning rate: lr=1.25e-05.
===> Epoch[374](300/324): Loss: 0.6778 || Learning rate: lr=1.25e-05.
===> Epoch[374](310/324): Loss: 0.8323 || Learning rate: lr=1.25e-05.
===> Epoch[374](320/324): Loss: 0.5294 || Learning rate: lr=1.25e-05.
===> Epoch[375](10/324): Loss: 0.5843 || Learning rate: lr=1.25e-05.
===> Epoch[375](20/324): Loss: 0.8525 || Learning rate: lr=1.25e-05.
===> Epoch[375](30/324): Loss: 0.8605 || Learning rate: lr=1.25e-05.
===> Epoch[375](40/324): Loss: 0.6512 || Learning rate: lr=1.25e-05.
===> Epoch[375](50/324): Loss: 0.6239 || Learning rate: lr=1.25e-05.
===> Epoch[375](60/324): Loss: 0.8153 || Learning rate: lr=1.25e-05.
===> Epoch[375](70/324): Loss: 0.4617 || Learning rate: lr=1.25e-05.
===> Epoch[375](80/324): Loss: 0.7383 || Learning rate: lr=1.25e-05.
===> Epoch[375](90/324): Loss: 0.8721 || Learning rate: lr=1.25e-05.
===> Epoch[375](100/324): Loss: 0.8460 || Learning rate: lr=1.25e-05.
===> Epoch[375](110/324): Loss: 0.5569 || Learning rate: lr=1.25e-05.
===> Epoch[375](120/324): Loss: 0.7042 || Learning rate: lr=1.25e-05.
===> Epoch[375](130/324): Loss: 0.9908 || Learning rate: lr=1.25e-05.
===> Epoch[375](140/324): Loss: 0.9149 || Learning rate: lr=1.25e-05.
===> Epoch[375](150/324): Loss: 0.3733 || Learning rate: lr=1.25e-05.
===> Epoch[375](160/324): Loss: 0.5084 || Learning rate: lr=1.25e-05.
===> Epoch[375](170/324): Loss: 0.6458 || Learning rate: lr=1.25e-05.
===> Epoch[375](180/324): Loss: 0.6689 || Learning rate: lr=1.25e-05.
===> Epoch[375](190/324): Loss: 0.5990 || Learning rate: lr=1.25e-05.
===> Epoch[375](200/324): Loss: 0.8778 || Learning rate: lr=1.25e-05.
===> Epoch[375](210/324): Loss: 0.6678 || Learning rate: lr=1.25e-05.
===> Epoch[375](220/324): Loss: 0.5926 || Learning rate: lr=1.25e-05.
===> Epoch[375](230/324): Loss: 0.9691 || Learning rate: lr=1.25e-05.
===> Epoch[375](240/324): Loss: 0.5673 || Learning rate: lr=1.25e-05.
===> Epoch[375](250/324): Loss: 0.6525 || Learning rate: lr=1.25e-05.
===> Epoch[375](260/324): Loss: 0.8510 || Learning rate: lr=1.25e-05.
===> Epoch[375](270/324): Loss: 0.6271 || Learning rate: lr=1.25e-05.
===> Epoch[375](280/324): Loss: 0.5438 || Learning rate: lr=1.25e-05.
===> Epoch[375](290/324): Loss: 0.3509 || Learning rate: lr=1.25e-05.
===> Epoch[375](300/324): Loss: 0.6369 || Learning rate: lr=1.25e-05.
===> Epoch[375](310/324): Loss: 0.5045 || Learning rate: lr=1.25e-05.
===> Epoch[375](320/324): Loss: 0.4331 || Learning rate: lr=1.25e-05.
===> Epoch[376](10/324): Loss: 0.6196 || Learning rate: lr=1.25e-05.
===> Epoch[376](20/324): Loss: 0.5065 || Learning rate: lr=1.25e-05.
===> Epoch[376](30/324): Loss: 0.7294 || Learning rate: lr=1.25e-05.
===> Epoch[376](40/324): Loss: 1.1843 || Learning rate: lr=1.25e-05.
===> Epoch[376](50/324): Loss: 0.5429 || Learning rate: lr=1.25e-05.
===> Epoch[376](60/324): Loss: 0.6596 || Learning rate: lr=1.25e-05.
===> Epoch[376](70/324): Loss: 0.5886 || Learning rate: lr=1.25e-05.
===> Epoch[376](80/324): Loss: 0.9428 || Learning rate: lr=1.25e-05.
===> Epoch[376](90/324): Loss: 0.5619 || Learning rate: lr=1.25e-05.
===> Epoch[376](100/324): Loss: 0.4834 || Learning rate: lr=1.25e-05.
===> Epoch[376](110/324): Loss: 0.6156 || Learning rate: lr=1.25e-05.
===> Epoch[376](120/324): Loss: 0.8193 || Learning rate: lr=1.25e-05.
===> Epoch[376](130/324): Loss: 0.5652 || Learning rate: lr=1.25e-05.
===> Epoch[376](140/324): Loss: 0.7541 || Learning rate: lr=1.25e-05.
===> Epoch[376](150/324): Loss: 0.5503 || Learning rate: lr=1.25e-05.
===> Epoch[376](160/324): Loss: 0.6034 || Learning rate: lr=1.25e-05.
===> Epoch[376](170/324): Loss: 0.7775 || Learning rate: lr=1.25e-05.
===> Epoch[376](180/324): Loss: 0.5908 || Learning rate: lr=1.25e-05.
===> Epoch[376](190/324): Loss: 0.7124 || Learning rate: lr=1.25e-05.
===> Epoch[376](200/324): Loss: 0.6661 || Learning rate: lr=1.25e-05.
===> Epoch[376](210/324): Loss: 0.8340 || Learning rate: lr=1.25e-05.
===> Epoch[376](220/324): Loss: 0.8144 || Learning rate: lr=1.25e-05.
===> Epoch[376](230/324): Loss: 0.6696 || Learning rate: lr=1.25e-05.
===> Epoch[376](240/324): Loss: 0.4651 || Learning rate: lr=1.25e-05.
===> Epoch[376](250/324): Loss: 0.5390 || Learning rate: lr=1.25e-05.
===> Epoch[376](260/324): Loss: 0.7121 || Learning rate: lr=1.25e-05.
===> Epoch[376](270/324): Loss: 0.8134 || Learning rate: lr=1.25e-05.
===> Epoch[376](280/324): Loss: 0.8655 || Learning rate: lr=1.25e-05.
===> Epoch[376](290/324): Loss: 0.7328 || Learning rate: lr=1.25e-05.
===> Epoch[376](300/324): Loss: 1.0300 || Learning rate: lr=1.25e-05.
===> Epoch[376](310/324): Loss: 0.4954 || Learning rate: lr=1.25e-05.
===> Epoch[376](320/324): Loss: 0.5069 || Learning rate: lr=1.25e-05.
===> Epoch[377](10/324): Loss: 0.5663 || Learning rate: lr=1.25e-05.
===> Epoch[377](20/324): Loss: 0.7261 || Learning rate: lr=1.25e-05.
===> Epoch[377](30/324): Loss: 0.5079 || Learning rate: lr=1.25e-05.
===> Epoch[377](40/324): Loss: 0.5820 || Learning rate: lr=1.25e-05.
===> Epoch[377](50/324): Loss: 0.5642 || Learning rate: lr=1.25e-05.
===> Epoch[377](60/324): Loss: 0.5929 || Learning rate: lr=1.25e-05.
===> Epoch[377](70/324): Loss: 0.4737 || Learning rate: lr=1.25e-05.
===> Epoch[377](80/324): Loss: 0.6109 || Learning rate: lr=1.25e-05.
===> Epoch[377](90/324): Loss: 0.5676 || Learning rate: lr=1.25e-05.
===> Epoch[377](100/324): Loss: 0.6208 || Learning rate: lr=1.25e-05.
===> Epoch[377](110/324): Loss: 1.0639 || Learning rate: lr=1.25e-05.
===> Epoch[377](120/324): Loss: 0.8449 || Learning rate: lr=1.25e-05.
===> Epoch[377](130/324): Loss: 0.4133 || Learning rate: lr=1.25e-05.
===> Epoch[377](140/324): Loss: 0.6788 || Learning rate: lr=1.25e-05.
===> Epoch[377](150/324): Loss: 0.4522 || Learning rate: lr=1.25e-05.
===> Epoch[377](160/324): Loss: 0.2954 || Learning rate: lr=1.25e-05.
===> Epoch[377](170/324): Loss: 0.4968 || Learning rate: lr=1.25e-05.
===> Epoch[377](180/324): Loss: 0.6961 || Learning rate: lr=1.25e-05.
===> Epoch[377](190/324): Loss: 0.7476 || Learning rate: lr=1.25e-05.
===> Epoch[377](200/324): Loss: 0.5587 || Learning rate: lr=1.25e-05.
===> Epoch[377](210/324): Loss: 0.4728 || Learning rate: lr=1.25e-05.
===> Epoch[377](220/324): Loss: 0.6155 || Learning rate: lr=1.25e-05.
===> Epoch[377](230/324): Loss: 0.5891 || Learning rate: lr=1.25e-05.
===> Epoch[377](240/324): Loss: 0.5659 || Learning rate: lr=1.25e-05.
===> Epoch[377](250/324): Loss: 0.6224 || Learning rate: lr=1.25e-05.
===> Epoch[377](260/324): Loss: 0.7842 || Learning rate: lr=1.25e-05.
===> Epoch[377](270/324): Loss: 0.8489 || Learning rate: lr=1.25e-05.
===> Epoch[377](280/324): Loss: 0.8017 || Learning rate: lr=1.25e-05.
===> Epoch[377](290/324): Loss: 0.8152 || Learning rate: lr=1.25e-05.
===> Epoch[377](300/324): Loss: 0.4442 || Learning rate: lr=1.25e-05.
===> Epoch[377](310/324): Loss: 0.6166 || Learning rate: lr=1.25e-05.
===> Epoch[377](320/324): Loss: 0.8186 || Learning rate: lr=1.25e-05.
===> Epoch[378](10/324): Loss: 0.4367 || Learning rate: lr=1.25e-05.
===> Epoch[378](20/324): Loss: 0.6810 || Learning rate: lr=1.25e-05.
===> Epoch[378](30/324): Loss: 0.3875 || Learning rate: lr=1.25e-05.
===> Epoch[378](40/324): Loss: 0.5997 || Learning rate: lr=1.25e-05.
===> Epoch[378](50/324): Loss: 0.7609 || Learning rate: lr=1.25e-05.
===> Epoch[378](60/324): Loss: 0.6041 || Learning rate: lr=1.25e-05.
===> Epoch[378](70/324): Loss: 0.7439 || Learning rate: lr=1.25e-05.
===> Epoch[378](80/324): Loss: 0.5273 || Learning rate: lr=1.25e-05.
===> Epoch[378](90/324): Loss: 0.6512 || Learning rate: lr=1.25e-05.
===> Epoch[378](100/324): Loss: 0.8015 || Learning rate: lr=1.25e-05.
===> Epoch[378](110/324): Loss: 0.6428 || Learning rate: lr=1.25e-05.
===> Epoch[378](120/324): Loss: 0.4601 || Learning rate: lr=1.25e-05.
===> Epoch[378](130/324): Loss: 0.6114 || Learning rate: lr=1.25e-05.
===> Epoch[378](140/324): Loss: 0.5156 || Learning rate: lr=1.25e-05.
===> Epoch[378](150/324): Loss: 0.5633 || Learning rate: lr=1.25e-05.
===> Epoch[378](160/324): Loss: 0.6582 || Learning rate: lr=1.25e-05.
===> Epoch[378](170/324): Loss: 0.5116 || Learning rate: lr=1.25e-05.
===> Epoch[378](180/324): Loss: 0.7465 || Learning rate: lr=1.25e-05.
===> Epoch[378](190/324): Loss: 0.6081 || Learning rate: lr=1.25e-05.
===> Epoch[378](200/324): Loss: 0.5284 || Learning rate: lr=1.25e-05.
===> Epoch[378](210/324): Loss: 0.7907 || Learning rate: lr=1.25e-05.
===> Epoch[378](220/324): Loss: 0.6365 || Learning rate: lr=1.25e-05.
===> Epoch[378](230/324): Loss: 0.7646 || Learning rate: lr=1.25e-05.
===> Epoch[378](240/324): Loss: 0.6549 || Learning rate: lr=1.25e-05.
===> Epoch[378](250/324): Loss: 0.7420 || Learning rate: lr=1.25e-05.
===> Epoch[378](260/324): Loss: 0.5851 || Learning rate: lr=1.25e-05.
===> Epoch[378](270/324): Loss: 0.7979 || Learning rate: lr=1.25e-05.
===> Epoch[378](280/324): Loss: 0.6265 || Learning rate: lr=1.25e-05.
===> Epoch[378](290/324): Loss: 0.7579 || Learning rate: lr=1.25e-05.
===> Epoch[378](300/324): Loss: 0.6189 || Learning rate: lr=1.25e-05.
===> Epoch[378](310/324): Loss: 0.4526 || Learning rate: lr=1.25e-05.
===> Epoch[378](320/324): Loss: 0.6351 || Learning rate: lr=1.25e-05.
===> Epoch[379](10/324): Loss: 0.7070 || Learning rate: lr=1.25e-05.
===> Epoch[379](20/324): Loss: 0.6111 || Learning rate: lr=1.25e-05.
===> Epoch[379](30/324): Loss: 0.6349 || Learning rate: lr=1.25e-05.
===> Epoch[379](40/324): Loss: 0.6850 || Learning rate: lr=1.25e-05.
===> Epoch[379](50/324): Loss: 0.6404 || Learning rate: lr=1.25e-05.
===> Epoch[379](60/324): Loss: 0.7322 || Learning rate: lr=1.25e-05.
===> Epoch[379](70/324): Loss: 0.6296 || Learning rate: lr=1.25e-05.
===> Epoch[379](80/324): Loss: 0.7085 || Learning rate: lr=1.25e-05.
===> Epoch[379](90/324): Loss: 0.8831 || Learning rate: lr=1.25e-05.
===> Epoch[379](100/324): Loss: 0.5100 || Learning rate: lr=1.25e-05.
===> Epoch[379](110/324): Loss: 0.4911 || Learning rate: lr=1.25e-05.
===> Epoch[379](120/324): Loss: 0.6629 || Learning rate: lr=1.25e-05.
===> Epoch[379](130/324): Loss: 0.9659 || Learning rate: lr=1.25e-05.
===> Epoch[379](140/324): Loss: 0.7684 || Learning rate: lr=1.25e-05.
===> Epoch[379](150/324): Loss: 0.7304 || Learning rate: lr=1.25e-05.
===> Epoch[379](160/324): Loss: 0.5412 || Learning rate: lr=1.25e-05.
===> Epoch[379](170/324): Loss: 1.0289 || Learning rate: lr=1.25e-05.
===> Epoch[379](180/324): Loss: 0.7077 || Learning rate: lr=1.25e-05.
===> Epoch[379](190/324): Loss: 0.7523 || Learning rate: lr=1.25e-05.
===> Epoch[379](200/324): Loss: 0.6850 || Learning rate: lr=1.25e-05.
===> Epoch[379](210/324): Loss: 0.5399 || Learning rate: lr=1.25e-05.
===> Epoch[379](220/324): Loss: 0.7533 || Learning rate: lr=1.25e-05.
===> Epoch[379](230/324): Loss: 0.5635 || Learning rate: lr=1.25e-05.
===> Epoch[379](240/324): Loss: 0.4301 || Learning rate: lr=1.25e-05.
===> Epoch[379](250/324): Loss: 0.4400 || Learning rate: lr=1.25e-05.
===> Epoch[379](260/324): Loss: 0.5453 || Learning rate: lr=1.25e-05.
===> Epoch[379](270/324): Loss: 0.5658 || Learning rate: lr=1.25e-05.
===> Epoch[379](280/324): Loss: 0.6536 || Learning rate: lr=1.25e-05.
===> Epoch[379](290/324): Loss: 0.6033 || Learning rate: lr=1.25e-05.
===> Epoch[379](300/324): Loss: 0.7200 || Learning rate: lr=1.25e-05.
===> Epoch[379](310/324): Loss: 0.6999 || Learning rate: lr=1.25e-05.
===> Epoch[379](320/324): Loss: 0.7118 || Learning rate: lr=1.25e-05.
===> Epoch[380](10/324): Loss: 0.6953 || Learning rate: lr=1.25e-05.
===> Epoch[380](20/324): Loss: 1.0913 || Learning rate: lr=1.25e-05.
===> Epoch[380](30/324): Loss: 0.5926 || Learning rate: lr=1.25e-05.
===> Epoch[380](40/324): Loss: 0.6612 || Learning rate: lr=1.25e-05.
===> Epoch[380](50/324): Loss: 0.4910 || Learning rate: lr=1.25e-05.
===> Epoch[380](60/324): Loss: 0.6072 || Learning rate: lr=1.25e-05.
===> Epoch[380](70/324): Loss: 0.5546 || Learning rate: lr=1.25e-05.
===> Epoch[380](80/324): Loss: 0.6909 || Learning rate: lr=1.25e-05.
===> Epoch[380](90/324): Loss: 0.5131 || Learning rate: lr=1.25e-05.
===> Epoch[380](100/324): Loss: 0.8908 || Learning rate: lr=1.25e-05.
===> Epoch[380](110/324): Loss: 0.7500 || Learning rate: lr=1.25e-05.
===> Epoch[380](120/324): Loss: 0.6769 || Learning rate: lr=1.25e-05.
===> Epoch[380](130/324): Loss: 0.5941 || Learning rate: lr=1.25e-05.
===> Epoch[380](140/324): Loss: 0.6357 || Learning rate: lr=1.25e-05.
===> Epoch[380](150/324): Loss: 0.6539 || Learning rate: lr=1.25e-05.
===> Epoch[380](160/324): Loss: 0.6839 || Learning rate: lr=1.25e-05.
===> Epoch[380](170/324): Loss: 0.6596 || Learning rate: lr=1.25e-05.
===> Epoch[380](180/324): Loss: 0.6485 || Learning rate: lr=1.25e-05.
===> Epoch[380](190/324): Loss: 0.7047 || Learning rate: lr=1.25e-05.
===> Epoch[380](200/324): Loss: 0.5557 || Learning rate: lr=1.25e-05.
===> Epoch[380](210/324): Loss: 0.7143 || Learning rate: lr=1.25e-05.
===> Epoch[380](220/324): Loss: 0.5074 || Learning rate: lr=1.25e-05.
===> Epoch[380](230/324): Loss: 0.6785 || Learning rate: lr=1.25e-05.
===> Epoch[380](240/324): Loss: 0.5604 || Learning rate: lr=1.25e-05.
===> Epoch[380](250/324): Loss: 0.5191 || Learning rate: lr=1.25e-05.
===> Epoch[380](260/324): Loss: 0.5584 || Learning rate: lr=1.25e-05.
===> Epoch[380](270/324): Loss: 0.8131 || Learning rate: lr=1.25e-05.
===> Epoch[380](280/324): Loss: 0.7660 || Learning rate: lr=1.25e-05.
===> Epoch[380](290/324): Loss: 0.6743 || Learning rate: lr=1.25e-05.
===> Epoch[380](300/324): Loss: 0.5840 || Learning rate: lr=1.25e-05.
===> Epoch[380](310/324): Loss: 0.4729 || Learning rate: lr=1.25e-05.
===> Epoch[380](320/324): Loss: 1.0309 || Learning rate: lr=1.25e-05.
Checkpoint saved to weights/epoch_v2_380.pth
===> Epoch[381](10/324): Loss: 0.8153 || Learning rate: lr=1.25e-05.
===> Epoch[381](20/324): Loss: 0.5128 || Learning rate: lr=1.25e-05.
===> Epoch[381](30/324): Loss: 0.5242 || Learning rate: lr=1.25e-05.
===> Epoch[381](40/324): Loss: 0.6814 || Learning rate: lr=1.25e-05.
===> Epoch[381](50/324): Loss: 0.5511 || Learning rate: lr=1.25e-05.
===> Epoch[381](60/324): Loss: 0.5788 || Learning rate: lr=1.25e-05.
===> Epoch[381](70/324): Loss: 0.8293 || Learning rate: lr=1.25e-05.
===> Epoch[381](80/324): Loss: 0.5827 || Learning rate: lr=1.25e-05.
===> Epoch[381](90/324): Loss: 1.1486 || Learning rate: lr=1.25e-05.
===> Epoch[381](100/324): Loss: 0.7453 || Learning rate: lr=1.25e-05.
===> Epoch[381](110/324): Loss: 0.5440 || Learning rate: lr=1.25e-05.
===> Epoch[381](120/324): Loss: 0.6414 || Learning rate: lr=1.25e-05.
===> Epoch[381](130/324): Loss: 0.8766 || Learning rate: lr=1.25e-05.
===> Epoch[381](140/324): Loss: 0.7356 || Learning rate: lr=1.25e-05.
===> Epoch[381](150/324): Loss: 0.8240 || Learning rate: lr=1.25e-05.
===> Epoch[381](160/324): Loss: 0.6182 || Learning rate: lr=1.25e-05.
===> Epoch[381](170/324): Loss: 0.4183 || Learning rate: lr=1.25e-05.
===> Epoch[381](180/324): Loss: 0.5336 || Learning rate: lr=1.25e-05.
===> Epoch[381](190/324): Loss: 0.5789 || Learning rate: lr=1.25e-05.
===> Epoch[381](200/324): Loss: 0.4874 || Learning rate: lr=1.25e-05.
===> Epoch[381](210/324): Loss: 0.6005 || Learning rate: lr=1.25e-05.
===> Epoch[381](220/324): Loss: 0.5055 || Learning rate: lr=1.25e-05.
===> Epoch[381](230/324): Loss: 0.3504 || Learning rate: lr=1.25e-05.
===> Epoch[381](240/324): Loss: 0.6995 || Learning rate: lr=1.25e-05.
===> Epoch[381](250/324): Loss: 0.6343 || Learning rate: lr=1.25e-05.
===> Epoch[381](260/324): Loss: 0.6280 || Learning rate: lr=1.25e-05.
===> Epoch[381](270/324): Loss: 0.6602 || Learning rate: lr=1.25e-05.
===> Epoch[381](280/324): Loss: 0.6144 || Learning rate: lr=1.25e-05.
===> Epoch[381](290/324): Loss: 0.7045 || Learning rate: lr=1.25e-05.
===> Epoch[381](300/324): Loss: 0.5499 || Learning rate: lr=1.25e-05.
===> Epoch[381](310/324): Loss: 0.6848 || Learning rate: lr=1.25e-05.
===> Epoch[381](320/324): Loss: 0.6289 || Learning rate: lr=1.25e-05.
===> Epoch[382](10/324): Loss: 0.7384 || Learning rate: lr=1.25e-05.
===> Epoch[382](20/324): Loss: 0.6828 || Learning rate: lr=1.25e-05.
===> Epoch[382](30/324): Loss: 0.5003 || Learning rate: lr=1.25e-05.
===> Epoch[382](40/324): Loss: 0.7105 || Learning rate: lr=1.25e-05.
===> Epoch[382](50/324): Loss: 0.8502 || Learning rate: lr=1.25e-05.
===> Epoch[382](60/324): Loss: 0.8691 || Learning rate: lr=1.25e-05.
===> Epoch[382](70/324): Loss: 0.7413 || Learning rate: lr=1.25e-05.
===> Epoch[382](80/324): Loss: 0.8034 || Learning rate: lr=1.25e-05.
===> Epoch[382](90/324): Loss: 0.6948 || Learning rate: lr=1.25e-05.
===> Epoch[382](100/324): Loss: 0.5995 || Learning rate: lr=1.25e-05.
===> Epoch[382](110/324): Loss: 0.7424 || Learning rate: lr=1.25e-05.
===> Epoch[382](120/324): Loss: 0.5169 || Learning rate: lr=1.25e-05.
===> Epoch[382](130/324): Loss: 0.9050 || Learning rate: lr=1.25e-05.
===> Epoch[382](140/324): Loss: 0.5059 || Learning rate: lr=1.25e-05.
===> Epoch[382](150/324): Loss: 0.4715 || Learning rate: lr=1.25e-05.
===> Epoch[382](160/324): Loss: 0.4847 || Learning rate: lr=1.25e-05.
===> Epoch[382](170/324): Loss: 0.5337 || Learning rate: lr=1.25e-05.
===> Epoch[382](180/324): Loss: 0.6291 || Learning rate: lr=1.25e-05.
===> Epoch[382](190/324): Loss: 0.4885 || Learning rate: lr=1.25e-05.
===> Epoch[382](200/324): Loss: 0.6612 || Learning rate: lr=1.25e-05.
===> Epoch[382](210/324): Loss: 0.6193 || Learning rate: lr=1.25e-05.
===> Epoch[382](220/324): Loss: 0.7127 || Learning rate: lr=1.25e-05.
===> Epoch[382](230/324): Loss: 0.5982 || Learning rate: lr=1.25e-05.
===> Epoch[382](240/324): Loss: 0.5409 || Learning rate: lr=1.25e-05.
===> Epoch[382](250/324): Loss: 0.5776 || Learning rate: lr=1.25e-05.
===> Epoch[382](260/324): Loss: 0.4364 || Learning rate: lr=1.25e-05.
===> Epoch[382](270/324): Loss: 0.5569 || Learning rate: lr=1.25e-05.
===> Epoch[382](280/324): Loss: 0.7054 || Learning rate: lr=1.25e-05.
===> Epoch[382](290/324): Loss: 0.6264 || Learning rate: lr=1.25e-05.
===> Epoch[382](300/324): Loss: 0.8190 || Learning rate: lr=1.25e-05.
===> Epoch[382](310/324): Loss: 0.6719 || Learning rate: lr=1.25e-05.
===> Epoch[382](320/324): Loss: 0.5785 || Learning rate: lr=1.25e-05.
===> Epoch[383](10/324): Loss: 0.5509 || Learning rate: lr=1.25e-05.
===> Epoch[383](20/324): Loss: 0.6381 || Learning rate: lr=1.25e-05.
===> Epoch[383](30/324): Loss: 0.7320 || Learning rate: lr=1.25e-05.
===> Epoch[383](40/324): Loss: 0.6953 || Learning rate: lr=1.25e-05.
===> Epoch[383](50/324): Loss: 0.6205 || Learning rate: lr=1.25e-05.
===> Epoch[383](60/324): Loss: 0.7131 || Learning rate: lr=1.25e-05.
===> Epoch[383](70/324): Loss: 0.5756 || Learning rate: lr=1.25e-05.
===> Epoch[383](80/324): Loss: 0.5266 || Learning rate: lr=1.25e-05.
===> Epoch[383](90/324): Loss: 0.8276 || Learning rate: lr=1.25e-05.
===> Epoch[383](100/324): Loss: 0.7711 || Learning rate: lr=1.25e-05.
===> Epoch[383](110/324): Loss: 0.6191 || Learning rate: lr=1.25e-05.
===> Epoch[383](120/324): Loss: 0.6287 || Learning rate: lr=1.25e-05.
===> Epoch[383](130/324): Loss: 0.4796 || Learning rate: lr=1.25e-05.
===> Epoch[383](140/324): Loss: 0.4285 || Learning rate: lr=1.25e-05.
===> Epoch[383](150/324): Loss: 0.4810 || Learning rate: lr=1.25e-05.
===> Epoch[383](160/324): Loss: 0.7574 || Learning rate: lr=1.25e-05.
===> Epoch[383](170/324): Loss: 0.7709 || Learning rate: lr=1.25e-05.
===> Epoch[383](180/324): Loss: 0.9959 || Learning rate: lr=1.25e-05.
===> Epoch[383](190/324): Loss: 0.7623 || Learning rate: lr=1.25e-05.
===> Epoch[383](200/324): Loss: 0.6200 || Learning rate: lr=1.25e-05.
===> Epoch[383](210/324): Loss: 0.5973 || Learning rate: lr=1.25e-05.
===> Epoch[383](220/324): Loss: 0.7784 || Learning rate: lr=1.25e-05.
===> Epoch[383](230/324): Loss: 0.6413 || Learning rate: lr=1.25e-05.
===> Epoch[383](240/324): Loss: 0.6302 || Learning rate: lr=1.25e-05.
===> Epoch[383](250/324): Loss: 1.0206 || Learning rate: lr=1.25e-05.
===> Epoch[383](260/324): Loss: 0.4800 || Learning rate: lr=1.25e-05.
===> Epoch[383](270/324): Loss: 0.7025 || Learning rate: lr=1.25e-05.
===> Epoch[383](280/324): Loss: 0.5191 || Learning rate: lr=1.25e-05.
===> Epoch[383](290/324): Loss: 0.5746 || Learning rate: lr=1.25e-05.
===> Epoch[383](300/324): Loss: 0.9792 || Learning rate: lr=1.25e-05.
===> Epoch[383](310/324): Loss: 0.5720 || Learning rate: lr=1.25e-05.
===> Epoch[383](320/324): Loss: 0.5523 || Learning rate: lr=1.25e-05.
===> Epoch[384](10/324): Loss: 0.6805 || Learning rate: lr=1.25e-05.
===> Epoch[384](20/324): Loss: 0.5573 || Learning rate: lr=1.25e-05.
===> Epoch[384](30/324): Loss: 0.9623 || Learning rate: lr=1.25e-05.
===> Epoch[384](40/324): Loss: 0.7305 || Learning rate: lr=1.25e-05.
===> Epoch[384](50/324): Loss: 0.8442 || Learning rate: lr=1.25e-05.
===> Epoch[384](60/324): Loss: 0.5607 || Learning rate: lr=1.25e-05.
===> Epoch[384](70/324): Loss: 0.6144 || Learning rate: lr=1.25e-05.
===> Epoch[384](80/324): Loss: 0.7831 || Learning rate: lr=1.25e-05.
===> Epoch[384](90/324): Loss: 0.6165 || Learning rate: lr=1.25e-05.
===> Epoch[384](100/324): Loss: 0.4778 || Learning rate: lr=1.25e-05.
===> Epoch[384](110/324): Loss: 0.6207 || Learning rate: lr=1.25e-05.
===> Epoch[384](120/324): Loss: 0.5626 || Learning rate: lr=1.25e-05.
===> Epoch[384](130/324): Loss: 0.5273 || Learning rate: lr=1.25e-05.
===> Epoch[384](140/324): Loss: 0.6965 || Learning rate: lr=1.25e-05.
===> Epoch[384](150/324): Loss: 0.3244 || Learning rate: lr=1.25e-05.
===> Epoch[384](160/324): Loss: 0.5669 || Learning rate: lr=1.25e-05.
===> Epoch[384](170/324): Loss: 0.5002 || Learning rate: lr=1.25e-05.
===> Epoch[384](180/324): Loss: 1.0039 || Learning rate: lr=1.25e-05.
===> Epoch[384](190/324): Loss: 0.6198 || Learning rate: lr=1.25e-05.
===> Epoch[384](200/324): Loss: 0.5515 || Learning rate: lr=1.25e-05.
===> Epoch[384](210/324): Loss: 0.4164 || Learning rate: lr=1.25e-05.
===> Epoch[384](220/324): Loss: 0.5168 || Learning rate: lr=1.25e-05.
===> Epoch[384](230/324): Loss: 0.6567 || Learning rate: lr=1.25e-05.
===> Epoch[384](240/324): Loss: 0.5663 || Learning rate: lr=1.25e-05.
===> Epoch[384](250/324): Loss: 0.6533 || Learning rate: lr=1.25e-05.
===> Epoch[384](260/324): Loss: 0.7445 || Learning rate: lr=1.25e-05.
===> Epoch[384](270/324): Loss: 0.5659 || Learning rate: lr=1.25e-05.
===> Epoch[384](280/324): Loss: 0.6260 || Learning rate: lr=1.25e-05.
===> Epoch[384](290/324): Loss: 0.6285 || Learning rate: lr=1.25e-05.
===> Epoch[384](300/324): Loss: 0.8165 || Learning rate: lr=1.25e-05.
===> Epoch[384](310/324): Loss: 1.1564 || Learning rate: lr=1.25e-05.
===> Epoch[384](320/324): Loss: 0.5276 || Learning rate: lr=1.25e-05.
===> Epoch[385](10/324): Loss: 0.6402 || Learning rate: lr=1.25e-05.
===> Epoch[385](20/324): Loss: 0.6918 || Learning rate: lr=1.25e-05.
===> Epoch[385](30/324): Loss: 0.6716 || Learning rate: lr=1.25e-05.
===> Epoch[385](40/324): Loss: 0.8300 || Learning rate: lr=1.25e-05.
===> Epoch[385](50/324): Loss: 0.7463 || Learning rate: lr=1.25e-05.
===> Epoch[385](60/324): Loss: 0.6445 || Learning rate: lr=1.25e-05.
===> Epoch[385](70/324): Loss: 0.6619 || Learning rate: lr=1.25e-05.
===> Epoch[385](80/324): Loss: 0.6044 || Learning rate: lr=1.25e-05.
===> Epoch[385](90/324): Loss: 0.6864 || Learning rate: lr=1.25e-05.
===> Epoch[385](100/324): Loss: 0.5923 || Learning rate: lr=1.25e-05.
===> Epoch[385](110/324): Loss: 0.6721 || Learning rate: lr=1.25e-05.
===> Epoch[385](120/324): Loss: 0.6477 || Learning rate: lr=1.25e-05.
===> Epoch[385](130/324): Loss: 0.5514 || Learning rate: lr=1.25e-05.
===> Epoch[385](140/324): Loss: 0.4650 || Learning rate: lr=1.25e-05.
===> Epoch[385](150/324): Loss: 0.6513 || Learning rate: lr=1.25e-05.
===> Epoch[385](160/324): Loss: 0.7261 || Learning rate: lr=1.25e-05.
===> Epoch[385](170/324): Loss: 0.5385 || Learning rate: lr=1.25e-05.
===> Epoch[385](180/324): Loss: 0.6009 || Learning rate: lr=1.25e-05.
===> Epoch[385](190/324): Loss: 0.4408 || Learning rate: lr=1.25e-05.
===> Epoch[385](200/324): Loss: 0.6252 || Learning rate: lr=1.25e-05.
===> Epoch[385](210/324): Loss: 0.7235 || Learning rate: lr=1.25e-05.
===> Epoch[385](220/324): Loss: 0.4710 || Learning rate: lr=1.25e-05.
===> Epoch[385](230/324): Loss: 0.5786 || Learning rate: lr=1.25e-05.
===> Epoch[385](240/324): Loss: 0.6620 || Learning rate: lr=1.25e-05.
===> Epoch[385](250/324): Loss: 0.6310 || Learning rate: lr=1.25e-05.
===> Epoch[385](260/324): Loss: 0.6293 || Learning rate: lr=1.25e-05.
===> Epoch[385](270/324): Loss: 0.6534 || Learning rate: lr=1.25e-05.
===> Epoch[385](280/324): Loss: 0.5405 || Learning rate: lr=1.25e-05.
===> Epoch[385](290/324): Loss: 0.5495 || Learning rate: lr=1.25e-05.
===> Epoch[385](300/324): Loss: 0.6634 || Learning rate: lr=1.25e-05.
===> Epoch[385](310/324): Loss: 0.8053 || Learning rate: lr=1.25e-05.
===> Epoch[385](320/324): Loss: 0.5338 || Learning rate: lr=1.25e-05.
===> Epoch[386](10/324): Loss: 0.5120 || Learning rate: lr=1.25e-05.
===> Epoch[386](20/324): Loss: 0.6287 || Learning rate: lr=1.25e-05.
===> Epoch[386](30/324): Loss: 0.7271 || Learning rate: lr=1.25e-05.
===> Epoch[386](40/324): Loss: 0.9473 || Learning rate: lr=1.25e-05.
===> Epoch[386](50/324): Loss: 0.6256 || Learning rate: lr=1.25e-05.
===> Epoch[386](60/324): Loss: 0.6683 || Learning rate: lr=1.25e-05.
===> Epoch[386](70/324): Loss: 0.7321 || Learning rate: lr=1.25e-05.
===> Epoch[386](80/324): Loss: 0.5101 || Learning rate: lr=1.25e-05.
===> Epoch[386](90/324): Loss: 0.5779 || Learning rate: lr=1.25e-05.
===> Epoch[386](100/324): Loss: 0.8520 || Learning rate: lr=1.25e-05.
===> Epoch[386](110/324): Loss: 0.6108 || Learning rate: lr=1.25e-05.
===> Epoch[386](120/324): Loss: 0.7669 || Learning rate: lr=1.25e-05.
===> Epoch[386](130/324): Loss: 0.6006 || Learning rate: lr=1.25e-05.
===> Epoch[386](140/324): Loss: 0.7617 || Learning rate: lr=1.25e-05.
===> Epoch[386](150/324): Loss: 0.4240 || Learning rate: lr=1.25e-05.
===> Epoch[386](160/324): Loss: 0.7014 || Learning rate: lr=1.25e-05.
===> Epoch[386](170/324): Loss: 0.5768 || Learning rate: lr=1.25e-05.
===> Epoch[386](180/324): Loss: 0.7573 || Learning rate: lr=1.25e-05.
===> Epoch[386](190/324): Loss: 0.7729 || Learning rate: lr=1.25e-05.
===> Epoch[386](200/324): Loss: 0.9579 || Learning rate: lr=1.25e-05.
===> Epoch[386](210/324): Loss: 0.6546 || Learning rate: lr=1.25e-05.
===> Epoch[386](220/324): Loss: 0.7316 || Learning rate: lr=1.25e-05.
===> Epoch[386](230/324): Loss: 1.1540 || Learning rate: lr=1.25e-05.
===> Epoch[386](240/324): Loss: 0.6764 || Learning rate: lr=1.25e-05.
===> Epoch[386](250/324): Loss: 0.6896 || Learning rate: lr=1.25e-05.
===> Epoch[386](260/324): Loss: 0.4054 || Learning rate: lr=1.25e-05.
===> Epoch[386](270/324): Loss: 0.4605 || Learning rate: lr=1.25e-05.
===> Epoch[386](280/324): Loss: 0.5044 || Learning rate: lr=1.25e-05.
===> Epoch[386](290/324): Loss: 0.6922 || Learning rate: lr=1.25e-05.
===> Epoch[386](300/324): Loss: 0.6271 || Learning rate: lr=1.25e-05.
===> Epoch[386](310/324): Loss: 0.7971 || Learning rate: lr=1.25e-05.
===> Epoch[386](320/324): Loss: 0.6340 || Learning rate: lr=1.25e-05.
===> Epoch[387](10/324): Loss: 0.5494 || Learning rate: lr=1.25e-05.
===> Epoch[387](20/324): Loss: 0.3653 || Learning rate: lr=1.25e-05.
===> Epoch[387](30/324): Loss: 0.4995 || Learning rate: lr=1.25e-05.
===> Epoch[387](40/324): Loss: 0.8023 || Learning rate: lr=1.25e-05.
===> Epoch[387](50/324): Loss: 0.5263 || Learning rate: lr=1.25e-05.
===> Epoch[387](60/324): Loss: 0.4878 || Learning rate: lr=1.25e-05.
===> Epoch[387](70/324): Loss: 0.6922 || Learning rate: lr=1.25e-05.
===> Epoch[387](80/324): Loss: 0.8256 || Learning rate: lr=1.25e-05.
===> Epoch[387](90/324): Loss: 0.5275 || Learning rate: lr=1.25e-05.
===> Epoch[387](100/324): Loss: 0.6383 || Learning rate: lr=1.25e-05.
===> Epoch[387](110/324): Loss: 0.8331 || Learning rate: lr=1.25e-05.
===> Epoch[387](120/324): Loss: 0.6144 || Learning rate: lr=1.25e-05.
===> Epoch[387](130/324): Loss: 0.5201 || Learning rate: lr=1.25e-05.
===> Epoch[387](140/324): Loss: 0.6280 || Learning rate: lr=1.25e-05.
===> Epoch[387](150/324): Loss: 0.7659 || Learning rate: lr=1.25e-05.
===> Epoch[387](160/324): Loss: 0.6262 || Learning rate: lr=1.25e-05.
===> Epoch[387](170/324): Loss: 0.5808 || Learning rate: lr=1.25e-05.
===> Epoch[387](180/324): Loss: 0.6465 || Learning rate: lr=1.25e-05.
===> Epoch[387](190/324): Loss: 0.6437 || Learning rate: lr=1.25e-05.
===> Epoch[387](200/324): Loss: 0.6814 || Learning rate: lr=1.25e-05.
===> Epoch[387](210/324): Loss: 0.7571 || Learning rate: lr=1.25e-05.
===> Epoch[387](220/324): Loss: 0.9222 || Learning rate: lr=1.25e-05.
===> Epoch[387](230/324): Loss: 0.4358 || Learning rate: lr=1.25e-05.
===> Epoch[387](240/324): Loss: 0.6888 || Learning rate: lr=1.25e-05.
===> Epoch[387](250/324): Loss: 0.7890 || Learning rate: lr=1.25e-05.
===> Epoch[387](260/324): Loss: 0.7983 || Learning rate: lr=1.25e-05.
===> Epoch[387](270/324): Loss: 0.6302 || Learning rate: lr=1.25e-05.
===> Epoch[387](280/324): Loss: 0.6808 || Learning rate: lr=1.25e-05.
===> Epoch[387](290/324): Loss: 0.4833 || Learning rate: lr=1.25e-05.
===> Epoch[387](300/324): Loss: 0.6446 || Learning rate: lr=1.25e-05.
===> Epoch[387](310/324): Loss: 0.8412 || Learning rate: lr=1.25e-05.
===> Epoch[387](320/324): Loss: 0.6428 || Learning rate: lr=1.25e-05.
===> Epoch[388](10/324): Loss: 0.7432 || Learning rate: lr=1.25e-05.
===> Epoch[388](20/324): Loss: 0.6560 || Learning rate: lr=1.25e-05.
===> Epoch[388](30/324): Loss: 0.6553 || Learning rate: lr=1.25e-05.
===> Epoch[388](40/324): Loss: 0.5839 || Learning rate: lr=1.25e-05.
===> Epoch[388](50/324): Loss: 0.7988 || Learning rate: lr=1.25e-05.
===> Epoch[388](60/324): Loss: 0.3794 || Learning rate: lr=1.25e-05.
===> Epoch[388](70/324): Loss: 0.4820 || Learning rate: lr=1.25e-05.
===> Epoch[388](80/324): Loss: 0.6307 || Learning rate: lr=1.25e-05.
===> Epoch[388](90/324): Loss: 0.7134 || Learning rate: lr=1.25e-05.
===> Epoch[388](100/324): Loss: 0.6720 || Learning rate: lr=1.25e-05.
===> Epoch[388](110/324): Loss: 0.5881 || Learning rate: lr=1.25e-05.
===> Epoch[388](120/324): Loss: 0.5831 || Learning rate: lr=1.25e-05.
===> Epoch[388](130/324): Loss: 0.6441 || Learning rate: lr=1.25e-05.
===> Epoch[388](140/324): Loss: 0.5391 || Learning rate: lr=1.25e-05.
===> Epoch[388](150/324): Loss: 0.3983 || Learning rate: lr=1.25e-05.
===> Epoch[388](160/324): Loss: 0.6382 || Learning rate: lr=1.25e-05.
===> Epoch[388](170/324): Loss: 0.6011 || Learning rate: lr=1.25e-05.
===> Epoch[388](180/324): Loss: 0.6001 || Learning rate: lr=1.25e-05.
===> Epoch[388](190/324): Loss: 0.8015 || Learning rate: lr=1.25e-05.
===> Epoch[388](200/324): Loss: 0.5753 || Learning rate: lr=1.25e-05.
===> Epoch[388](210/324): Loss: 0.6778 || Learning rate: lr=1.25e-05.
===> Epoch[388](220/324): Loss: 0.7329 || Learning rate: lr=1.25e-05.
===> Epoch[388](230/324): Loss: 0.6259 || Learning rate: lr=1.25e-05.
===> Epoch[388](240/324): Loss: 0.5615 || Learning rate: lr=1.25e-05.
===> Epoch[388](250/324): Loss: 0.7357 || Learning rate: lr=1.25e-05.
===> Epoch[388](260/324): Loss: 0.7394 || Learning rate: lr=1.25e-05.
===> Epoch[388](270/324): Loss: 0.6673 || Learning rate: lr=1.25e-05.
===> Epoch[388](280/324): Loss: 0.5010 || Learning rate: lr=1.25e-05.
===> Epoch[388](290/324): Loss: 0.5937 || Learning rate: lr=1.25e-05.
===> Epoch[388](300/324): Loss: 0.5865 || Learning rate: lr=1.25e-05.
===> Epoch[388](310/324): Loss: 0.6723 || Learning rate: lr=1.25e-05.
===> Epoch[388](320/324): Loss: 1.1418 || Learning rate: lr=1.25e-05.
===> Epoch[389](10/324): Loss: 0.5842 || Learning rate: lr=1.25e-05.
===> Epoch[389](20/324): Loss: 0.4412 || Learning rate: lr=1.25e-05.
===> Epoch[389](30/324): Loss: 0.4819 || Learning rate: lr=1.25e-05.
===> Epoch[389](40/324): Loss: 0.8262 || Learning rate: lr=1.25e-05.
===> Epoch[389](50/324): Loss: 0.6776 || Learning rate: lr=1.25e-05.
===> Epoch[389](60/324): Loss: 0.9033 || Learning rate: lr=1.25e-05.
===> Epoch[389](70/324): Loss: 0.5535 || Learning rate: lr=1.25e-05.
===> Epoch[389](80/324): Loss: 0.7223 || Learning rate: lr=1.25e-05.
===> Epoch[389](90/324): Loss: 0.7443 || Learning rate: lr=1.25e-05.
===> Epoch[389](100/324): Loss: 0.5783 || Learning rate: lr=1.25e-05.
===> Epoch[389](110/324): Loss: 0.7599 || Learning rate: lr=1.25e-05.
===> Epoch[389](120/324): Loss: 0.5726 || Learning rate: lr=1.25e-05.
===> Epoch[389](130/324): Loss: 0.6600 || Learning rate: lr=1.25e-05.
===> Epoch[389](140/324): Loss: 0.6475 || Learning rate: lr=1.25e-05.
===> Epoch[389](150/324): Loss: 0.5208 || Learning rate: lr=1.25e-05.
===> Epoch[389](160/324): Loss: 0.7144 || Learning rate: lr=1.25e-05.
===> Epoch[389](170/324): Loss: 0.6758 || Learning rate: lr=1.25e-05.
===> Epoch[389](180/324): Loss: 0.3985 || Learning rate: lr=1.25e-05.
===> Epoch[389](190/324): Loss: 0.4649 || Learning rate: lr=1.25e-05.
===> Epoch[389](200/324): Loss: 0.7086 || Learning rate: lr=1.25e-05.
===> Epoch[389](210/324): Loss: 0.8093 || Learning rate: lr=1.25e-05.
===> Epoch[389](220/324): Loss: 0.4812 || Learning rate: lr=1.25e-05.
===> Epoch[389](230/324): Loss: 0.7023 || Learning rate: lr=1.25e-05.
===> Epoch[389](240/324): Loss: 0.9254 || Learning rate: lr=1.25e-05.
===> Epoch[389](250/324): Loss: 0.4596 || Learning rate: lr=1.25e-05.
===> Epoch[389](260/324): Loss: 1.1086 || Learning rate: lr=1.25e-05.
===> Epoch[389](270/324): Loss: 0.8375 || Learning rate: lr=1.25e-05.
===> Epoch[389](280/324): Loss: 0.6220 || Learning rate: lr=1.25e-05.
===> Epoch[389](290/324): Loss: 0.7904 || Learning rate: lr=1.25e-05.
===> Epoch[389](300/324): Loss: 0.6592 || Learning rate: lr=1.25e-05.
===> Epoch[389](310/324): Loss: 0.9990 || Learning rate: lr=1.25e-05.
===> Epoch[389](320/324): Loss: 0.6116 || Learning rate: lr=1.25e-05.
===> Epoch[390](10/324): Loss: 0.4133 || Learning rate: lr=1.25e-05.
===> Epoch[390](20/324): Loss: 0.4689 || Learning rate: lr=1.25e-05.
===> Epoch[390](30/324): Loss: 0.9468 || Learning rate: lr=1.25e-05.
===> Epoch[390](40/324): Loss: 0.5354 || Learning rate: lr=1.25e-05.
===> Epoch[390](50/324): Loss: 0.9518 || Learning rate: lr=1.25e-05.
===> Epoch[390](60/324): Loss: 0.5616 || Learning rate: lr=1.25e-05.
===> Epoch[390](70/324): Loss: 0.6562 || Learning rate: lr=1.25e-05.
===> Epoch[390](80/324): Loss: 0.6132 || Learning rate: lr=1.25e-05.
===> Epoch[390](90/324): Loss: 0.5385 || Learning rate: lr=1.25e-05.
===> Epoch[390](100/324): Loss: 0.5934 || Learning rate: lr=1.25e-05.
===> Epoch[390](110/324): Loss: 0.6610 || Learning rate: lr=1.25e-05.
===> Epoch[390](120/324): Loss: 0.5194 || Learning rate: lr=1.25e-05.
===> Epoch[390](130/324): Loss: 0.7531 || Learning rate: lr=1.25e-05.
===> Epoch[390](140/324): Loss: 0.5604 || Learning rate: lr=1.25e-05.
===> Epoch[390](150/324): Loss: 0.6233 || Learning rate: lr=1.25e-05.
===> Epoch[390](160/324): Loss: 0.7112 || Learning rate: lr=1.25e-05.
===> Epoch[390](170/324): Loss: 0.7217 || Learning rate: lr=1.25e-05.
===> Epoch[390](180/324): Loss: 0.4425 || Learning rate: lr=1.25e-05.
===> Epoch[390](190/324): Loss: 0.7612 || Learning rate: lr=1.25e-05.
===> Epoch[390](200/324): Loss: 0.5185 || Learning rate: lr=1.25e-05.
===> Epoch[390](210/324): Loss: 0.6353 || Learning rate: lr=1.25e-05.
===> Epoch[390](220/324): Loss: 0.6226 || Learning rate: lr=1.25e-05.
===> Epoch[390](230/324): Loss: 0.9293 || Learning rate: lr=1.25e-05.
===> Epoch[390](240/324): Loss: 0.8931 || Learning rate: lr=1.25e-05.
===> Epoch[390](250/324): Loss: 0.7759 || Learning rate: lr=1.25e-05.
===> Epoch[390](260/324): Loss: 0.6288 || Learning rate: lr=1.25e-05.
===> Epoch[390](270/324): Loss: 0.5519 || Learning rate: lr=1.25e-05.
===> Epoch[390](280/324): Loss: 0.5424 || Learning rate: lr=1.25e-05.
===> Epoch[390](290/324): Loss: 0.6823 || Learning rate: lr=1.25e-05.
===> Epoch[390](300/324): Loss: 0.5281 || Learning rate: lr=1.25e-05.
===> Epoch[390](310/324): Loss: 0.5698 || Learning rate: lr=1.25e-05.
===> Epoch[390](320/324): Loss: 0.7393 || Learning rate: lr=1.25e-05.
===> Epoch[391](10/324): Loss: 0.6718 || Learning rate: lr=1.25e-05.
===> Epoch[391](20/324): Loss: 0.5896 || Learning rate: lr=1.25e-05.
===> Epoch[391](30/324): Loss: 0.6268 || Learning rate: lr=1.25e-05.
===> Epoch[391](40/324): Loss: 0.8008 || Learning rate: lr=1.25e-05.
===> Epoch[391](50/324): Loss: 0.6916 || Learning rate: lr=1.25e-05.
===> Epoch[391](60/324): Loss: 0.7437 || Learning rate: lr=1.25e-05.
===> Epoch[391](70/324): Loss: 0.5445 || Learning rate: lr=1.25e-05.
===> Epoch[391](80/324): Loss: 0.6132 || Learning rate: lr=1.25e-05.
===> Epoch[391](90/324): Loss: 0.5421 || Learning rate: lr=1.25e-05.
===> Epoch[391](100/324): Loss: 0.6211 || Learning rate: lr=1.25e-05.
===> Epoch[391](110/324): Loss: 0.6151 || Learning rate: lr=1.25e-05.
===> Epoch[391](120/324): Loss: 0.6529 || Learning rate: lr=1.25e-05.
===> Epoch[391](130/324): Loss: 0.8175 || Learning rate: lr=1.25e-05.
===> Epoch[391](140/324): Loss: 0.6922 || Learning rate: lr=1.25e-05.
===> Epoch[391](150/324): Loss: 0.7394 || Learning rate: lr=1.25e-05.
===> Epoch[391](160/324): Loss: 0.6365 || Learning rate: lr=1.25e-05.
===> Epoch[391](170/324): Loss: 1.0189 || Learning rate: lr=1.25e-05.
===> Epoch[391](180/324): Loss: 0.4351 || Learning rate: lr=1.25e-05.
===> Epoch[391](190/324): Loss: 0.5903 || Learning rate: lr=1.25e-05.
===> Epoch[391](200/324): Loss: 0.8206 || Learning rate: lr=1.25e-05.
===> Epoch[391](210/324): Loss: 0.5217 || Learning rate: lr=1.25e-05.
===> Epoch[391](220/324): Loss: 0.6304 || Learning rate: lr=1.25e-05.
===> Epoch[391](230/324): Loss: 0.5969 || Learning rate: lr=1.25e-05.
===> Epoch[391](240/324): Loss: 0.4371 || Learning rate: lr=1.25e-05.
===> Epoch[391](250/324): Loss: 0.4793 || Learning rate: lr=1.25e-05.
===> Epoch[391](260/324): Loss: 0.6829 || Learning rate: lr=1.25e-05.
===> Epoch[391](270/324): Loss: 1.1970 || Learning rate: lr=1.25e-05.
===> Epoch[391](280/324): Loss: 0.6289 || Learning rate: lr=1.25e-05.
===> Epoch[391](290/324): Loss: 0.5235 || Learning rate: lr=1.25e-05.
===> Epoch[391](300/324): Loss: 0.6067 || Learning rate: lr=1.25e-05.
===> Epoch[391](310/324): Loss: 0.7825 || Learning rate: lr=1.25e-05.
===> Epoch[391](320/324): Loss: 0.5454 || Learning rate: lr=1.25e-05.
===> Epoch[392](10/324): Loss: 0.4152 || Learning rate: lr=1.25e-05.
===> Epoch[392](20/324): Loss: 0.9506 || Learning rate: lr=1.25e-05.
===> Epoch[392](30/324): Loss: 0.4645 || Learning rate: lr=1.25e-05.
===> Epoch[392](40/324): Loss: 0.4841 || Learning rate: lr=1.25e-05.
===> Epoch[392](50/324): Loss: 0.6967 || Learning rate: lr=1.25e-05.
===> Epoch[392](60/324): Loss: 0.5702 || Learning rate: lr=1.25e-05.
===> Epoch[392](70/324): Loss: 0.4485 || Learning rate: lr=1.25e-05.
===> Epoch[392](80/324): Loss: 0.8609 || Learning rate: lr=1.25e-05.
===> Epoch[392](90/324): Loss: 0.7088 || Learning rate: lr=1.25e-05.
===> Epoch[392](100/324): Loss: 0.7641 || Learning rate: lr=1.25e-05.
===> Epoch[392](110/324): Loss: 0.7201 || Learning rate: lr=1.25e-05.
===> Epoch[392](120/324): Loss: 0.8772 || Learning rate: lr=1.25e-05.
===> Epoch[392](130/324): Loss: 0.6054 || Learning rate: lr=1.25e-05.
===> Epoch[392](140/324): Loss: 0.5804 || Learning rate: lr=1.25e-05.
===> Epoch[392](150/324): Loss: 0.5267 || Learning rate: lr=1.25e-05.
===> Epoch[392](160/324): Loss: 0.5940 || Learning rate: lr=1.25e-05.
===> Epoch[392](170/324): Loss: 0.5583 || Learning rate: lr=1.25e-05.
===> Epoch[392](180/324): Loss: 0.5481 || Learning rate: lr=1.25e-05.
===> Epoch[392](190/324): Loss: 0.8849 || Learning rate: lr=1.25e-05.
===> Epoch[392](200/324): Loss: 0.4439 || Learning rate: lr=1.25e-05.
===> Epoch[392](210/324): Loss: 0.4990 || Learning rate: lr=1.25e-05.
===> Epoch[392](220/324): Loss: 0.7357 || Learning rate: lr=1.25e-05.
===> Epoch[392](230/324): Loss: 0.7947 || Learning rate: lr=1.25e-05.
===> Epoch[392](240/324): Loss: 0.5935 || Learning rate: lr=1.25e-05.
===> Epoch[392](250/324): Loss: 0.7026 || Learning rate: lr=1.25e-05.
===> Epoch[392](260/324): Loss: 0.5925 || Learning rate: lr=1.25e-05.
===> Epoch[392](270/324): Loss: 0.7477 || Learning rate: lr=1.25e-05.
===> Epoch[392](280/324): Loss: 0.6933 || Learning rate: lr=1.25e-05.
===> Epoch[392](290/324): Loss: 0.6984 || Learning rate: lr=1.25e-05.
===> Epoch[392](300/324): Loss: 0.7155 || Learning rate: lr=1.25e-05.
===> Epoch[392](310/324): Loss: 0.3818 || Learning rate: lr=1.25e-05.
===> Epoch[392](320/324): Loss: 0.5970 || Learning rate: lr=1.25e-05.
===> Epoch[393](10/324): Loss: 0.9447 || Learning rate: lr=1.25e-05.
===> Epoch[393](20/324): Loss: 0.5751 || Learning rate: lr=1.25e-05.
===> Epoch[393](30/324): Loss: 0.5802 || Learning rate: lr=1.25e-05.
===> Epoch[393](40/324): Loss: 0.5708 || Learning rate: lr=1.25e-05.
===> Epoch[393](50/324): Loss: 0.7023 || Learning rate: lr=1.25e-05.
===> Epoch[393](60/324): Loss: 0.5521 || Learning rate: lr=1.25e-05.
===> Epoch[393](70/324): Loss: 0.6993 || Learning rate: lr=1.25e-05.
===> Epoch[393](80/324): Loss: 0.5270 || Learning rate: lr=1.25e-05.
===> Epoch[393](90/324): Loss: 0.7669 || Learning rate: lr=1.25e-05.
===> Epoch[393](100/324): Loss: 0.4796 || Learning rate: lr=1.25e-05.
===> Epoch[393](110/324): Loss: 0.6732 || Learning rate: lr=1.25e-05.
===> Epoch[393](120/324): Loss: 0.5855 || Learning rate: lr=1.25e-05.
===> Epoch[393](130/324): Loss: 0.6061 || Learning rate: lr=1.25e-05.
===> Epoch[393](140/324): Loss: 0.6246 || Learning rate: lr=1.25e-05.
===> Epoch[393](150/324): Loss: 0.5383 || Learning rate: lr=1.25e-05.
===> Epoch[393](160/324): Loss: 0.7553 || Learning rate: lr=1.25e-05.
===> Epoch[393](170/324): Loss: 0.6058 || Learning rate: lr=1.25e-05.
===> Epoch[393](180/324): Loss: 0.5545 || Learning rate: lr=1.25e-05.
===> Epoch[393](190/324): Loss: 0.6178 || Learning rate: lr=1.25e-05.
===> Epoch[393](200/324): Loss: 0.9451 || Learning rate: lr=1.25e-05.
===> Epoch[393](210/324): Loss: 0.7635 || Learning rate: lr=1.25e-05.
===> Epoch[393](220/324): Loss: 0.8909 || Learning rate: lr=1.25e-05.
===> Epoch[393](230/324): Loss: 0.7579 || Learning rate: lr=1.25e-05.
===> Epoch[393](240/324): Loss: 0.4339 || Learning rate: lr=1.25e-05.
===> Epoch[393](250/324): Loss: 0.7165 || Learning rate: lr=1.25e-05.
===> Epoch[393](260/324): Loss: 0.6390 || Learning rate: lr=1.25e-05.
===> Epoch[393](270/324): Loss: 0.5877 || Learning rate: lr=1.25e-05.
===> Epoch[393](280/324): Loss: 0.7057 || Learning rate: lr=1.25e-05.
===> Epoch[393](290/324): Loss: 0.6426 || Learning rate: lr=1.25e-05.
===> Epoch[393](300/324): Loss: 0.7027 || Learning rate: lr=1.25e-05.
===> Epoch[393](310/324): Loss: 0.5521 || Learning rate: lr=1.25e-05.
===> Epoch[393](320/324): Loss: 0.6758 || Learning rate: lr=1.25e-05.
===> Epoch[394](10/324): Loss: 0.4865 || Learning rate: lr=1.25e-05.
===> Epoch[394](20/324): Loss: 0.5757 || Learning rate: lr=1.25e-05.
===> Epoch[394](30/324): Loss: 0.4015 || Learning rate: lr=1.25e-05.
===> Epoch[394](40/324): Loss: 0.7154 || Learning rate: lr=1.25e-05.
===> Epoch[394](50/324): Loss: 0.5578 || Learning rate: lr=1.25e-05.
===> Epoch[394](60/324): Loss: 0.7230 || Learning rate: lr=1.25e-05.
===> Epoch[394](70/324): Loss: 0.6466 || Learning rate: lr=1.25e-05.
===> Epoch[394](80/324): Loss: 0.6394 || Learning rate: lr=1.25e-05.
===> Epoch[394](90/324): Loss: 0.7505 || Learning rate: lr=1.25e-05.
===> Epoch[394](100/324): Loss: 0.7333 || Learning rate: lr=1.25e-05.
===> Epoch[394](110/324): Loss: 0.5770 || Learning rate: lr=1.25e-05.
===> Epoch[394](120/324): Loss: 0.5512 || Learning rate: lr=1.25e-05.
===> Epoch[394](130/324): Loss: 0.5619 || Learning rate: lr=1.25e-05.
===> Epoch[394](140/324): Loss: 0.6452 || Learning rate: lr=1.25e-05.
===> Epoch[394](150/324): Loss: 0.4001 || Learning rate: lr=1.25e-05.
===> Epoch[394](160/324): Loss: 0.6586 || Learning rate: lr=1.25e-05.
===> Epoch[394](170/324): Loss: 0.6979 || Learning rate: lr=1.25e-05.
===> Epoch[394](180/324): Loss: 0.8182 || Learning rate: lr=1.25e-05.
===> Epoch[394](190/324): Loss: 0.7058 || Learning rate: lr=1.25e-05.
===> Epoch[394](200/324): Loss: 0.5211 || Learning rate: lr=1.25e-05.
===> Epoch[394](210/324): Loss: 0.7786 || Learning rate: lr=1.25e-05.
===> Epoch[394](220/324): Loss: 0.5829 || Learning rate: lr=1.25e-05.
===> Epoch[394](230/324): Loss: 0.5698 || Learning rate: lr=1.25e-05.
===> Epoch[394](240/324): Loss: 0.5078 || Learning rate: lr=1.25e-05.
===> Epoch[394](250/324): Loss: 0.5778 || Learning rate: lr=1.25e-05.
===> Epoch[394](260/324): Loss: 0.5912 || Learning rate: lr=1.25e-05.
===> Epoch[394](270/324): Loss: 0.5779 || Learning rate: lr=1.25e-05.
===> Epoch[394](280/324): Loss: 0.5584 || Learning rate: lr=1.25e-05.
===> Epoch[394](290/324): Loss: 0.6727 || Learning rate: lr=1.25e-05.
===> Epoch[394](300/324): Loss: 0.6196 || Learning rate: lr=1.25e-05.
===> Epoch[394](310/324): Loss: 0.7444 || Learning rate: lr=1.25e-05.
===> Epoch[394](320/324): Loss: 0.7952 || Learning rate: lr=1.25e-05.
===> Epoch[395](10/324): Loss: 0.4853 || Learning rate: lr=1.25e-05.
===> Epoch[395](20/324): Loss: 0.8369 || Learning rate: lr=1.25e-05.
===> Epoch[395](30/324): Loss: 0.4956 || Learning rate: lr=1.25e-05.
===> Epoch[395](40/324): Loss: 0.4240 || Learning rate: lr=1.25e-05.
===> Epoch[395](50/324): Loss: 0.5160 || Learning rate: lr=1.25e-05.
===> Epoch[395](60/324): Loss: 0.9185 || Learning rate: lr=1.25e-05.
===> Epoch[395](70/324): Loss: 0.7149 || Learning rate: lr=1.25e-05.
===> Epoch[395](80/324): Loss: 0.8457 || Learning rate: lr=1.25e-05.
===> Epoch[395](90/324): Loss: 0.5754 || Learning rate: lr=1.25e-05.
===> Epoch[395](100/324): Loss: 0.5002 || Learning rate: lr=1.25e-05.
===> Epoch[395](110/324): Loss: 0.7863 || Learning rate: lr=1.25e-05.
===> Epoch[395](120/324): Loss: 0.5121 || Learning rate: lr=1.25e-05.
===> Epoch[395](130/324): Loss: 0.5211 || Learning rate: lr=1.25e-05.
===> Epoch[395](140/324): Loss: 0.6333 || Learning rate: lr=1.25e-05.
===> Epoch[395](150/324): Loss: 0.6089 || Learning rate: lr=1.25e-05.
===> Epoch[395](160/324): Loss: 0.7554 || Learning rate: lr=1.25e-05.
===> Epoch[395](170/324): Loss: 0.4768 || Learning rate: lr=1.25e-05.
===> Epoch[395](180/324): Loss: 0.7117 || Learning rate: lr=1.25e-05.
===> Epoch[395](190/324): Loss: 0.6547 || Learning rate: lr=1.25e-05.
===> Epoch[395](200/324): Loss: 0.5050 || Learning rate: lr=1.25e-05.
===> Epoch[395](210/324): Loss: 0.6092 || Learning rate: lr=1.25e-05.
===> Epoch[395](220/324): Loss: 0.7121 || Learning rate: lr=1.25e-05.
===> Epoch[395](230/324): Loss: 0.5619 || Learning rate: lr=1.25e-05.
===> Epoch[395](240/324): Loss: 0.7138 || Learning rate: lr=1.25e-05.
===> Epoch[395](250/324): Loss: 0.5559 || Learning rate: lr=1.25e-05.
===> Epoch[395](260/324): Loss: 0.6328 || Learning rate: lr=1.25e-05.
===> Epoch[395](270/324): Loss: 0.6579 || Learning rate: lr=1.25e-05.
===> Epoch[395](280/324): Loss: 0.7988 || Learning rate: lr=1.25e-05.
===> Epoch[395](290/324): Loss: 0.9157 || Learning rate: lr=1.25e-05.
===> Epoch[395](300/324): Loss: 0.7302 || Learning rate: lr=1.25e-05.
===> Epoch[395](310/324): Loss: 0.7050 || Learning rate: lr=1.25e-05.
===> Epoch[395](320/324): Loss: 0.6632 || Learning rate: lr=1.25e-05.
===> Epoch[396](10/324): Loss: 0.7794 || Learning rate: lr=1.25e-05.
===> Epoch[396](20/324): Loss: 0.7463 || Learning rate: lr=1.25e-05.
===> Epoch[396](30/324): Loss: 0.5932 || Learning rate: lr=1.25e-05.
===> Epoch[396](40/324): Loss: 0.7438 || Learning rate: lr=1.25e-05.
===> Epoch[396](50/324): Loss: 0.7504 || Learning rate: lr=1.25e-05.
===> Epoch[396](60/324): Loss: 0.6623 || Learning rate: lr=1.25e-05.
===> Epoch[396](70/324): Loss: 0.4873 || Learning rate: lr=1.25e-05.
===> Epoch[396](80/324): Loss: 0.6484 || Learning rate: lr=1.25e-05.
===> Epoch[396](90/324): Loss: 0.5984 || Learning rate: lr=1.25e-05.
===> Epoch[396](100/324): Loss: 0.7422 || Learning rate: lr=1.25e-05.
===> Epoch[396](110/324): Loss: 0.6791 || Learning rate: lr=1.25e-05.
===> Epoch[396](120/324): Loss: 0.5347 || Learning rate: lr=1.25e-05.
===> Epoch[396](130/324): Loss: 0.5193 || Learning rate: lr=1.25e-05.
===> Epoch[396](140/324): Loss: 0.8144 || Learning rate: lr=1.25e-05.
===> Epoch[396](150/324): Loss: 0.5063 || Learning rate: lr=1.25e-05.
===> Epoch[396](160/324): Loss: 0.5311 || Learning rate: lr=1.25e-05.
===> Epoch[396](170/324): Loss: 0.5136 || Learning rate: lr=1.25e-05.
===> Epoch[396](180/324): Loss: 0.8244 || Learning rate: lr=1.25e-05.
===> Epoch[396](190/324): Loss: 0.5902 || Learning rate: lr=1.25e-05.
===> Epoch[396](200/324): Loss: 0.6666 || Learning rate: lr=1.25e-05.
===> Epoch[396](210/324): Loss: 0.6241 || Learning rate: lr=1.25e-05.
===> Epoch[396](220/324): Loss: 0.7270 || Learning rate: lr=1.25e-05.
===> Epoch[396](230/324): Loss: 0.3894 || Learning rate: lr=1.25e-05.
===> Epoch[396](240/324): Loss: 0.6035 || Learning rate: lr=1.25e-05.
===> Epoch[396](250/324): Loss: 0.7281 || Learning rate: lr=1.25e-05.
===> Epoch[396](260/324): Loss: 0.5121 || Learning rate: lr=1.25e-05.
===> Epoch[396](270/324): Loss: 0.5568 || Learning rate: lr=1.25e-05.
===> Epoch[396](280/324): Loss: 0.6054 || Learning rate: lr=1.25e-05.
===> Epoch[396](290/324): Loss: 0.5845 || Learning rate: lr=1.25e-05.
===> Epoch[396](300/324): Loss: 0.7297 || Learning rate: lr=1.25e-05.
===> Epoch[396](310/324): Loss: 0.7170 || Learning rate: lr=1.25e-05.
===> Epoch[396](320/324): Loss: 0.8990 || Learning rate: lr=1.25e-05.
===> Epoch[397](10/324): Loss: 0.8302 || Learning rate: lr=1.25e-05.
===> Epoch[397](20/324): Loss: 0.8198 || Learning rate: lr=1.25e-05.
===> Epoch[397](30/324): Loss: 0.6807 || Learning rate: lr=1.25e-05.
===> Epoch[397](40/324): Loss: 0.7014 || Learning rate: lr=1.25e-05.
===> Epoch[397](50/324): Loss: 0.6788 || Learning rate: lr=1.25e-05.
===> Epoch[397](60/324): Loss: 0.6620 || Learning rate: lr=1.25e-05.
===> Epoch[397](70/324): Loss: 0.5382 || Learning rate: lr=1.25e-05.
===> Epoch[397](80/324): Loss: 0.3600 || Learning rate: lr=1.25e-05.
===> Epoch[397](90/324): Loss: 0.6520 || Learning rate: lr=1.25e-05.
===> Epoch[397](100/324): Loss: 0.6151 || Learning rate: lr=1.25e-05.
===> Epoch[397](110/324): Loss: 0.7169 || Learning rate: lr=1.25e-05.
===> Epoch[397](120/324): Loss: 0.8164 || Learning rate: lr=1.25e-05.
===> Epoch[397](130/324): Loss: 0.4997 || Learning rate: lr=1.25e-05.
===> Epoch[397](140/324): Loss: 0.4534 || Learning rate: lr=1.25e-05.
===> Epoch[397](150/324): Loss: 0.6670 || Learning rate: lr=1.25e-05.
===> Epoch[397](160/324): Loss: 0.6655 || Learning rate: lr=1.25e-05.
===> Epoch[397](170/324): Loss: 0.7519 || Learning rate: lr=1.25e-05.
===> Epoch[397](180/324): Loss: 0.4891 || Learning rate: lr=1.25e-05.
===> Epoch[397](190/324): Loss: 0.4124 || Learning rate: lr=1.25e-05.
===> Epoch[397](200/324): Loss: 0.8480 || Learning rate: lr=1.25e-05.
===> Epoch[397](210/324): Loss: 0.7503 || Learning rate: lr=1.25e-05.
===> Epoch[397](220/324): Loss: 0.5839 || Learning rate: lr=1.25e-05.
===> Epoch[397](230/324): Loss: 0.5830 || Learning rate: lr=1.25e-05.
===> Epoch[397](240/324): Loss: 0.4881 || Learning rate: lr=1.25e-05.
===> Epoch[397](250/324): Loss: 0.5911 || Learning rate: lr=1.25e-05.
===> Epoch[397](260/324): Loss: 0.8196 || Learning rate: lr=1.25e-05.
===> Epoch[397](270/324): Loss: 0.7906 || Learning rate: lr=1.25e-05.
===> Epoch[397](280/324): Loss: 0.7760 || Learning rate: lr=1.25e-05.
===> Epoch[397](290/324): Loss: 0.5684 || Learning rate: lr=1.25e-05.
===> Epoch[397](300/324): Loss: 0.5197 || Learning rate: lr=1.25e-05.
===> Epoch[397](310/324): Loss: 0.4952 || Learning rate: lr=1.25e-05.
===> Epoch[397](320/324): Loss: 0.5566 || Learning rate: lr=1.25e-05.
===> Epoch[398](10/324): Loss: 0.5557 || Learning rate: lr=1.25e-05.
===> Epoch[398](20/324): Loss: 0.5032 || Learning rate: lr=1.25e-05.
===> Epoch[398](30/324): Loss: 0.6438 || Learning rate: lr=1.25e-05.
===> Epoch[398](40/324): Loss: 0.8948 || Learning rate: lr=1.25e-05.
===> Epoch[398](50/324): Loss: 0.7827 || Learning rate: lr=1.25e-05.
===> Epoch[398](60/324): Loss: 0.7140 || Learning rate: lr=1.25e-05.
===> Epoch[398](70/324): Loss: 0.4379 || Learning rate: lr=1.25e-05.
===> Epoch[398](80/324): Loss: 0.7473 || Learning rate: lr=1.25e-05.
===> Epoch[398](90/324): Loss: 0.4987 || Learning rate: lr=1.25e-05.
===> Epoch[398](100/324): Loss: 0.6430 || Learning rate: lr=1.25e-05.
===> Epoch[398](110/324): Loss: 0.6810 || Learning rate: lr=1.25e-05.
===> Epoch[398](120/324): Loss: 0.6178 || Learning rate: lr=1.25e-05.
===> Epoch[398](130/324): Loss: 0.5820 || Learning rate: lr=1.25e-05.
===> Epoch[398](140/324): Loss: 0.4338 || Learning rate: lr=1.25e-05.
===> Epoch[398](150/324): Loss: 0.5295 || Learning rate: lr=1.25e-05.
===> Epoch[398](160/324): Loss: 0.9507 || Learning rate: lr=1.25e-05.
===> Epoch[398](170/324): Loss: 0.6095 || Learning rate: lr=1.25e-05.
===> Epoch[398](180/324): Loss: 0.5476 || Learning rate: lr=1.25e-05.
===> Epoch[398](190/324): Loss: 0.7948 || Learning rate: lr=1.25e-05.
===> Epoch[398](200/324): Loss: 0.6074 || Learning rate: lr=1.25e-05.
===> Epoch[398](210/324): Loss: 0.7278 || Learning rate: lr=1.25e-05.
===> Epoch[398](220/324): Loss: 0.8495 || Learning rate: lr=1.25e-05.
===> Epoch[398](230/324): Loss: 0.6971 || Learning rate: lr=1.25e-05.
===> Epoch[398](240/324): Loss: 0.8541 || Learning rate: lr=1.25e-05.
===> Epoch[398](250/324): Loss: 0.4907 || Learning rate: lr=1.25e-05.
===> Epoch[398](260/324): Loss: 0.7566 || Learning rate: lr=1.25e-05.
===> Epoch[398](270/324): Loss: 0.6001 || Learning rate: lr=1.25e-05.
===> Epoch[398](280/324): Loss: 0.7406 || Learning rate: lr=1.25e-05.
===> Epoch[398](290/324): Loss: 0.5520 || Learning rate: lr=1.25e-05.
===> Epoch[398](300/324): Loss: 0.5962 || Learning rate: lr=1.25e-05.
===> Epoch[398](310/324): Loss: 0.4682 || Learning rate: lr=1.25e-05.
===> Epoch[398](320/324): Loss: 0.6598 || Learning rate: lr=1.25e-05.
===> Epoch[399](10/324): Loss: 0.5922 || Learning rate: lr=1.25e-05.
===> Epoch[399](20/324): Loss: 0.5019 || Learning rate: lr=1.25e-05.
===> Epoch[399](30/324): Loss: 0.4612 || Learning rate: lr=1.25e-05.
===> Epoch[399](40/324): Loss: 0.7314 || Learning rate: lr=1.25e-05.
===> Epoch[399](50/324): Loss: 0.6868 || Learning rate: lr=1.25e-05.
===> Epoch[399](60/324): Loss: 1.0765 || Learning rate: lr=1.25e-05.
===> Epoch[399](70/324): Loss: 0.8475 || Learning rate: lr=1.25e-05.
===> Epoch[399](80/324): Loss: 0.6513 || Learning rate: lr=1.25e-05.
===> Epoch[399](90/324): Loss: 0.6631 || Learning rate: lr=1.25e-05.
===> Epoch[399](100/324): Loss: 0.6919 || Learning rate: lr=1.25e-05.
===> Epoch[399](110/324): Loss: 0.6298 || Learning rate: lr=1.25e-05.
===> Epoch[399](120/324): Loss: 0.5062 || Learning rate: lr=1.25e-05.
===> Epoch[399](130/324): Loss: 0.5034 || Learning rate: lr=1.25e-05.
===> Epoch[399](140/324): Loss: 0.5622 || Learning rate: lr=1.25e-05.
===> Epoch[399](150/324): Loss: 0.7986 || Learning rate: lr=1.25e-05.
===> Epoch[399](160/324): Loss: 0.5067 || Learning rate: lr=1.25e-05.
===> Epoch[399](170/324): Loss: 0.4848 || Learning rate: lr=1.25e-05.
===> Epoch[399](180/324): Loss: 0.5283 || Learning rate: lr=1.25e-05.
===> Epoch[399](190/324): Loss: 0.6130 || Learning rate: lr=1.25e-05.
===> Epoch[399](200/324): Loss: 0.6044 || Learning rate: lr=1.25e-05.
===> Epoch[399](210/324): Loss: 0.7540 || Learning rate: lr=1.25e-05.
===> Epoch[399](220/324): Loss: 0.5217 || Learning rate: lr=1.25e-05.
===> Epoch[399](230/324): Loss: 0.6926 || Learning rate: lr=1.25e-05.
===> Epoch[399](240/324): Loss: 0.4834 || Learning rate: lr=1.25e-05.
===> Epoch[399](250/324): Loss: 0.5685 || Learning rate: lr=1.25e-05.
===> Epoch[399](260/324): Loss: 0.7848 || Learning rate: lr=1.25e-05.
===> Epoch[399](270/324): Loss: 0.8806 || Learning rate: lr=1.25e-05.
===> Epoch[399](280/324): Loss: 0.6700 || Learning rate: lr=1.25e-05.
===> Epoch[399](290/324): Loss: 0.5844 || Learning rate: lr=1.25e-05.
===> Epoch[399](300/324): Loss: 0.5193 || Learning rate: lr=1.25e-05.
===> Epoch[399](310/324): Loss: 0.7722 || Learning rate: lr=1.25e-05.
===> Epoch[399](320/324): Loss: 0.4075 || Learning rate: lr=1.25e-05.
===> Epoch[400](10/324): Loss: 0.7641 || Learning rate: lr=1.25e-05.
===> Epoch[400](20/324): Loss: 0.7967 || Learning rate: lr=1.25e-05.
===> Epoch[400](30/324): Loss: 0.6490 || Learning rate: lr=1.25e-05.
===> Epoch[400](40/324): Loss: 0.5469 || Learning rate: lr=1.25e-05.
===> Epoch[400](50/324): Loss: 0.7291 || Learning rate: lr=1.25e-05.
===> Epoch[400](60/324): Loss: 0.6382 || Learning rate: lr=1.25e-05.
===> Epoch[400](70/324): Loss: 0.8616 || Learning rate: lr=1.25e-05.
===> Epoch[400](80/324): Loss: 0.5436 || Learning rate: lr=1.25e-05.
===> Epoch[400](90/324): Loss: 1.0771 || Learning rate: lr=1.25e-05.
===> Epoch[400](100/324): Loss: 0.5262 || Learning rate: lr=1.25e-05.
===> Epoch[400](110/324): Loss: 0.7261 || Learning rate: lr=1.25e-05.
===> Epoch[400](120/324): Loss: 0.4453 || Learning rate: lr=1.25e-05.
===> Epoch[400](130/324): Loss: 0.7803 || Learning rate: lr=1.25e-05.
===> Epoch[400](140/324): Loss: 1.0501 || Learning rate: lr=1.25e-05.
===> Epoch[400](150/324): Loss: 0.5523 || Learning rate: lr=1.25e-05.
===> Epoch[400](160/324): Loss: 0.5965 || Learning rate: lr=1.25e-05.
===> Epoch[400](170/324): Loss: 0.6144 || Learning rate: lr=1.25e-05.
===> Epoch[400](180/324): Loss: 0.4847 || Learning rate: lr=1.25e-05.
===> Epoch[400](190/324): Loss: 0.6300 || Learning rate: lr=1.25e-05.
===> Epoch[400](200/324): Loss: 0.5702 || Learning rate: lr=1.25e-05.
===> Epoch[400](210/324): Loss: 0.6646 || Learning rate: lr=1.25e-05.
===> Epoch[400](220/324): Loss: 0.6242 || Learning rate: lr=1.25e-05.
===> Epoch[400](230/324): Loss: 0.7116 || Learning rate: lr=1.25e-05.
===> Epoch[400](240/324): Loss: 0.6455 || Learning rate: lr=1.25e-05.
===> Epoch[400](250/324): Loss: 0.4711 || Learning rate: lr=1.25e-05.
===> Epoch[400](260/324): Loss: 0.6283 || Learning rate: lr=1.25e-05.
===> Epoch[400](270/324): Loss: 0.5859 || Learning rate: lr=1.25e-05.
===> Epoch[400](280/324): Loss: 0.6665 || Learning rate: lr=1.25e-05.
===> Epoch[400](290/324): Loss: 0.6969 || Learning rate: lr=1.25e-05.
===> Epoch[400](300/324): Loss: 0.3708 || Learning rate: lr=1.25e-05.
===> Epoch[400](310/324): Loss: 0.7699 || Learning rate: lr=1.25e-05.
===> Epoch[400](320/324): Loss: 0.8840 || Learning rate: lr=1.25e-05.
Checkpoint saved to weights/epoch_v2_400.pth

종료 코드 0(으)로 완료된 프로세스
